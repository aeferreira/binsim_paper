{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Simplification in Metabolomics\n",
    "\n",
    "Notebook to support the study on the application of **Bin**ary **Sim**plification as a competing form of pre-processing procedure for high-resolution metabolomics data.\n",
    "\n",
    "This is notebook `paper_binsim_supervised.ipynb`\n",
    "\n",
    "\n",
    "## Organization of the Notebook\n",
    "\n",
    "- Set up database of data sets\n",
    "- Application of different pre-treatments (including BinSim) to each data set\n",
    "- **Random Forest - optimization, predictive accuracy and important features: comparison after aplication of different pre-treatment procedures**\n",
    "- **PLS-DA - optimization, predictive accuracy and important features: comparison after aplication of different pre-treatment procedures**\n",
    "\n",
    "Generation of Random Forest and PLS-DA models will only be performed if `GENERATE = True` in the cells before application of each one.\n",
    "\n",
    "Furthermore, pre-processed datasets are loaded from json and hdf storages present in folder `paperimages`. These were created by notebook `paper_binsim_data_prep.ipynb`.\n",
    "\n",
    "Given the randomness of CV-folds the results may vary slightly from those presented in the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Needed Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from pathlib import Path\n",
    "\n",
    "# json for persistence\n",
    "import json\n",
    "from time import perf_counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.testing import assert_frame_equal\n",
    "\n",
    "import scipy.spatial.distance as dist\n",
    "import scipy.cluster.hierarchy as hier\n",
    "import scipy.stats as stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib import ticker\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Metabolinks package\n",
    "import metabolinks as mtl\n",
    "import metabolinks.transformations as transf\n",
    "\n",
    "# Python files in the repository\n",
    "import multianalysis as ma\n",
    "from elips import plot_confidence_ellipse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of dataset records\n",
    "\n",
    "`datasets` is the global dict that holds all data sets. It is a **dict of dict's**.\n",
    "\n",
    "Each data set is **represented as a dict**.\n",
    "\n",
    "Each record has the following fields (keys):\n",
    "\n",
    "- `name`: the table/figure name of the data set\n",
    "- `source`: the biological source for each dataset\n",
    "- `mode`: the aquisition mode\n",
    "- `alignment`: the alignment used to generate the data matrix\n",
    "- `data`: the data matrix\n",
    "- `target`: the sample labels, possibly already integer encoded\n",
    "- `<treatment name>`: transformed data matrix. These treatment names can be\n",
    "    - `Ionly`: missing value imputed data, only\n",
    "    - `P`: Pareto scaled data\n",
    "    - `NP`: Pareto scaled and normalized\n",
    "    - `NGP`: normalized, glog transformed and Pareto scaled\n",
    "    - `BinSim`: binary simplified data\n",
    "\n",
    "The keys of `datasets` may be shared with dicts holding records resulting from comparison analysis.\n",
    "\n",
    "Here are the keys (and respective names) of datasets used in this study:\n",
    "\n",
    "- GD_neg_global2 (GDg2-)\n",
    "- GD_pos_global2 (GDg2+)\n",
    "- GD_neg_class2 (GDc2-)\n",
    "- GD_pos_class2 (GDc2+)\n",
    "- YD (YD 2/15)\n",
    "- YD2 (YD 6/15)\n",
    "- vitis_types (GD types)\n",
    "- HD (HD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of grapevine data sets\n",
    "\n",
    "Grapevine Datasets (Negative and Positive) - 33 samples belonging to 11 different grapevine varieties (3 samples per variety/biological group) of FT-ICR-MS metabolomics data obtained in negative and positive ionization mode.\n",
    "\n",
    "5 different _Vitis_ species (other than _V. vinifera_) varieties:\n",
    "\n",
    "- CAN - 3 Samples (14, 15, 16) of _V. candicans Engelmann_ (VIVC variety number: 13508)\n",
    "- RIP - 3 Samples (17, 18, 19) of _V. riparia Michaux_ (Riparia Gloire de Montpellier, VIVC variety number: 4824) \n",
    "- ROT - 3 Samples (20, 21, 22) of _V. rotundifolia_ (Muscadinia Rotundifolia Michaux cv. Rotundifolia, VIVC variety number: 13586)\n",
    "- RU - 3 Samples (35, 36, 37) of _V. rupestris Scheele_ (Rupestris du lot, VIVC variety number: 10389)\n",
    "- LAB - 3 Samples (8, 9, 10) of _V. labrusca_ (Isabella, VIVC variety number: 5560)\n",
    "\n",
    "6 different _V. vinifera_ cultivars varieties are:\n",
    "\n",
    "- SYL - 3 samples (11, 12, 13) of the subspecies _sylvestris_ (VIVC variety number: -)\n",
    "- CS - 3 Samples (29, 30, 31) of the subspecies _sativa_ cultivar Cabernet Sauvignon (VIVC variety number: 1929)\n",
    "- PN - 3 Samples (23, 24, 25) of the subspecies _sativa_ cultivar Pinot Noir (VIVC variety number: 9279)\n",
    "- REG - 3 Samples (38, 39, 40) of the subspecies _sativa_ cultivar Regent (VIVC variety number: 4572)\n",
    "- RL - 3 Samples (26, 27, 28) of the subspecies _sativa_ cultivar Riesling Weiss (VIVC variety number: 10077)\n",
    "- TRI - 3 Samples (32, 33, 34) of the subspecies _sativa_ cultivar Cabernet Sauvignon (VIVC variety number: 15685)\n",
    "\n",
    "References for these data:\n",
    "\n",
    "- Maia M, Ferreira AEN, Nascimento R, et al. Integrating metabolomics and targeted gene expression to uncover potential biomarkers of fungal / oomycetes ‑ associated disease susceptibility in grapevine. Sci Rep. Published online 2020:1-15. doi:10.1038/s41598-020-72781-2\n",
    "- Maia M, Figueiredo A, Silva MS, Ferreira A. Grapevine untargeted metabolomics to uncover potential biomarkers of fungal/oomycetes-associated diseases. 2020. doi:10.6084/m9.figshare.12357314.v1\n",
    "\n",
    "**Peak Alignment** and **Peak Filtering** were performed with function `metabolinks.peak_alignment.align()`. Human leucine enkephalin (Sigma Aldrich) was used as the reference feature (internal standard, [M+H]+ = 556.276575 Da or [M-H]- = 554.262022 Da).\n",
    "\n",
    "**4** data matrices were constructed from this data:\n",
    "\n",
    "- Data sets named `GD_pos_global2` (GDg2+) and `GD_neg_global2` (GDg2-) were generated after retaining only features that occur (globally) at least twice in all 33 samples of the data sets (filtering/alignment) for the **positive mode** data acquisition and the **negative mode** data acquisition, respectively.\n",
    "- Data sets named `GD_pos_class2` (GDc2+) and `GD_neg_class2` (GDc2-) were generated after retaining only features that occur at least twice in the three replicates of at least one _Vitis_ variety in the data sets (filtering/alignment) for the **positive mode** data acquisition and the **negative mode** data acquisition, respectively.\n",
    "\n",
    "For the purpose of assessing the performance of supervised methods each of these four datasets was used with target labels defining classes corresponding to replicates of each of the 11 Vitis species/cultivars.\n",
    "\n",
    "For the purpose of assessing the performance of supervised methods under a binary (two-class) problem, data set `GD_neg_class2` was also used with target labels defining two classes: Vitis vinifera cultivars and \"wild\", non-vinifera Vitis species. This is dataset `vitis_types` (GD types)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of the yeast data set\n",
    "\n",
    "Yeast dataset - 15 samples belonging to 5 different yeast strains of _Saccharomyces cerevisiae_ (3 biological replicates per strain/biological group) of FT-ICR-MS metabolomics data obtained in positive ionization mode. The 5 strains were: the reference strain BY4741 (represented as BY) and 4 single-gene deletion mutants of this strain – ΔGLO1 (GLO1), ΔGLO2 (GLO2), ΔGRE3 (GRE3) and ΔENO1 (ENO1). These deleted genes are directly or indirectly related to methylglyoxal metabolism.\n",
    "\n",
    "Reference for this data:\n",
    "\n",
    "- Luz J, Pendão AS, Silva MS, Cordeiro C. FT-ICR-MS based untargeted metabolomics for the discrimination of yeast mutants. 2021. doi:10.6084/m9.figshare.15173559.v1\n",
    "\n",
    "**Peak Alignment** and **Peak Filtering** was performed with MetaboScape 4.0 software (see reference above for details in sample preparation, pre-processing, formula assignment). In short, Yeast Dataset was obtained with Electrospray Ionization in Positive Mode and pre-processed by MetaboScape 4.0 (Bruker Daltonics). Human leucine enkephalin (Sigma Aldrich) was used as the reference feature (internal standard, [M+H]+ = 556.276575 Da or [M-H]- = 554.262022 Da).\n",
    "\n",
    "**2** data matrices were constructed from this data:\n",
    "\n",
    "- Data set named `YD` (YD 2/15) was generated after retaining only features that occur (globally) at least twice in all 15 samples (filtering/alignment).\n",
    "- Data set named `YD2` (YD 6/15) was generated after retaining only features that occur (globally) at least six times in all 15 samples (filtering/alignment).\n",
    "\n",
    "For the purpose of assessing the performance of supervised methods, this data set was used with target labels defining classes corresponding to replicates of each of the 4 yeast strains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of the human dataset\n",
    "\n",
    "**HD is generated, described and analysed in notebook `paper_binsim_dataset_HD.ipynb`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path.cwd() / \"paperimages\" / 'processed_data.json'\n",
    "storepath = Path.cwd() / \"paperimages\" / 'processed_data.h5'\n",
    "with pd.HDFStore(storepath) as store:\n",
    "\n",
    "    with open(path, encoding='utf8') as read_file:\n",
    "        datasets = json.load(read_file)\n",
    "    \n",
    "    for dskey, dataset in datasets.items():\n",
    "        for key in dataset:\n",
    "            value = dataset[key]\n",
    "            if isinstance(value, str) and value.startswith(\"INSTORE\"):\n",
    "                storekey = value.split(\"_\", 1)[1]\n",
    "                dataset[key] = store[storekey]\n",
    "            # convert colors to tuples, since they are read as lists from json file\n",
    "            elif key == 'label_colors':\n",
    "                dataset[key] = {lbl: tuple(c) for lbl, c in value.items()}\n",
    "            elif key == 'sample_colors':\n",
    "                dataset[key] = [tuple(c) for c in value]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Statistical Analysis\n",
    "\n",
    "The Supervised Statistical Analysis methods used will be Random Forest and PLS-DA.\n",
    "\n",
    "The performance of the classifiers will be evaluated by their predictive **accuracy** (which will always be estimated by internal stratified 3-fold cross-validation or 5-fold cross-validation in `vitis_types`).\n",
    "\n",
    "Each method will be applied to the 4 differently-treated data sets (P, NP, NGP and BinSim) for each of the 7 benchmark data sets.\n",
    "\n",
    "**Note**: If constant `GENERATE` is **True**, Random Forest and PLS-DA will be applied. They are always on the cell before the application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import sklearn.ensemble as skensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC curves\n",
    "\n",
    "ROC curves are computed for the `vitis_types` classifier only (2-class problem) using `RandomForestClassifier` from scikit-learn in the function `RF_ROC_cv` from multianalysis.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(16)\n",
    "name = 'vitis_types'\n",
    "pos_label = 'vinifera'\n",
    "dataset = datasets[name]\n",
    "y = dataset['target']\n",
    "resROC = {}\n",
    "for treatment in ('P', 'NP', 'NGP', 'BinSim'):\n",
    "    df = dataset[treatment]\n",
    "    res = ma.RF_ROC_cv(df, y, pos_label, n_fold=5, n_trees=20, n_iter=20)\n",
    "    resROC[treatment] = res\n",
    "\n",
    "p4 = sns.color_palette('tab10', 4)\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    with sns.plotting_context(\"notebook\", font_scale=1.2):\n",
    "        f, ax = plt.subplots(1, 1, figsize=(5,5))\n",
    "        for treatment, color in zip(resROC, p4):\n",
    "            res = resROC[treatment]\n",
    "            mean_fpr = res['average fpr']\n",
    "            mean_tpr = res['average tpr']\n",
    "            mean_auc = res['mean AUC']\n",
    "            ax.plot(mean_fpr, mean_tpr, color=color,\n",
    "                   label=f'{treatment} (AUC = {mean_auc:.3f})',\n",
    "                   lw=2, alpha=0.8)\n",
    "        \n",
    "        ax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='lightgrey', alpha=.8)\n",
    "        ax.legend()\n",
    "        ax.set_xlim(None,1)\n",
    "        ax.set_ylim(0,None)\n",
    "        ax.set(xlabel='False positive rate', ylabel='True positive rate', title='')\n",
    "              # title=\"Random forest ROC curves for Vitis types data set\")\n",
    "        plt.show()\n",
    "        f.savefig('paperimages/ROC_vitis.pdf', dpi=200)\n",
    "        f.savefig('paperimages/ROC_vitis.png', dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization of the number of trees\n",
    "\n",
    "Random Forest models with different number of trees are built to assess when the predictive accuracy of the different models stops increasing with the number of trees. Grid search of number of trees from 10 to 100 for the random forests with 5 tree interval. See where the cross-validation estimated predictive accuracy stops improving for each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GENERATE:\n",
    "    # NOTE: for debugging\n",
    "    top_tree_in_grid=100\n",
    "    # otherwise\n",
    "    #top_tree_in_grid=200\n",
    "\n",
    "    # Vector with values for the parameter n_estimators\n",
    "    # Models will be built from 10 to top_tree_in_grid trees in 5 tree intervals\n",
    "    values = {'n_estimators': range(10,top_tree_in_grid,5)}\n",
    "\n",
    "    rf = skensemble.RandomForestClassifier(n_estimators=200)\n",
    "    clf = GridSearchCV(rf, values, cv=3)\n",
    "    clf_2class = GridSearchCV(rf, values, cv=5)\n",
    "\n",
    "    # For each dataset, build  Random Forest models with the different number of trees\n",
    "    # and store the predictive accuracy (estimated by k-fold cross-validation)\n",
    "\n",
    "    RF_optim = {}\n",
    "    for name, dataset in datasets.items():\n",
    "        for treatment in ('P', 'NP', 'NGP', 'P_RF', 'NP_RF', 'NGP_RF', 'BinSim'):\n",
    "            print('Fitting to', dataset['name'], 'pre-treatment', treatment, '...', end=' ')\n",
    "            rfname = name + ' ' + treatment\n",
    "            RF_optim[rfname] = {'dskey': name, 'dataset': dataset['name'], 'treatment':treatment}\n",
    "\n",
    "            if name == 'vitis_types':\n",
    "                clf2use = clf_2class\n",
    "            else:\n",
    "                clf2use = clf\n",
    "            clf2use.fit(dataset[treatment], dataset['target'])\n",
    "            \n",
    "            RF_optim[rfname]['scores'] = list(clf2use.cv_results_['mean_test_score'])\n",
    "            RF_optim[rfname]['n_trees'] = list(clf2use.cv_results_['param_n_estimators'])\n",
    "\n",
    "            print('Done!')\n",
    "    print('writing results to file')\n",
    "    path = Path.cwd() / 'paperimages' / 'RF_optim.json'\n",
    "    print(path.name)\n",
    "    with open(path, \"w\", encoding='utf8') as write_file:\n",
    "        json.dump(RF_optim, write_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path.cwd() / 'paperimages' / 'RF_optim.json'\n",
    "with open(path, \"r\", encoding='utf8') as read_file:\n",
    "    RF_optim = json.load(read_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plots of tree number optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results and adjusting parameters of the plot\n",
    "\n",
    "def plot_RF_otimization_ntrees(RF_optim, dskey, ax=None, ylabel='', title='', ylim=(30,101)):\n",
    "    p7 = sns.color_palette('tab20', 7)\n",
    "    to_plot = [optim for key, optim in RF_optim.items() if optim['dskey'] == dskey]\n",
    "    treatments = ('P', 'P_RF', 'NP', 'NP_RF', 'NGP', 'NGP_RF', 'BinSim')\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    for treatment, color in zip(treatments, p7):\n",
    "        for optim in to_plot:\n",
    "            if optim['treatment'] == treatment:\n",
    "                break\n",
    "        ax.plot(optim['n_trees'], [s*100 for s in optim['scores']], label=treatment, color=color)\n",
    "    ax.set(ylabel=ylabel, xlabel='Number of Trees', ylim=ylim, title=title)\n",
    "    ax.legend()\n",
    "\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    with sns.plotting_context(\"notebook\", font_scale=1.2):\n",
    "        f, axs = plt.subplots(4, 2, figsize=(12,20), constrained_layout=True)\n",
    "\n",
    "        for dskey, ax in zip(datasets, axs.ravel()):\n",
    "        \n",
    "            plot_RF_otimization_ntrees(RF_optim, dskey, ax=ax,\n",
    "                                       ylabel='Random Forest CV Mean Accuracy (%)',\n",
    "                                       title=datasets[dskey][\"name\"])\n",
    "\n",
    "        f.suptitle('Optimization of the number of trees')\n",
    "        axs[3][1].set_visible(False)\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest models\n",
    "\n",
    "Random Forest models were built with the `RandomForestClassifier` from scikit-learn using the `RF_model_CV` from multianalysis.py (each step explained better there).\n",
    "\n",
    "This function performs n iterations to randomly sample the folds in k-fold cross-validation - more combinations of training and test samples are used to offset the small (in terms of samples per group) dataset. \n",
    "\n",
    "It then stores predictive accuracy of the models (across the iterations) and an ordered list of the most to least important features (average across the iterations) in building the model according to the Gini Importance calculated by scikit-learn of each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GENERATE:\n",
    "    # NOTE: for debugging\n",
    "    iter_num=20\n",
    "    # otherwise\n",
    "    #iter_num=100\n",
    "\n",
    "    RF_all = {}\n",
    "\n",
    "    # Application of the Random Forests for each differently-treated dataset\n",
    "    for name, dataset in datasets.items():\n",
    "        for treatment in ('P', 'P_RF', 'NP', 'NP_RF', 'NGP', 'NGP_RF', 'BinSim'):\n",
    "            print(f'Fitting random forest for {name} with treatment {treatment}', end=' ...')\n",
    "            rfname = name + ' ' + treatment\n",
    "            RF_all[rfname] = {'dskey': name, 'dataset': dataset['name'], 'treatment':treatment}\n",
    "            n_fold = 5 if name == 'vitis_types' else 3\n",
    "\n",
    "            fit = ma.RF_model_CV(dataset[treatment], dataset['target'], iter_num=iter_num, n_fold=n_fold, n_trees=100)\n",
    "            RF_all[rfname].update(fit)\n",
    "\n",
    "            print(f'done')    \n",
    "    fname = 'paperimages/RF_all.json'\n",
    "    with open(fname, \"w\", encoding='utf8') as write_file:\n",
    "        json.dump(RF_all, write_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'paperimages/RF_all.json'\n",
    "with open(fname, \"r\", encoding='utf8') as read_file:\n",
    "    RF_all = json.load(read_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read HD example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path.cwd() / \"paperimages\" / 'processed_data_HD.json'\n",
    "storepath = Path.cwd() / \"paperimages\" / 'processed_data_HD.h5'\n",
    "with pd.HDFStore(storepath) as store:\n",
    "\n",
    "    with open(path, encoding='utf8') as read_file:\n",
    "        hd_datasets = json.load(read_file)\n",
    "\n",
    "    for dskey, dataset in hd_datasets.items():\n",
    "        for key in dataset:\n",
    "            value = dataset[key]\n",
    "            if isinstance(value, str) and value.startswith(\"INSTORE\"):\n",
    "                storekey = value.split(\"_\", 1)[1]\n",
    "                dataset[key] = store[storekey]\n",
    "            # transform colors, saved as lists in json, back into tuples\n",
    "            elif key == 'label_colors':\n",
    "                dataset[key] = {lbl: tuple(c) for lbl, c in value.items()}\n",
    "            elif key == 'sample_colors':\n",
    "                dataset[key] = [tuple(c) for c in value]\n",
    "\n",
    "datasets['HD'] = hd_datasets['HD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'paperimages/RF_all_HD.json'\n",
    "with open(fname, \"r\", encoding='utf8') as read_file:\n",
    "    RF_all_HD = json.load(read_file)\n",
    "\n",
    "RF_all.update(RF_all_HD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results of the Random Forest - Performance (Predictive Accuracy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy across the iterations\n",
    "accuracies = pd.DataFrame({name: RF_all[name]['accuracy'] for name in RF_all})\n",
    "accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distributions of Predictive Accuracies for GDg2+ and GDg2-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treatments = ('P', 'P_RF', 'NP', 'NP_RF', 'NGP', 'NGP_RF', 'BinSim')\n",
    "column_names = [t+'-' for t in treatments] + [t+'+' for t in treatments]\n",
    "\n",
    "# Violin plot of the distribution of the predictive accuracy (in %) across the iterations of randomly sampled folds for each \n",
    "# differently-treated dataset.\n",
    "\n",
    "cols2keep = [col for col in accuracies.columns if 'global2' in col]\n",
    "\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    f, ax = plt.subplots(figsize=(14,6))\n",
    "    res100 = accuracies[cols2keep] * 100\n",
    "    res100.columns = column_names\n",
    "\n",
    "    #colors = ['blue','orange','green','red']\n",
    "    colors = sns.color_palette('tab20', 7)\n",
    "    colors.extend(colors)\n",
    "\n",
    "    sns.violinplot(data=res100, palette=colors)\n",
    "\n",
    "    plt.ylabel('Prediction Accuracy (%) - Random Forest', fontsize=13)\n",
    "    plt.ylim([25,100])\n",
    "    ax.tick_params(axis='x', which='major', labelsize = 12)\n",
    "    ax.tick_params(axis='y', which='major', labelsize = 15)\n",
    "    for ticklabel, tickcolor in zip(plt.gca().get_xticklabels(), colors):\n",
    "        ticklabel.set_color(tickcolor)\n",
    "    f.suptitle('Predictive Accuracy of Random Forest models - GD alignment global2', fontsize=16)\n",
    "    #plt.title('Yeast Dataset', fontsize = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_stats = pd.DataFrame({'Average accuracy': accuracies.mean(axis=0),\n",
    "                               'STD': accuracies.std(axis=0)})\n",
    "accuracy_stats = accuracy_stats.assign(dataset=[RF_all[name]['dataset'] for name in RF_all],\n",
    "                                       treatment=[RF_all[name]['treatment'] for name in RF_all])\n",
    "accuracy_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average Predictive Accuracies of Random Forest models\n",
    "\n",
    "Error bars were built based on the standard deviation of the predictive accuracies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing BinSIm with treatments based on 1/2 min imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p4 = sns.color_palette('tab10', 4)\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    with sns.plotting_context(\"notebook\", font_scale=1.3):\n",
    "        f, ax = plt.subplots(1, 1, figsize=(14, 6))\n",
    "        x = np.arange(len(datasets))  # the label locations\n",
    "        labels = [datasets[name]['name'] for name in datasets]\n",
    "        width = 0.17  # the width of the bars\n",
    "        for i, treatment in enumerate(('P', 'NP', 'NGP', 'BinSim')):\n",
    "            acc_treatment = accuracy_stats[accuracy_stats['treatment']==treatment]\n",
    "            offset = - 0.3 + i * 0.2\n",
    "            rects = ax.bar(x + offset, acc_treatment['Average accuracy'], width, label=treatment, color = p4[i])\n",
    "            ax.errorbar(x + offset, y=acc_treatment['Average accuracy'], yerr=acc_treatment['STD'],\n",
    "                        ls='none', ecolor='0.2', capsize=3)\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(labels)\n",
    "        ax.set(ylabel='Average accuracy', title='', ylim=(0.4,1.02))\n",
    "        ax.text(-0.5, 0.95, 'A', weight='bold', fontsize=15)\n",
    "        ax.legend(loc='upper left', bbox_to_anchor=(0.14, 1))\n",
    "        #f.savefig('paperimages/RF_performance.pdf' , dpi=200)\n",
    "        #f.savefig('paperimages/RF_performance.png' , dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing BinSIm with treatments based on RF imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p4 = sns.color_palette('tab10', 4)\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    with sns.plotting_context(\"notebook\", font_scale=1.3):\n",
    "        f, ax = plt.subplots(1, 1, figsize=(14, 6))\n",
    "        x = np.arange(len(datasets))  # the label locations\n",
    "        labels = [datasets[name]['name'] for name in datasets]\n",
    "        width = 0.17  # the width of the bars\n",
    "        for i, treatment in enumerate(('P_RF', 'NP_RF', 'NGP_RF', 'BinSim')):\n",
    "            acc_treatment = accuracy_stats[accuracy_stats['treatment']==treatment]\n",
    "            offset = - 0.3 + i * 0.2\n",
    "            rects = ax.bar(x + offset, acc_treatment['Average accuracy'], width, label=treatment, color = p4[i])\n",
    "            ax.errorbar(x + offset, y=acc_treatment['Average accuracy'], yerr=acc_treatment['STD'],\n",
    "                        ls='none', ecolor='0.2', capsize=3)\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(labels)\n",
    "        ax.set(ylabel='Average accuracy', title='', ylim=(0.4,1.02))\n",
    "        ax.text(-0.5, 0.95, 'A', weight='bold', fontsize=15)\n",
    "        ax.legend(loc='upper left', bbox_to_anchor=(0.14, 1))\n",
    "        #f.savefig('paperimages/RF_performance.pdf' , dpi=200)\n",
    "        #f.savefig('paperimages/RF_performance.png' , dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing BinSIm all other treatments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "p7 = sns.color_palette('tab20', 7)\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    with sns.plotting_context(\"notebook\", font_scale=1.3):\n",
    "        f, ax = plt.subplots(1, 1, figsize=(14, 6))\n",
    "        x = np.arange(len(datasets))  # the label locations\n",
    "        labels = [datasets[name]['name'] for name in datasets]\n",
    "        width = 0.12  # the width of the bars\n",
    "        for i, treatment in enumerate(('P', 'P_RF', 'NP', 'NP_RF', 'NGP', 'NGP_RF', 'BinSim')):\n",
    "            acc_treatment = accuracy_stats[accuracy_stats['treatment']==treatment]\n",
    "            offset = - 0.5 + i * 0.12\n",
    "            rects = ax.bar(x + offset, acc_treatment['Average accuracy'], width, label=treatment, color = p7[i])\n",
    "            ax.errorbar(x + offset, y=acc_treatment['Average accuracy'], yerr=acc_treatment['STD'],\n",
    "                        ls='none', ecolor='0.2', capsize=3)\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(labels)\n",
    "        ax.set(ylabel='Average accuracy', title='', ylim=(0.4,1.02))\n",
    "        ax.text(-0.5, 0.95, 'A', weight='bold', fontsize=15)\n",
    "        ax.legend(loc='upper left', bbox_to_anchor=(0.15, 1), fontsize=12)\n",
    "        #f.savefig('paperimages/RF_performance.pdf' , dpi=200)\n",
    "        #f.savefig('paperimages/RF_performance.png' , dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important feature analysis - Random Forest\n",
    "\n",
    "Importance metric - Gini Importance\n",
    "\n",
    "The 2% most important features in each case were taken and the median of the occurrence of those features in samples (`# samples`), the median of the occurrence of those features in groups/varieties (`# classes`), the ratio between these two measures (`samples/classes`, has a maximum of 3) and the number of features chosen (`# top 2%`) were calculated. Furthermore, \n",
    "the percentage of the models explained by the 2% of the most important features (`% model explained`) and the number of times the importance of the most important feature is greater than the average importance of a feature in each case (`top feat / average`).\n",
    "\n",
    "Swarmplot were also built to see the distributions of the number of samples each important feature in each method appears to see their overall distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the characteristics of a the top 'fraction' of features based on their importance (by a certain metric).\n",
    "def compute_RF_top_feat_stats(name, resdict, fraction=0.02):\n",
    "    imp_features = resdict['important_features']\n",
    "\n",
    "    original_data = datasets[resdict['dskey']]['data']\n",
    "    labels = datasets[resdict['dskey']]['target']\n",
    "\n",
    "    ratio = []\n",
    "    nsamples = []\n",
    "    n_groups = []\n",
    "    feature_locs = []\n",
    "\n",
    "    n_features = original_data.shape[1]\n",
    "\n",
    "    # Top 2% (rounded)\n",
    "    number = round(fraction * n_features)\n",
    "    # Calculate, store how many times most important\n",
    "    # feature is more important than the average importance\n",
    "    \n",
    "    resdict['top feature fold importance'] = imp_features[0][1]/(1/n_features)\n",
    "\n",
    "    top_features = imp_features[:number]\n",
    "    \n",
    "    resdict['number top features'] = number\n",
    "    \n",
    "    s = 0 # Count the % explained - add for each feature\n",
    "    for loc, importance in top_features: # Iterate for only the number of features considered as important\n",
    "\n",
    "        s += importance * 100 \n",
    "\n",
    "         # fetch feature form unprocessed data\n",
    "        column = original_data.iloc[:, loc]\n",
    "        feature_notnull = column.notnull()\n",
    "\n",
    "        # Count how many samples the feature appears in\n",
    "        feature_nsamples = feature_notnull.sum()\n",
    "        nsamples.append(feature_nsamples) \n",
    "\n",
    "        # Count how many groups the feature appears in\n",
    "        group_occurrence = {}        \n",
    "        for n, f in enumerate(feature_notnull):\n",
    "            if f == True:\n",
    "                group_occurrence[labels[n]] = 1\n",
    "        feature_ngroups = sum(list((group_occurrence.values())))\n",
    "        n_groups.append(feature_ngroups)\n",
    "\n",
    "        # Calculate the ratio of number of samples to number of groups the feature appears in\n",
    "        ratio.append(feature_nsamples/feature_ngroups)\n",
    "\n",
    "        # Store identification of feature\n",
    "        feature_locs.append(loc)\n",
    "\n",
    "    resdict['% top features explanation'] = s\n",
    "\n",
    "    resdict.update({'n samples': nsamples, 'n groups': n_groups, 'ratios':ratio, 'top features': feature_locs})\n",
    "\n",
    "for name, resdict in RF_all.items():\n",
    "    compute_RF_top_feat_stats(name, resdict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important Features Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RF_stats = {'# samples': {name: np.median(RF_all[name]['n samples']) for name in RF_all},\n",
    "               '# samples max': {name: np.max(RF_all[name]['n samples']) for name in RF_all},\n",
    "               '# samples min': {name: np.min(RF_all[name]['n samples']) for name in RF_all},\n",
    "               '# classes': {name: np.median(RF_all[name]['n groups']) for name in RF_all},\n",
    "               '# classes max': {name: np.max(RF_all[name]['n groups']) for name in RF_all},\n",
    "               '# classes min': {name: np.min(RF_all[name]['n groups']) for name in RF_all},\n",
    "               'samples/classes': {name: np.median(RF_all[name]['ratios']) for name in RF_all},\n",
    "               '# top 2%': {name: np.median(RF_all[name]['number top features']) for name in RF_all},\n",
    "               '% model explained': {name: np.median(RF_all[name]['% top features explanation']) for name in RF_all},\n",
    "               'top feat / average':{name: np.median(RF_all[name]['top feature fold importance']) for name in RF_all}}\n",
    "df_RF_stats = pd.DataFrame(df_RF_stats)\n",
    "\n",
    "df_RF_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Occurrences by samples and by classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_only = df_RF_stats[['# samples', '# classes', 'samples/classes']]\n",
    "line = pd.DataFrame([], columns=dist_only.columns, index=[''])\n",
    "display_df = pd.concat([dist_only[dist_only.index.str.startswith('GD_neg_global2')], line, dist_only[dist_only.index.str.startswith('YD2')]])\n",
    "#display_df\n",
    "\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    with sns.plotting_context(\"notebook\", font_scale=1.2):\n",
    "        f, ax = plt.subplots(figsize=(6,9))\n",
    "        hm = sns.heatmap(display_df, annot=True, fmt='.1f', ax=ax, cmap = sns.cm.rocket_r)\n",
    "        bottom, top = ax.get_ylim()\n",
    "\n",
    "        plt.text(1.5,-0.5,'GDg2-',horizontalalignment='center', verticalalignment='center')\n",
    "        plt.text(1.5,7.5,'YD 2/15',horizontalalignment='center', verticalalignment='center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting swarmplots with the distributions of samples important features appear in\n",
    "\n",
    "#### Swarmplot for the GDg2- and YD 2/15 data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_RF_stats[df_RF_stats.index.str.startswith('GD_neg_global2') & ~df_RF_stats.index.str.endswith('RF')].iloc[:, :6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RF_stats[df_RF_stats.index.str.startswith('YD ') & ~df_RF_stats.index.str.endswith('RF')].iloc[:, :6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treatments = ('P','NP','NGP','BinSim')\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    with sns.plotting_context(\"notebook\", font_scale=1.8):\n",
    "        f, (axl, axr) = plt.subplots(1,2, figsize=(14,5), constrained_layout=True)\n",
    "        colors = sns.color_palette('tab10', 4)\n",
    "        tl = ticker.FixedLocator([3, 6, 9, 12, 15, 18, 24, 30, 33])\n",
    "        \n",
    "        df_RF_sample_dist = {}\n",
    "        for name, res in RF_all.items():\n",
    "            if res['dataset'] == \"GDg2-\" and res[\"treatment\"] in treatments:\n",
    "                df_RF_sample_dist[name] = res['n samples']\n",
    "                \n",
    "        #df_RF_sample_dist = {name: RF_all[name]['n samples'] for name in RF_all if (RF_all[name]['dskey']=='GD_neg_global2') and (RF_all[name]['treatment'] in treatments)}\n",
    "        df_RF_sample_dist = pd.DataFrame(df_RF_sample_dist)\n",
    "\n",
    "        sns.swarmplot(data=df_RF_sample_dist, size=6, palette=colors, ax=axl)\n",
    "\n",
    "        axl.set(xticklabels = ['P','NP','NGP','BinSim'])\n",
    "        axl.set_ylabel('Number of samples')\n",
    "        for ticklabel, tickcolor in zip(axl.get_xticklabels(), colors):\n",
    "            ticklabel.set_color(tickcolor)\n",
    "        axl.yaxis.set_major_locator(tl)\n",
    "        for spine in axl.spines.values():\n",
    "            spine.set_edgecolor('0.1')\n",
    "\n",
    "        df_RF_sample_dist = {}\n",
    "        for name, res in RF_all.items():\n",
    "            if res['dataset'] == \"YD 2/15\" and res[\"treatment\"] in treatments:\n",
    "                df_RF_sample_dist[name] = res['n samples']\n",
    "        #df_RF_sample_dist = {name: RF_all[name]['n samples'] for name in RF_all if RF_all[name]['dskey']=='YD'}\n",
    "        df_RF_sample_dist = pd.DataFrame(df_RF_sample_dist)\n",
    "\n",
    "        sns.swarmplot(data=df_RF_sample_dist, size=6, palette=colors, ax=axr)\n",
    "\n",
    "        axr.set(xticklabels = ['P','NP','NGP','BinSim'])\n",
    "        #axr.set_ylabel('Number of samples')\n",
    "        for ticklabel, tickcolor in zip(axr.get_xticklabels(), colors):\n",
    "            ticklabel.set_color(tickcolor)\n",
    "        axr.yaxis.set_major_locator(tl)\n",
    "        for spine in axr.spines.values():\n",
    "            spine.set_edgecolor('0.1')\n",
    "\n",
    "        #title = \"\"\"Distribution of sample occurrence of the 2 % top features of Random Forest models\n",
    "        #GDg2-\"\"\"\n",
    "        title = ''\n",
    "\n",
    "        #f.suptitle(title, fontsize=14)\n",
    "        f.savefig('paperimages/fi_dist_RF.png', dpi=600)\n",
    "        f.savefig('paperimages/fi_dist_RF.pdf', dpi=200)\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RF_sample_dist = {}\n",
    "for name, res in RF_all.items():\n",
    "    if res['dataset'] == \"GDg2-\" and res[\"treatment\"] in treatments:\n",
    "        df_RF_sample_dist[name] = res['n samples']\n",
    "\n",
    "#df_RF_sample_dist = {name: RF_all[name]['n samples'] for name in RF_all if (RF_all[name]['dskey']=='GD_neg_global2') and (RF_all[name]['treatment'] in treatments)}\n",
    "df_RF_sample_dist = pd.DataFrame(df_RF_sample_dist)\n",
    "binsim_dist = df_RF_sample_dist['GD_neg_global2 BinSim']\n",
    "missbinsim_dist = 100.0 * (33 - binsim_dist) / 33\n",
    "print('total', len(missbinsim_dist))\n",
    "print('with at least 50 % mv', len(missbinsim_dist)-(missbinsim_dist > 50).sum())\n",
    "print('with at least 80 % mv', len(missbinsim_dist)-(missbinsim_dist > 80).sum())\n",
    "print('with at least 90 % mv', len(missbinsim_dist)-(missbinsim_dist > 90).sum())\n",
    "47/73"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import ticker\n",
    "\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    with sns.plotting_context(\"notebook\", font_scale=1.4):\n",
    "        f, (axl, axr) = plt.subplots(1, 2, figsize=(16,12.5))\n",
    "\n",
    "        colors = sns.color_palette('tab10', 4)\n",
    "        tl = ticker.FixedLocator([2, 3, 6, 12, 18, 24, 30, 33])\n",
    "        \n",
    "        to_plot = [name for name in RF_all if RF_all[name]['dskey']=='GD_neg_global2' and RF_all[name]['treatment'] in treatments]\n",
    "        df_RF_sample_dist = {RF_all[name]['treatment']: RF_all[name]['n samples'] for name in to_plot}\n",
    "        df_RF_sample_dist = pd.DataFrame(df_RF_sample_dist)\n",
    "        # Plot the swarm plot and adjust parameters\n",
    "        sns.swarmplot(data=df_RF_sample_dist, size=3, palette=colors, orient='h', ax=axl)\n",
    "        title = \"GDg2-\"\n",
    "        axl.set(xlabel='Nº of samples', title=title)\n",
    "        for ticklabel, tickcolor in zip(axl.get_yticklabels(), colors):\n",
    "            ticklabel.set_color(tickcolor)\n",
    "        axl.xaxis.set_major_locator(tl)\n",
    "        \n",
    "        to_plot = [name for name in RF_all if RF_all[name]['dskey']=='GD_pos_global2' and RF_all[name]['treatment'] in treatments]\n",
    "        df_RF_sample_dist = {RF_all[name]['treatment']: RF_all[name]['n samples'] for name in to_plot}\n",
    "        df_RF_sample_dist = pd.DataFrame(df_RF_sample_dist)\n",
    "        \n",
    "        sns.swarmplot(data=df_RF_sample_dist, size=3, palette=colors, orient='h', ax=axr)\n",
    "        title = \"GDg2+\"\n",
    "        axr.set(xlabel='Nº of samples', yticklabels = [], title=title)\n",
    "        axr.xaxis.set_major_locator(tl)\n",
    "\n",
    "        f.suptitle(\"Sample occurrence of top 2% important features of Random Forest models\", fontsize=18)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RF_sample_dist = {name: RF_all[name]['n samples'] for name in RF_all if RF_all[name]['dskey']=='YD' and RF_all[name]['treatment'] in treatments}\n",
    "df_RF_sample_dist = pd.DataFrame(df_RF_sample_dist)\n",
    "\n",
    "# Plot the swarm plot and adjust parameters\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    with sns.plotting_context(\"notebook\", font_scale=1.5):\n",
    "        f, ax = plt.subplots(figsize=(8,8))\n",
    "\n",
    "        colors = sns.color_palette('tab10', 4)\n",
    "        sns.swarmplot(data=df_RF_sample_dist, size=4, palette=colors)\n",
    "\n",
    "        ax.set(xticklabels = ['P','NP','NGP','BinSim'])\n",
    "        ax.set_ylabel('Number of samples')\n",
    "        for ticklabel, tickcolor in zip(ax.get_xticklabels(), colors):\n",
    "            ticklabel.set_color(tickcolor)\n",
    "        title = \"\"\"Distribution of sample occurrence of the 2 % top features of Random Forest models\n",
    "        YD 2/15\"\"\"\n",
    "\n",
    "        #f.suptitle(title, fontsize=14)\n",
    "        f.savefig('paperimages/fi_distYD_RF.png', dpi=600)\n",
    "        f.savefig('paperimages/fi_distYD_RF.pdf', dpi=200)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RF_sample_dist = {name: RF_all[name]['n samples'] for name in RF_all if RF_all[name]['dskey']=='vitis_types' and RF_all[name]['treatment'] in treatments}\n",
    "df_RF_sample_dist = pd.DataFrame(df_RF_sample_dist)\n",
    "\n",
    "# Plot the swarm plot and adjust parameters\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    with sns.plotting_context(\"notebook\", font_scale=1.4):\n",
    "        f, ax = plt.subplots(figsize=(14,8))\n",
    "\n",
    "        colors = sns.color_palette('tab10', 4)\n",
    "        sns.swarmplot(data=df_RF_sample_dist, size=4, palette=colors)\n",
    "\n",
    "        ax.set(xticklabels = ['P','NP','NGP','BinSim'])\n",
    "        ax.set_ylabel('Number of samples')\n",
    "        for ticklabel, tickcolor in zip(ax.get_xticklabels(), colors):\n",
    "            ticklabel.set_color(tickcolor)\n",
    "        title = f\"\"\"Distribution of sample occurrence of the 2 % top features of Random Forest models\n",
    "        {datasets[RF_all[name]['dskey']]['name']}\"\"\"\n",
    "\n",
    "        f.suptitle(title, fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overlaps of Features considered as Important to build each differently-treated data set - Random Forest\n",
    "\n",
    "For each benchmark data set, 4 differently-treated data sets were obtained. These each have their own set of important features (top 2% chosen). The following cells compute the number of features in common between those sets (common in all 4, in each possible set of 3 and 2 and exclusive to only 1) for each benchmark data set.\n",
    "\n",
    "The features appearing in all 4 are not taken into account for counting the features that appear in each set of 3 or sets of 2, and the same for the features in sets of 3 not appearing in the counts for the sets of 2. This is made so all values can be directly put into Venn Diagram without the need for further calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treatments = ('P','NP','NGP','BinSim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we only want counts, let's use the list of locs for the important features\n",
    "features_as_sets = {name:{} for name in datasets}\n",
    "for name, rf_dict in RF_all.items():\n",
    "    if rf_dict['treatment'] not in treatments:\n",
    "        continue\n",
    "    features_as_sets[rf_dict['dskey']][rf_dict['treatment']] = set(RF_all[name]['top features'])\n",
    "\n",
    "def features_overlap(features):\n",
    "    \"\"\"Compute Venn diagram counts. Expects a dict of sets as input.\"\"\"\n",
    "    counts = {}\n",
    "    # union\n",
    "    union = list(itertools.accumulate(features.values(), set.union))[-1]\n",
    "    counts['union'] = len(union)\n",
    " \n",
    "    # exclusive\n",
    "    for i in features:\n",
    "        fi = features[i]\n",
    "        for j in features:\n",
    "            if j == i:\n",
    "                continue\n",
    "            fi = fi - features[j]\n",
    "        counts[(i,)] = len(fi)\n",
    "    # pairwise\n",
    "    for i,j in itertools.combinations(features,2):\n",
    "        tuplecomb = (i,j)\n",
    "        fi = set.intersection(features[i], features[j])\n",
    "        for others in features:\n",
    "            if others in tuplecomb:\n",
    "                continue\n",
    "            fi = fi - features[others]\n",
    "        counts[tuplecomb] = len(fi)\n",
    "    # overlap of 3\n",
    "    for i,j, k in itertools.combinations(features,3):\n",
    "        tuplecomb = (i,j, k)\n",
    "        fi = set.intersection(features[i], features[j], features[k])\n",
    "        for others in features:\n",
    "            if others in tuplecomb:\n",
    "                continue\n",
    "            fi = fi - features[others]\n",
    "        counts[tuplecomb] = len(fi)\n",
    "    # present in all\n",
    "    intersection = list(itertools.accumulate(features.values(), set.intersection))[-1]\n",
    "    counts[tuple(features.keys())] = len(intersection)\n",
    "    \n",
    "    # Assuming respect for the order insertion in a dict...\n",
    "    assert sum(list(counts.values())[1:]) == counts['union']\n",
    "    \n",
    "    return counts\n",
    "\n",
    "def print_overlaps(counts):\n",
    "    for k in counts:\n",
    "        if k == 'union':\n",
    "            print('Nº of features across all (union):', counts[k])   \n",
    "        elif len(k) == 1:\n",
    "            print('exclusive to', k[0], counts[k])\n",
    "        else:\n",
    "            print('Number of features in', k, counts[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for GDg2-\n",
    "features = features_as_sets['GD_neg_global2']\n",
    "\n",
    "counts = features_overlap(features)\n",
    "print_overlaps(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for dskey in datasets:\n",
    "    print(f'\\nFeature overlap for dataset {datasets[dskey][\"name\"]} -----------------------')\n",
    "    features = features_as_sets[dskey]\n",
    "    counts = features_overlap(features)\n",
    "    print_overlaps(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projection in Latent Structures Discriminant Analysis (PLS-DA)\n",
    "\n",
    "PLS-DA models were built using the `PLSRegression` of scikit-learn while imposing a prediction decision rule where the group corresponding to the maximum value in the predicted regression vector (one value for each group) obtained for a test sample (ypred) is chosen. The target vectors matrix was made by each different group is made into a column and 1 represents the sample belonging to that group (0 means it doesn't belong) with one-hot encoding. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization - Search for the best number of components of PLS model\n",
    "\n",
    "The number of components were optimized by the 1 - PRESS/SS or Q$^2$ (PLS Score) of models built with 1 to n components.\n",
    "\n",
    "PRESS - Predictive Residual Sum of Squares; SS - residual Sum of Squares\n",
    "\n",
    "Strategy: Build PLS-DA with different number of components and extract the PLS score (inverse relation to the mean-squared error ) of the models estimated with stratified k-fold cross-validation. Observe at which point (number of components) the PLS Score starts approaching a \"stable maximum value\". This was done using the `optim_PLSDA_n_components` from multianalysis.py (see details there).\n",
    "\n",
    "These regression metrics are not suitable to evaluate the performance of the classifier, they were just used to optimize the number of components to build the final PLS-DA models.\n",
    "\n",
    "**Note**: GD data sets here are only shown until a maximum of 10 components for visualization purposes. This optimization was made with a higher maximum number of components than 10, where we observed than 11 components was a good choice for these data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stdout\n",
    "if GENERATE:\n",
    "    treatments = ('P', 'P_RF', 'NP', 'NP_RF', 'NGP', 'NGP_RF', 'BinSim')\n",
    "    # above is to supress PLS warnings\n",
    "\n",
    "    # NOTE: for debugging\n",
    "    max_comp=10\n",
    "    # otherwise\n",
    "    #max_comp=50\n",
    "\n",
    "    # Store Results\n",
    "    PLS_optim = {}\n",
    "\n",
    "    # Build and extract metrics from models build with different number of components by using the optim_PLS function.\n",
    "    for name, dataset in datasets.items():\n",
    "        for treatment in treatments:\n",
    "            print(f'Fitting PLS-DA model for {name} with treatment {treatment}', end=' ...')\n",
    "            plsdaname = name + ' ' + treatment\n",
    "            PLS_optim[plsdaname] = {'dskey': name, 'dataset':dataset['name'], 'treatment':treatment}\n",
    "            n_fold = 5 if name == 'vitis_types' else 3\n",
    "            optim = ma.optim_PLSDA_n_components(dataset[treatment], dataset['target'],\n",
    "                                                max_comp=max_comp, n_fold=n_fold).CVscores\n",
    "            PLS_optim[plsdaname]['CV_scores'] = optim\n",
    "            print(f'done')\n",
    "    fname = 'paperimages/PLSDA_optim.json'\n",
    "    with open(fname, \"w\", encoding='utf8') as write_file:\n",
    "        json.dump(PLS_optim, write_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'paperimages/PLSDA_optim.json'\n",
    "with open(fname, \"r\", encoding='utf8') as read_file:\n",
    "    PLS_optim = json.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treatments = ('P', 'P_RF', 'NP', 'NP_RF', 'NGP', 'NGP_RF', 'BinSim')\n",
    "treat_colors = dict(zip(treatments, sns.color_palette('tab20', 7)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results and adjusting plot parameters\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    with sns.plotting_context(\"notebook\", font_scale=1.2):\n",
    "        f, (axl, axr) = plt.subplots(1, 2, figsize = (12,6))\n",
    "        plt.suptitle('Performance based on number of components, PLS-DA. GD_global2 datasets', fontsize=15)\n",
    "\n",
    "        for name, data in PLS_optim.items():\n",
    "\n",
    "            if data['dskey'] == 'GD_neg_global2':\n",
    "                # Negative Grapevine Dataset\n",
    "                axl.plot(range(1, len(data['CV_scores']) + 1), data['CV_scores'],\n",
    "                         color = treat_colors[data['treatment']],\n",
    "                         label=data['treatment'])\n",
    "                axl.set(xlabel='Number of Components',\n",
    "                        ylabel='PLS Score (1 - PRESS/SS)',\n",
    "                        title='Negative Mode Grapevine Dataset')\n",
    "                axl.legend()\n",
    "                axl.set_ylim([0, 1])\n",
    "\n",
    "            if data['dskey'] == 'GD_pos_global2':\n",
    "                # Positive Grapevine Dataset\n",
    "                axr.plot(range(1, len(data['CV_scores']) + 1), data['CV_scores'],\n",
    "                         color = treat_colors[data['treatment']],\n",
    "                         label=data['treatment'])\n",
    "                axr.set(xlabel='Number of Components',\n",
    "                        ylabel='',\n",
    "                        title='Positive Mode Grapevine Dataset')\n",
    "                axr.legend()\n",
    "                axr.set_ylim([0, 1])\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results and adjusting plot parameters\n",
    "\n",
    "\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    with sns.plotting_context(\"notebook\", font_scale=1.2):\n",
    "        f, (axl, axr) = plt.subplots(1, 2, figsize = (12,6))\n",
    "        plt.suptitle('Performance based on number of components, PLS-DA. GD_class2 datasets', fontsize=15)\n",
    "\n",
    "        for name, data in PLS_optim.items():\n",
    "\n",
    "            if data['dskey'] == 'GD_neg_class2':\n",
    "                # Negative Grapevine Dataset\n",
    "                axl.plot(range(1, len(data['CV_scores']) + 1), data['CV_scores'],\n",
    "                         color = treat_colors[data['treatment']],\n",
    "                         label=data['treatment'])\n",
    "                axl.set(xlabel='Number of Components',\n",
    "                        ylabel='PLS Score (1 - PRESS/SS)',\n",
    "                        title='Negative Mode Grapevine Dataset')\n",
    "                axl.legend()\n",
    "                axl.set_ylim([0, 1])\n",
    "\n",
    "            if data['dskey'] == 'GD_pos_class2':\n",
    "                # Positive Grapevine Dataset\n",
    "                axr.plot(range(1, len(data['CV_scores']) + 1), data['CV_scores'],\n",
    "                         color = treat_colors[data['treatment']],\n",
    "                         label=data['treatment'])\n",
    "                axr.set(xlabel='Number of Components',\n",
    "                        ylabel='',\n",
    "                        title='Positive Mode Grapevine Dataset')\n",
    "                axr.legend()\n",
    "                axr.set_ylim([0, 1])\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results and adjusting plot parameters\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    with sns.plotting_context(\"notebook\", font_scale=1.2):\n",
    "        f, (axl, axr) = plt.subplots(1, 2, figsize = (12,6))\n",
    "        plt.suptitle('Performance based on number of components, PLS-DA.', fontsize=15)\n",
    "\n",
    "        for name, data in PLS_optim.items():\n",
    "\n",
    "            if data['dskey'] == 'YD':\n",
    "                # Negative Grapevine Dataset\n",
    "                axl.plot(range(1, len(data['CV_scores']) + 1), data['CV_scores'],\n",
    "                         color = treat_colors[data['treatment']],\n",
    "                         label=data['treatment'])\n",
    "                axl.set(xlabel='Number of Components',\n",
    "                        ylabel='PLS Score (1 - PRESS/SS)',\n",
    "                        title='Yeast data set')\n",
    "                axl.legend()\n",
    "                axl.set_ylim([0, 1])\n",
    "\n",
    "            if data['dskey'] == 'vitis_types':\n",
    "                # Positive Grapevine Dataset\n",
    "                axr.plot(range(1, len(data['CV_scores']) + 1), data['CV_scores'],\n",
    "                         color = treat_colors[data['treatment']],\n",
    "                         label=data['treatment'])\n",
    "                axr.set(xlabel='Number of Components',\n",
    "                        ylabel='',\n",
    "                        title='GD target is Vitis types')\n",
    "                axr.legend()\n",
    "                axr.set_ylim([0, 1])\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the YD 2/15, YD 6/15 and GD types, PLS-DA models are going to be built with 6 components.\n",
    "\n",
    "For all the other GD data sets (GDg2-, GDg2+, GDc2-, GDc2+), PLS-DA models are going to be built with 11 components (results for the optimization with more than 10 components are not shown here)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLS-DA models\n",
    "\n",
    "PLS-DA models were built with the `PLSRegression` (PLS2 algorithm used) from scikit-learn using the `PLSDA_model_CV` from multianalysis.py (each step explained better there).\n",
    "\n",
    "The prediction decision rule used was to choose the group corresponding to the maximum value in the predicted regression vector - ypred (one value for each group, PLS output) - obtained for a test sample - naive MAX rule. The target vectors matrix was made by each different group is made into a column and 1 represents the sample belonging to that group (0 means it doesn't belong) with one-hot encoding. \n",
    "\n",
    "This function performs n iterations to randomly sample the folds in k-fold cross-validation - more combinations of training and test samples are used to offset the small (in terms of samples per group) dataset. \n",
    "\n",
    "It then stores predictive accuracy of the models, the Q$^2$ score (across the iterations) and an ordered list of the most to least important features (average across the iterations) in building the model according to a chosen feature importance metric.\n",
    "\n",
    "The function allows the choice of 3 different feature importance metrics (feat_type):\n",
    "\n",
    "- **VIP (Variable Importance/Influence in Projection)** - used in the paper (slowest)\n",
    "- Coef. (regression coefficients - sum)\n",
    "- Weights (Sum of the X-weights for each feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stdout\n",
    "GENERATE = False\n",
    "if GENERATE:\n",
    "    PLSDA_all = {}\n",
    "\n",
    "    # NOTE: for debugging\n",
    "    iter_num=10\n",
    "\n",
    "    # For each differently-treated dataset, fit PLS-DA models on n randomly sampled folds (for stratified cross-validation)\n",
    "    for name, dataset in datasets.items():\n",
    "        for treatment in ('P', 'P_RF', 'NP', 'NP_RF', 'NGP', 'NGP_RF', 'BinSim'):\n",
    "            print(f'Fitting a PLS-DA model to {name} with treatment {treatment}', end=' ...')\n",
    "            plsdaname = name + ' ' + treatment\n",
    "            PLSDA_all[plsdaname] = {'dskey': name, 'dataset': dataset['name'], 'treatment':treatment}\n",
    "            n_comp = 15 if name.startswith('GD') else 6\n",
    "            n_fold = 5 if name == 'vitis_types' else 3\n",
    "            fit = ma.PLSDA_model_CV(dataset[treatment], dataset['target'],\n",
    "                                    n_comp=n_comp, n_fold=n_fold,\n",
    "                                    iter_num=iter_num,\n",
    "                                    feat_type='VIP')\n",
    "            PLSDA_all[plsdaname].update(fit)\n",
    "            print(f'done')     \n",
    "    PLSDA_all\n",
    "    fname = 'paperimages/PLSDA_all.json'\n",
    "    with open(fname, \"w\", encoding='utf8') as write_file:\n",
    "        json.dump(PLSDA_all, write_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results of the PLS-DA - Performance (Predictive Accuracy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy across iterations\n",
    "fname = 'paperimages/PLSDA_all.json'\n",
    "with open(fname, \"r\", encoding='utf8') as read_file:\n",
    "    PLSDA_all = json.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'paperimages/PLSDA_all_HD.json'\n",
    "with open(fname, \"r\", encoding='utf8') as read_file:\n",
    "    PLSDA_all_HD = json.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLSDA_all.update(PLSDA_all_HD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = pd.DataFrame({name: PLSDA_all[name]['accuracy'] for name in PLSDA_all})\n",
    "accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution for GDg2- and GDg2+ and then for GDc2- and GDc2+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#column_names = ['P -', 'NP -', 'NGP -', 'BinSim -', 'P +', 'NP +', 'NGP +', 'BinSim +']\n",
    "# Violin plot of the distribution of the predictive accuracy (in %) across 100 iterations of randomly sampled folds for each \n",
    "# differently-treated dataset and adjustments to the parameters of the plot.\n",
    "\n",
    "cols2keep = [col for col in accuracies.columns if 'global2' in col]\n",
    "treatments = ('P', 'P_RF', 'NP', 'NP_RF', 'NGP', 'NGP_RF', 'BinSim')\n",
    "column_names = [t+'-' for t in treatments] + [t+'+' for t in treatments]\n",
    "\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    f, ax = plt.subplots(figsize=(14,6))\n",
    "    res100 = accuracies[cols2keep] * 100\n",
    "    res100.columns = column_names\n",
    "\n",
    "    colors = sns.color_palette('tab20', 7)\n",
    "    colors.extend(colors)\n",
    "\n",
    "    sns.violinplot(data=res100, palette=colors)\n",
    "\n",
    "    plt.ylabel('Prediction Accuracy (%) - PLSDA', fontsize=13)\n",
    "    plt.ylim([25,100])\n",
    "    ax.tick_params(axis='x', which='major', labelsize = 12)\n",
    "    ax.tick_params(axis='y', which='major', labelsize = 15)\n",
    "    for ticklabel, tickcolor in zip(ax.get_xticklabels(), colors):\n",
    "        ticklabel.set_color(tickcolor)\n",
    "    f.suptitle('Predictive Accuracy of PLS-DA models - Grapevine Datasets global2', fontsize=16)\n",
    "    #plt.title('Yeast Dataset', fontsize = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['P -', 'NP -', 'NGP -', 'BinSim -', 'P +', 'NP +', 'NGP +', 'BinSim +']\n",
    "# Violin plot of the distribution of the predictive accuracy (in %) across the iterations of randomly sampled folds for each \n",
    "# differently-treated dataset and adjustments to the parameters of the plot.\n",
    "\n",
    "cols2keep = [col for col in accuracies.columns if 'class2' in col]\n",
    "treatments = ('P', 'P_RF', 'NP', 'NP_RF', 'NGP', 'NGP_RF', 'BinSim')\n",
    "column_names = [t+'-' for t in treatments] + [t+'+' for t in treatments]\n",
    "\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    f, ax = plt.subplots(figsize=(14,6))\n",
    "    res100 = accuracies[cols2keep] * 100\n",
    "    res100.columns = column_names\n",
    "\n",
    "    #colors = ['blue','orange','green','red']\n",
    "    colors = sns.color_palette('tab20', 7)\n",
    "    colors.extend(colors)\n",
    "\n",
    "    sns.violinplot(data=res100, palette=colors)\n",
    "\n",
    "    plt.ylabel('Prediction Accuracy (%) - PLSDA', fontsize=13)\n",
    "    plt.ylim([25,105])\n",
    "    ax.tick_params(axis='x', which='major', labelsize = 12)\n",
    "    ax.tick_params(axis='y', which='major', labelsize = 15)\n",
    "    for ticklabel, tickcolor in zip(ax.get_xticklabels(), colors):\n",
    "        ticklabel.set_color(tickcolor)\n",
    "    f.suptitle('Predictive Accuracy of PLS-DA models - Grapevine Datasets class2', fontsize=16)\n",
    "    #plt.title('Yeast Dataset', fontsize = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_stats = pd.DataFrame({'Average accuracy': accuracies.mean(axis=0),\n",
    "                               'STD': accuracies.std(axis=0)})\n",
    "accuracy_stats = accuracy_stats.assign(dataset=[PLSDA_all[name]['dataset'] for name in PLSDA_all],\n",
    "                                       treatment=[PLSDA_all[name]['treatment'] for name in PLSDA_all])\n",
    "accuracy_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average Predictive Accuracies of PLS-DA models\n",
    "\n",
    "Error bars were built based on the standard deviation of the predictive accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p4 = sns.color_palette('tab10', 4)\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    with sns.plotting_context(\"notebook\", font_scale=1.3):\n",
    "        f, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "        x = np.arange(len(datasets))  # the label locations\n",
    "        labels = [datasets[name]['name'] for name in datasets]\n",
    "        width = 0.17  # the width of the bars\n",
    "        for i, treatment in enumerate(('P', 'NP', 'NGP', 'BinSim')):\n",
    "            acc_treatment = accuracy_stats[accuracy_stats['treatment']==treatment]\n",
    "            offset = - 0.3 + i * 0.2\n",
    "            rects = ax.bar(x + offset, acc_treatment['Average accuracy'], width, label=treatment, color = p4[i])\n",
    "            ax.errorbar(x + offset, y=acc_treatment['Average accuracy'], yerr=acc_treatment['STD'],\n",
    "                        ls='none', ecolor='0.2', capsize=3)\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(labels)\n",
    "        ax.set(ylabel='Average accuracy', title='', ylim=(0.4,1.02))\n",
    "        ax.text(-0.5, 0.95, 'B', weight='bold', fontsize=15)\n",
    "        #ax.legend(loc='upper left', bbox_to_anchor=(0.12, 1))\n",
    "        #f.savefig('paperimages/PLSDA_performance.pdf' , dpi=200)\n",
    "        #f.savefig('paperimages/PLSDA_performance.png' , dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p4 = sns.color_palette('tab10', 4)\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    with sns.plotting_context(\"notebook\", font_scale=1.3):\n",
    "        f, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "        x = np.arange(len(datasets))  # the label locations\n",
    "        labels = [datasets[name]['name'] for name in datasets]\n",
    "        width = 0.17  # the width of the bars\n",
    "        for i, treatment in enumerate(('P_RF', 'NP_RF', 'NGP_RF', 'BinSim')):\n",
    "            acc_treatment = accuracy_stats[accuracy_stats['treatment']==treatment]\n",
    "            offset = - 0.3 + i * 0.2\n",
    "            rects = ax.bar(x + offset, acc_treatment['Average accuracy'], width, label=treatment, color = p4[i])\n",
    "            ax.errorbar(x + offset, y=acc_treatment['Average accuracy'], yerr=acc_treatment['STD'],\n",
    "                        ls='none', ecolor='0.2', capsize=3)\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(labels)\n",
    "        ax.set(ylabel='Average accuracy', title='', ylim=(0.4,1.02))\n",
    "        ax.text(-0.5, 0.95, 'B', weight='bold', fontsize=15)\n",
    "        #ax.legend(loc='upper left', bbox_to_anchor=(0.12, 1))\n",
    "        #f.savefig('paperimages/PLSDA_performance.pdf' , dpi=200)\n",
    "        #f.savefig('paperimages/PLSDA_performance.png' , dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p7 = sns.color_palette('tab20', 7)\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    with sns.plotting_context(\"notebook\", font_scale=1.3):\n",
    "        f, ax = plt.subplots(1, 1, figsize=(14, 6))\n",
    "        x = np.arange(len(datasets))  # the label locations\n",
    "        labels = [datasets[name]['name'] for name in datasets]\n",
    "        width = 0.12  # the width of the bars\n",
    "        for i, treatment in enumerate(('P', 'P_RF', 'NP', 'NP_RF', 'NGP', 'NGP_RF', 'BinSim')):\n",
    "            acc_treatment = accuracy_stats[accuracy_stats['treatment']==treatment]\n",
    "            offset = - 0.4 + i * 0.12\n",
    "            rects = ax.bar(x + offset, acc_treatment['Average accuracy'], width, label=treatment, color = p7[i])\n",
    "            ax.errorbar(x + offset, y=acc_treatment['Average accuracy'], yerr=acc_treatment['STD'],\n",
    "                        ls='none', ecolor='0.2', capsize=3)\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(labels)\n",
    "        ax.set(ylabel='Average accuracy', title='', ylim=(0.4,1.02))\n",
    "        ax.text(-0.5, 0.95, 'B', weight='bold', fontsize=15)\n",
    "        #ax.legend(loc='upper left', bbox_to_anchor=(0.15, 1), fontsize=12)\n",
    "        #f.savefig('paperimages/RF_performance.pdf' , dpi=200)\n",
    "        #f.savefig('paperimages/RF_performance.png' , dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy plots for RF and PLS-DA together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuraciesRF = pd.DataFrame({name: RF_all[name]['accuracy'] for name in RF_all})\n",
    "accuracy_stats_RF = pd.DataFrame({'Average accuracy': accuraciesRF.mean(axis=0),\n",
    "                                  'STD': accuraciesRF.std(axis=0)})\n",
    "accuracy_stats_RF = accuracy_stats_RF.assign(dataset=[RF_all[name]['dataset'] for name in RF_all],\n",
    "                                       treatment=[RF_all[name]['treatment'] for name in RF_all])\n",
    "\n",
    "\n",
    "accuraciesPLSDA = pd.DataFrame({name: PLSDA_all[name]['accuracy'] for name in PLSDA_all})\n",
    "accuracy_stats_PLSDA = pd.DataFrame({'Average accuracy': accuraciesPLSDA.mean(axis=0),\n",
    "                                     'STD': accuraciesPLSDA.std(axis=0)})\n",
    "accuracy_stats_PLSDA = accuracy_stats_PLSDA.assign(dataset=[PLSDA_all[name]['dataset'] for name in PLSDA_all],\n",
    "                                       treatment=[PLSDA_all[name]['treatment'] for name in PLSDA_all])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p4 = sns.color_palette('tab10', 4)\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    with sns.plotting_context(\"notebook\", font_scale=1.5):\n",
    "        f, (axu, axl) = plt.subplots(2, 1, figsize=(12, 10), constrained_layout=True)\n",
    "        x = np.arange(len(datasets))  # the label locations\n",
    "        labels = [datasets[name]['name'] for name in datasets]\n",
    "        width = 0.17  # the width of the bars\n",
    "        \n",
    "        for i, treatment in enumerate(('P', 'NP', 'NGP', 'BinSim')):\n",
    "            acc_treatment = accuracy_stats_RF[accuracy_stats_RF['treatment']==treatment]\n",
    "            offset = - 0.3 + i * 0.2\n",
    "            rects = axu.bar(x + offset, acc_treatment['Average accuracy'], width, label=treatment, color = p4[i])\n",
    "            axu.errorbar(x + offset, y=acc_treatment['Average accuracy'], yerr=acc_treatment['STD'],\n",
    "                        ls='none', ecolor='0.2', capsize=3)\n",
    "        axu.set_xticks(x)\n",
    "        axu.set_xticklabels(labels)\n",
    "        axu.set(ylabel='Average accuracy', title='', ylim=(0.4,1.02))\n",
    "        axu.text(-0.5, 0.95, 'A', weight='bold', fontsize=16)\n",
    "        for spine in axu.spines.values():\n",
    "            spine.set_edgecolor('0.1')\n",
    "        \n",
    "        for i, treatment in enumerate(('P', 'NP', 'NGP', 'BinSim')):\n",
    "            acc_treatment = accuracy_stats_PLSDA[accuracy_stats_PLSDA['treatment']==treatment]\n",
    "            offset = - 0.3 + i * 0.2\n",
    "            rects = axl.bar(x + offset, acc_treatment['Average accuracy'], width, label=treatment, color = p4[i])\n",
    "            axl.errorbar(x + offset, y=acc_treatment['Average accuracy'], yerr=acc_treatment['STD'],\n",
    "                        ls='none', ecolor='0.2', capsize=3)\n",
    "        axl.set_xticks(x)\n",
    "        axl.set_xticklabels(labels)\n",
    "        axl.set(ylabel='Average accuracy', title='', ylim=(0.4,1.02))\n",
    "        axl.text(-0.5, 0.95, 'B', weight='bold', fontsize=16)\n",
    "        for spine in axl.spines.values():\n",
    "            spine.set_edgecolor('0.1')\n",
    "        axu.legend(loc='upper left', bbox_to_anchor=(0.15, 0.98))\n",
    "        plt.show()\n",
    "        #f.savefig('paperimages/supervised_performance.pdf' , dpi=200)\n",
    "        #f.savefig('paperimages/supervised_performance.png' , dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p4 = sns.color_palette('tab10', 4)\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    with sns.plotting_context(\"notebook\", font_scale=1.5):\n",
    "        f, (axu, axl) = plt.subplots(2, 1, figsize=(12, 10), constrained_layout=True)\n",
    "        x = np.arange(len(datasets))  # the label locations\n",
    "        labels = [datasets[name]['name'] for name in datasets]\n",
    "        width = 0.17  # the width of the bars\n",
    "        \n",
    "        for i, treatment in enumerate(('P_RF', 'NP_RF', 'NGP_RF', 'BinSim')):\n",
    "            acc_treatment = accuracy_stats_RF[accuracy_stats_RF['treatment']==treatment]\n",
    "            offset = - 0.3 + i * 0.2\n",
    "            rects = axu.bar(x + offset, acc_treatment['Average accuracy'], width, label=treatment, color = p4[i])\n",
    "            axu.errorbar(x + offset, y=acc_treatment['Average accuracy'], yerr=acc_treatment['STD'],\n",
    "                        ls='none', ecolor='0.2', capsize=3)\n",
    "        axu.set_xticks(x)\n",
    "        axu.set_xticklabels(labels)\n",
    "        axu.set(ylabel='Average accuracy', title='', ylim=(0.4,1.02))\n",
    "        axu.text(-0.5, 0.95, 'A', weight='bold', fontsize=16)\n",
    "        for spine in axu.spines.values():\n",
    "            spine.set_edgecolor('0.1')\n",
    "        \n",
    "        for i, treatment in enumerate(('P_RF', 'NP_RF', 'NGP_RF', 'BinSim')):\n",
    "            acc_treatment = accuracy_stats_PLSDA[accuracy_stats_PLSDA['treatment']==treatment]\n",
    "            offset = - 0.3 + i * 0.2\n",
    "            rects = axl.bar(x + offset, acc_treatment['Average accuracy'], width, label=treatment, color = p4[i])\n",
    "            axl.errorbar(x + offset, y=acc_treatment['Average accuracy'], yerr=acc_treatment['STD'],\n",
    "                        ls='none', ecolor='0.2', capsize=3)\n",
    "        axl.set_xticks(x)\n",
    "        axl.set_xticklabels(labels)\n",
    "        axl.set(ylabel='Average accuracy', title='', ylim=(0.4,1.02))\n",
    "        axl.text(-0.5, 0.95, 'B', weight='bold', fontsize=16)\n",
    "        for spine in axl.spines.values():\n",
    "            spine.set_edgecolor('0.1')\n",
    "        axu.legend(loc='upper left', bbox_to_anchor=(0.15, 0.98))\n",
    "        plt.show()\n",
    "        #f.savefig('paperimages/supervised_performance_rf.pdf' , dpi=200)\n",
    "        #f.savefig('paperimages/supervised_performance_rf.png' , dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_treatment(t):\n",
    "    if t == 'BinSim':\n",
    "        return t\n",
    "    if t.endswith('RF'):\n",
    "        return 'RF ' + t.split('_', 1)[0]\n",
    "    else:\n",
    "        return '½ min ' + t\n",
    "\n",
    "p7 = sns.color_palette('tab20', 7)\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    with sns.plotting_context(\"notebook\", font_scale=1.8):\n",
    "        f, (axu, axl) = plt.subplots(2, 1, figsize=(16, 10), constrained_layout=True)\n",
    "        x = np.arange(len(datasets))  # the label locations\n",
    "        labels = [datasets[name]['name'] for name in datasets]\n",
    "        #for a typographically accurate minus sign\n",
    "        labels = [lbl.replace('-', '−') for lbl in labels]\n",
    "        \n",
    "        width = 0.12  # the width of the bars\n",
    "        \n",
    "        for i, treatment in enumerate(('P', 'P_RF', 'NP', 'NP_RF', 'NGP', 'NGP_RF', 'BinSim')):\n",
    "            acc_treatment = accuracy_stats_RF[accuracy_stats_RF['treatment']==treatment]\n",
    "            offset = - 0.4 + i * 0.12\n",
    "            rects = axu.bar(x + offset, acc_treatment['Average accuracy'], width, label=transform_treatment(treatment), color = p7[i])\n",
    "            axu.errorbar(x + offset, y=acc_treatment['Average accuracy'], yerr=acc_treatment['STD'],\n",
    "                        ls='none', ecolor='0.2', capsize=3)\n",
    "        axu.set_xticks(x)\n",
    "        axu.set_xticklabels(labels)\n",
    "        axu.set(ylabel='Average accuracy', title='', ylim=(0.4,1.02))\n",
    "        axu.text(-0.5, 0.95, 'A', weight='bold', fontsize=18)\n",
    "        for spine in axu.spines.values():\n",
    "            spine.set_edgecolor('0.1')\n",
    "        \n",
    "        for i, treatment in enumerate(('P', 'P_RF', 'NP', 'NP_RF', 'NGP', 'NGP_RF', 'BinSim')):\n",
    "            acc_treatment = accuracy_stats_PLSDA[accuracy_stats_PLSDA['treatment']==treatment]\n",
    "            offset = - 0.4 + i * 0.12\n",
    "            rects = axl.bar(x + offset, acc_treatment['Average accuracy'], width, label=treatment, color = p7[i])\n",
    "            axl.errorbar(x + offset, y=acc_treatment['Average accuracy'], yerr=acc_treatment['STD'],\n",
    "                        ls='none', ecolor='0.2', capsize=3)\n",
    "        axl.set_xticks(x)\n",
    "        axl.set_xticklabels(labels)\n",
    "        axl.set(ylabel='Average accuracy', title='', ylim=(0.4,1.02))\n",
    "        axl.text(-0.5, 0.95, 'B', weight='bold', fontsize=18)\n",
    "        for spine in axl.spines.values():\n",
    "            spine.set_edgecolor('0.1')\n",
    "        axu.legend(loc='upper left', bbox_to_anchor=(1, 1), fontsize=18)\n",
    "        axl.grid(b=False, which='major', axis='x')\n",
    "        axu.grid(b=False, which='major', axis='x')\n",
    "        plt.show()\n",
    "        f.savefig('paperimages/supervised_performance.pdf' , dpi=200)\n",
    "        f.savefig('paperimages/supervised_performance.png' , dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important feature analysis - PLS-DA\n",
    "\n",
    "**The same process as it was applied for Random Forest.**\n",
    "\n",
    "Importance metric - VIP score\n",
    "\n",
    "The 2% most important features in each case were taken and the median of the occurrence of those features in samples (`# samples`), the median of the occurrence of those features in groups/varieties (`# classes`), the ratio between these two measures (`samples/classes`, has a maximum of 3) and the number of features chosen (`# top 2%`) were calculated. Furthermore, \n",
    "the percentage of the models explained by the 2% of the most important features (`% model explained`) and the number of times the importance of the most important feature is greater than the average importance of a feature in each case (`top feat / average`).\n",
    "\n",
    "Swarmplot were also built to see the distributions of the number of samples each important feature in each method appears to see their overall distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the characteristics of a the top 'fraction' of features based on their importance (by a certain metric).\n",
    "def compute_PLSDA_top_feat_stats(name, resdict, fraction=0.02):\n",
    "    imp_features = resdict['important_features']\n",
    "\n",
    "    original_data = datasets[resdict['dskey']]['data']\n",
    "    labels = datasets[resdict['dskey']]['target']\n",
    "\n",
    "    ratio = []\n",
    "    nsamples = []\n",
    "    n_groups = []\n",
    "    feature_locs = []\n",
    "\n",
    "    n_features = original_data.shape[1]\n",
    "\n",
    "    # Top 2% (rounded)\n",
    "    number = round(fraction * n_features)\n",
    "    # Calculate, store how many times most important\n",
    "    # feature is more important than the average importance\n",
    "    \n",
    "    resdict['top feature fold importance'] = imp_features[0][1]\n",
    "\n",
    "    top_features = imp_features[:number]\n",
    "    \n",
    "    resdict['number top features'] = number\n",
    "    \n",
    "    s = 0 # Count the % explained - add for each feature\n",
    "    for loc, importance in top_features: # Iterate for only the number of features considered as important\n",
    "\n",
    "        s += importance * 100 / n_features\n",
    "\n",
    "         # fetch feature form unprocessed data\n",
    "        column = original_data.iloc[:, loc]\n",
    "        feature_notnull = column.notnull()\n",
    "\n",
    "        # Count how many samples the feature appears in\n",
    "        feature_nsamples = feature_notnull.sum()\n",
    "        nsamples.append(feature_nsamples) \n",
    "\n",
    "        # Count how many groups the feature appears in\n",
    "        group_occurrence = {}        \n",
    "        for n, f in enumerate(feature_notnull):\n",
    "            if f == True:\n",
    "                group_occurrence[labels[n]] = 1\n",
    "        feature_ngroups = sum(list((group_occurrence.values())))\n",
    "        n_groups.append(feature_ngroups)\n",
    "\n",
    "        # Calculate the ratio of number of samples to number of groups the feature appears in\n",
    "        ratio.append(feature_nsamples/feature_ngroups)\n",
    "\n",
    "        # Store identification of feature\n",
    "        feature_locs.append(loc)\n",
    "\n",
    "    resdict['% top features explanation'] = s\n",
    "\n",
    "    resdict.update({'n samples': nsamples, 'n groups': n_groups, 'ratios':ratio, 'top features': feature_locs})\n",
    "\n",
    "for name, resdict in PLSDA_all.items():\n",
    "    compute_PLSDA_top_feat_stats(name, resdict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important Feature Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_PLSDA_stats = {'# samples': {name: np.median(PLSDA_all[name]['n samples']) for name in PLSDA_all},\n",
    "                  '# samples max': {name: np.max(PLSDA_all[name]['n samples']) for name in PLSDA_all},\n",
    "                  '# samples min': {name: np.min(PLSDA_all[name]['n samples']) for name in PLSDA_all},\n",
    "                  '# classes': {name: np.median(PLSDA_all[name]['n groups']) for name in PLSDA_all},\n",
    "                  '# classes max': {name: np.max(PLSDA_all[name]['n groups']) for name in PLSDA_all},\n",
    "                  '# classes min': {name: np.min(PLSDA_all[name]['n groups']) for name in PLSDA_all},\n",
    "               'samples/classes': {name: np.median(PLSDA_all[name]['ratios']) for name in PLSDA_all},\n",
    "               '# top 2%': {name: np.median(PLSDA_all[name]['number top features']) for name in PLSDA_all},\n",
    "               '% model explained': {name: np.median(PLSDA_all[name]['% top features explanation']) for name in PLSDA_all},\n",
    "               'top feat / average':{name: np.median(PLSDA_all[name]['top feature fold importance']) for name in PLSDA_all}}\n",
    "df_PLSDA_stats = pd.DataFrame(df_PLSDA_stats)\n",
    "\n",
    "df_PLSDA_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Occurences by samples and by classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_only = df_PLSDA_stats[['# samples', '# classes', 'samples/classes']]\n",
    "line = pd.DataFrame([], columns=dist_only.columns, index=[''])\n",
    "display_df = pd.concat([dist_only[dist_only.index.str.startswith('GD_neg_global2')], line, dist_only[dist_only.index.str.startswith('YD2')]])\n",
    "#display_df\n",
    "\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    with sns.plotting_context(\"notebook\", font_scale=1.2):\n",
    "        f, ax = plt.subplots(figsize=(6,9))\n",
    "        hm = sns.heatmap(display_df, annot=True, fmt='.1f', ax=ax, cmap = sns.cm.rocket_r)\n",
    "        bottom, top = ax.get_ylim()\n",
    "\n",
    "        plt.text(1.5,-0.5,'GDg2-',horizontalalignment='center', verticalalignment='center')\n",
    "        plt.text(1.5,7.5,'YD 6/15',horizontalalignment='center', verticalalignment='center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting swarmplots with the distributions of samples important features appear in\n",
    "\n",
    "#### Swarmplot for the GDg2- and YD 2/15 data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_PLSDA_stats[df_PLSDA_stats.index.str.startswith('GD_neg_global2') & ~df_PLSDA_stats.index.str.endswith('RF')].iloc[:, :6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_PLSDA_stats[df_PLSDA_stats.index.str.startswith('YD ') & ~df_PLSDA_stats.index.str.endswith('RF')].iloc[:, :6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treatments = ('P','NP','NGP','BinSim')\n",
    "\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    with sns.plotting_context(\"notebook\", font_scale=1.8):\n",
    "        f, (axl, axr) = plt.subplots(1,2, figsize=(14,5), constrained_layout=True)\n",
    "        colors = sns.color_palette('tab10', 4)\n",
    "        tl = ticker.FixedLocator([3, 6, 9, 12, 15, 18, 24, 30, 33])\n",
    "        \n",
    "        df_PLSDA_sample_dist = {}\n",
    "        for name, res in PLSDA_all.items():\n",
    "            if res['dskey'] == \"GD_neg_global2\" and res[\"treatment\"] in treatments:\n",
    "                df_PLSDA_sample_dist[name] = res['n samples']\n",
    "        \n",
    "        df_PLSDA_sample_dist = pd.DataFrame(df_PLSDA_sample_dist)\n",
    "        \n",
    "        sns.swarmplot(data=df_PLSDA_sample_dist, size=6, palette=colors, ax=axl)\n",
    "\n",
    "        axl.set(xticklabels = ['P','NP','NGP','BinSim'])\n",
    "        axl.set_ylabel('Number of samples')\n",
    "        for ticklabel, tickcolor in zip(axl.get_xticklabels(), colors):\n",
    "            ticklabel.set_color(tickcolor)\n",
    "        axl.yaxis.set_major_locator(tl)\n",
    "        for spine in axl.spines.values():\n",
    "            spine.set_edgecolor('0.1')\n",
    "\n",
    "        df_PLSDA_sample_dist = {}\n",
    "        for name, res in PLSDA_all.items():\n",
    "            if res['dskey'] == \"YD\" and res[\"treatment\"] in treatments:\n",
    "                df_PLSDA_sample_dist[name] = res['n samples']\n",
    "        \n",
    "        df_PLSDA_sample_dist = pd.DataFrame(df_PLSDA_sample_dist)\n",
    " \n",
    "        sns.swarmplot(data=df_PLSDA_sample_dist, size=6, palette=colors, ax=axr)\n",
    "\n",
    "        axr.set(xticklabels = ['P','NP','NGP','BinSim'])\n",
    "        #axr.set_ylabel('Number of samples')\n",
    "        for ticklabel, tickcolor in zip(axr.get_xticklabels(), colors):\n",
    "            ticklabel.set_color(tickcolor)\n",
    "        axr.yaxis.set_major_locator(tl)\n",
    "        for spine in axr.spines.values():\n",
    "            spine.set_edgecolor('0.1')\n",
    "\n",
    "        #title = \"\"\"Distribution of sample occurrence of the 2 % top features of Random Forest models\n",
    "        #GDg2-\"\"\"\n",
    "        title = ''\n",
    "\n",
    "        #f.suptitle(title, fontsize=14)\n",
    "        f.savefig('paperimages/fi_dist_PLSDA.png', dpi=600)\n",
    "        f.savefig('paperimages/fi_dist_PLSDA.pdf', dpi=200)\n",
    "        #plt.savefig('RF-NGD-IF')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different swarmplots for different data sets with different image parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style(\"whitegrid\"):\n",
    "    with sns.plotting_context(\"notebook\", font_scale=1.4):\n",
    "        f, (axl, axr) = plt.subplots(1, 2, figsize=(16,12.5))\n",
    "\n",
    "        colors = sns.color_palette('tab10', 4)\n",
    "        tl = ticker.FixedLocator([2, 3, 6, 12, 18, 24, 30, 33])\n",
    "        \n",
    "        to_plot = [name for name in PLSDA_all if PLSDA_all[name]['dskey']=='GD_neg_global2' and PLSDA_all[name]['treatment'] in treatments]\n",
    "        df_sample_dist = {PLSDA_all[name]['treatment']: PLSDA_all[name]['n samples'] for name in to_plot}\n",
    "        df_sample_dist = pd.DataFrame(df_sample_dist)\n",
    "        # Plot the swarm plot and adjust parameters\n",
    "        sns.swarmplot(data=df_sample_dist, size=3, palette=colors, orient='h', ax=axl)\n",
    "        title = \"Negative Grapevine Dataset\"\n",
    "        axl.set(xlabel='Nº of samples', title=title)\n",
    "        for ticklabel, tickcolor in zip(axl.get_yticklabels(), colors):\n",
    "            ticklabel.set_color(tickcolor)\n",
    "        axl.xaxis.set_major_locator(tl)\n",
    "        \n",
    "        to_plot = [name for name in PLSDA_all if PLSDA_all[name]['dskey']=='GD_pos_global2' and PLSDA_all[name]['treatment'] in treatments]\n",
    "        df_sample_dist = {PLSDA_all[name]['treatment']: PLSDA_all[name]['n samples'] for name in to_plot}\n",
    "        df_sample_dist = pd.DataFrame(df_sample_dist)\n",
    "        \n",
    "        sns.swarmplot(data=df_sample_dist, size=3, palette=colors, orient='h', ax=axr)\n",
    "        title = \"Positive Grapevine Dataset\"\n",
    "        axr.set(xlabel='Nº of samples', yticklabels = [], title=title)\n",
    "        axr.xaxis.set_major_locator(tl)\n",
    "\n",
    "        f.suptitle(\"Sample occurrence of top 2% important features of PLS-DA models\", fontsize=18)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_PLSDA_sample_dist = {name: PLSDA_all[name]['n samples'] for name in PLSDA_all if PLSDA_all[name]['dskey']=='YD' and PLSDA_all[name]['treatment'] in treatments}\n",
    "df_PLSDA_sample_dist = pd.DataFrame(df_PLSDA_sample_dist)\n",
    "# Plot the swarm plot and adjust parameters\n",
    "\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    with sns.plotting_context(\"notebook\", font_scale=1.5):\n",
    "        f, ax = plt.subplots(figsize=(8,8))\n",
    "\n",
    "        colors = sns.color_palette('tab10', 4)\n",
    "        sns.swarmplot(data=df_PLSDA_sample_dist, size=4, palette=colors)\n",
    "\n",
    "        ax.set(xticklabels = ['P','NP','NGP','BinSim'])\n",
    "        ax.set_ylabel('Number of samples')\n",
    "        for ticklabel, tickcolor in zip(ax.get_xticklabels(), colors):\n",
    "            ticklabel.set_color(tickcolor)\n",
    "        title = \"\"\"Distribution of sample occurrence of the 2 % top features of PLS-DA models\n",
    "        YD 2/15\"\"\"\n",
    "\n",
    "        #f.suptitle(title, fontsize=14)\n",
    "        #f.savefig('paperimages/fi_distYD_PLSDA.png', dpi=600)\n",
    "        #f.savefig('paperimages/fi_distYD_PLSDA.pdf', dpi=200)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_PLSDA_sample_dist = {name: PLSDA_all[name]['n samples'] for name in PLSDA_all if PLSDA_all[name]['dskey']=='vitis_types' and PLSDA_all[name]['treatment'] in treatments}\n",
    "df_PLSDA_sample_dist = pd.DataFrame(df_PLSDA_sample_dist)\n",
    "# Plot the swarm plot and adjust parameters\n",
    "\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    with sns.plotting_context(\"notebook\", font_scale=1.4):\n",
    "        f, ax = plt.subplots(figsize=(14,8))\n",
    "\n",
    "        colors = sns.color_palette('tab10', 4)\n",
    "        sns.swarmplot(data=df_PLSDA_sample_dist, size=4, palette=colors)\n",
    "\n",
    "        ax.set(xticklabels = ['P','NP','NGP','BinSim'])\n",
    "        ax.set_ylabel('Nº of samples')\n",
    "        for ticklabel, tickcolor in zip(ax.get_xticklabels(), colors):\n",
    "            ticklabel.set_color(tickcolor)\n",
    "        title = \"\"\"Distribution of sample occurrence of the 2 % top features of Random Forest models\n",
    "        GD, target is Vitis types\"\"\"\n",
    "\n",
    "        f.suptitle(title, fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overlaps of Features considered as Important to build each differently-treated data set - PLS-DA\n",
    "\n",
    "For each benchmark data set, 4 differently-treated data sets were obtained. These each have their own set of important features (top 2% chosen). The following cells compute the number of features in common between those sets (common in all 4, in each possible set of 3 and 2 and exclusive to only 1) for each benchmark data set.\n",
    "\n",
    "The features appearing in all 4 are not taken into account for counting the features that appear in each set of 3 or sets of 2, and the same for the features in sets of 3 not appearing in the counts for the sets of 2. This is made so all values can be directly put into Venn Diagram without the need for further calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we only want counts, let's use the list of locs for the important features\n",
    "features_as_sets = {name:{} for name in datasets}\n",
    "for name, plsdict in PLSDA_all.items():\n",
    "    if plsdict['treatment'] not in treatments:\n",
    "        continue\n",
    "    features_as_sets[plsdict['dskey']][plsdict['treatment']] = set(PLSDA_all[name]['top features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dskey in datasets:\n",
    "    print(f'\\nFeature overlap for dataset {datasets[dskey][\"name\"]} -----------------------')\n",
    "    features = features_as_sets[dskey]\n",
    "    counts = features_overlap(features)\n",
    "    print_overlaps(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of Sample Projection on the two most important Components/Latent Variables of PLS models built with the full dataset and sample representation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GDg2-, YD and GD types with NGP or BinSim pre-treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for projection\n",
    "def plot_PLS(principaldf, label_colors, components=(1,2), title=\"PCA\", ax=None):\n",
    "    \"Plot the projection of samples in the 2 main components of a PLS-DA model.\"\n",
    "    \n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    \n",
    "    loc_c1, loc_c2 = [c - 1 for c in components]\n",
    "    col_c1_name, col_c2_name = principaldf.columns[[loc_c1, loc_c2]]\n",
    "    \n",
    "    #ax.axis('equal')\n",
    "    ax.set_xlabel(f'{col_c1_name}')\n",
    "    ax.set_ylabel(f'{col_c2_name}')\n",
    "\n",
    "    unique_labels = principaldf['Label'].unique()\n",
    "\n",
    "    for lbl in unique_labels:\n",
    "        subset = principaldf[principaldf['Label']==lbl]\n",
    "        ax.scatter(subset[col_c1_name],\n",
    "                   subset[col_c2_name],\n",
    "                   s=50, color=label_colors[lbl], label=lbl)\n",
    "\n",
    "    #ax.legend(framealpha=1)\n",
    "    ax.set_title(title, fontsize=15)\n",
    "\n",
    "def plot_ellipses_PLS(principaldf, label_colors, components=(1,2),ax=None, q=None, nstd=2):\n",
    "    \"Plot the projection of samples in the 2 main components of a PLS-DA model.\"\n",
    "    \n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    \n",
    "    loc_c1, loc_c2 = [c - 1 for c in components]\n",
    "    points = principaldf.iloc[:, [loc_c1, loc_c2]]\n",
    "    \n",
    "    #ax.axis('equal')\n",
    "\n",
    "    unique_labels = principaldf['Label'].unique()\n",
    "\n",
    "    for lbl in unique_labels:\n",
    "        subset_points = points[principaldf['Label']==lbl]\n",
    "        plot_confidence_ellipse(subset_points, q, nstd, ax=ax, ec=label_colors[lbl], fc='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 11\n",
    "\n",
    "model, scores = ma.fit_PLSDA_model(datasets['GD_neg_class2']['NGP'],\n",
    "                                   datasets['GD_neg_class2']['target'], n_comp=n_components)\n",
    "model2, scores2 = ma.fit_PLSDA_model(datasets['GD_neg_class2']['BinSim'],\n",
    "                                     datasets['GD_neg_class2']['target'], n_comp=n_components)\n",
    "\n",
    "lcolors = datasets['GD_neg_class2']['label_colors']\n",
    "\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    with sns.plotting_context(\"notebook\", font_scale=1.2):\n",
    "        fig, (axl, axr) = plt.subplots(1,2, figsize=(14,7))\n",
    "        plot_PLS(scores, lcolors, title=\"Negative GD class2, NGP treatment\", ax=axl)\n",
    "        #plt.legend(loc='upper left', ncol=2)\n",
    "\n",
    "        plot_PLS(scores2, lcolors, title=\"Negative GD class2, BinSim treatment\", ax=axr)\n",
    "        axr.set_ylabel('')\n",
    "        axr.legend(loc='upper right', ncol=2)               \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 11\n",
    "\n",
    "model, scores = ma.fit_PLSDA_model(datasets['YD']['NGP'],\n",
    "                                   datasets['YD']['target'], n_comp=n_components)\n",
    "model2, scores2 = ma.fit_PLSDA_model(datasets['YD']['BinSim'],\n",
    "                                     datasets['YD']['target'], n_comp=n_components)\n",
    "\n",
    "lcolors = datasets['YD']['label_colors']\n",
    "\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    with sns.plotting_context(\"notebook\", font_scale=1.2):\n",
    "        fig, (axl, axr) = plt.subplots(1,2, figsize=(14,7))\n",
    "        plot_PLS(scores, lcolors, title=\"YD, NGP treatment\", ax=axl)\n",
    "        #plt.legend(loc='upper left', ncol=2)\n",
    "\n",
    "        plot_PLS(scores2, lcolors, title=\"YD, BinSim treatment\", ax=axr)\n",
    "        axr.set_ylabel('')\n",
    "        axr.legend(loc='upper left', ncol=2)               \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 11\n",
    "\n",
    "model, scores = ma.fit_PLSDA_model(datasets['vitis_types']['NGP'],\n",
    "                                   datasets['vitis_types']['target'], n_comp=n_components)\n",
    "model2, scores2 = ma.fit_PLSDA_model(datasets['vitis_types']['BinSim'],\n",
    "                                     datasets['vitis_types']['target'], n_comp=n_components)\n",
    "\n",
    "lcolors = datasets['vitis_types']['label_colors']\n",
    "\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    with sns.plotting_context(\"notebook\", font_scale=1.2):\n",
    "        fig, (axl, axr) = plt.subplots(1,2, figsize=(14,7))\n",
    "        plot_PLS(scores, lcolors, title=\"GD target is Vitis types, NGP treatment\", ax=axl)\n",
    "        #plt.legend(loc='upper left', ncol=2)\n",
    "\n",
    "        plot_PLS(scores2, lcolors, title=\"GD target is Vitis types, BinSim treatment\", ax=axr)\n",
    "        axr.set_ylabel('')\n",
    "        axr.legend(loc='upper right', ncol=1)               \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
