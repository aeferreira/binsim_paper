{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Simplification in Metabolomics\n",
    "\n",
    "Notebook to support the study on the application of **Bin**ary **Sim**plification as a competing form of pre-processing procedure for high-resolution metabolomics data.\n",
    "\n",
    "This is notebook `paper_binsim_dataset_HD.ipynb`\n",
    "\n",
    "\n",
    "## Organization of the Notebook\n",
    "\n",
    "- Read and Prepare dataset with human samples\n",
    "- Application of different pre-treatments (including BinSim) to this dataset\n",
    "- Add dataset to database of datasets\n",
    "- Create persistance storage of data including the latest dataset\n",
    "- Analyse dataset characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Needed Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from pathlib import Path\n",
    "\n",
    "# json for persistence\n",
    "import json\n",
    "from time import perf_counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.testing import assert_frame_equal\n",
    "\n",
    "import scipy.spatial.distance as dist\n",
    "import scipy.cluster.hierarchy as hier\n",
    "import scipy.stats as stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib import ticker\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "# Metabolinks package\n",
    "import metabolinks as mtl\n",
    "import metabolinks.transformations as transf\n",
    "\n",
    "# Python files in the repository\n",
    "import multianalysis as ma\n",
    "from elips import plot_confidence_ellipse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of dataset records\n",
    "\n",
    "`datasets` is the global dict that holds all data sets. It is a **dict of dict's**.\n",
    "\n",
    "Each data set is **represented as a dict**.\n",
    "\n",
    "Each record has the following fields (keys):\n",
    "\n",
    "- `name`: name of the data set (see below the list of data sets)\n",
    "- `source`: the biological source for each dataset\n",
    "- `mode`: the aquisition mode\n",
    "- `alignment`: the alignment used to generate the data matrix\n",
    "- `data`: the data matrix, the original data before any pre-treatments\n",
    "- `target`: the sample labels, possibly already integer encoded\n",
    "- `<treatment name>`: transformed data matrix. These treatment names can be\n",
    "    - `Ionly`: missing value imputation by 1/2 min, only\n",
    "    - `P`: Pareto scaled data, after missing value imputation\n",
    "    - `NP`: Missing value imputed, Pareto scaled and normalized\n",
    "    - `NGP`: Missing value imputed, normalized, glog transformed and Pareto scaled\n",
    "    - `BinSim`: binary simplified data\n",
    "\n",
    "The keys of `datasets` may be shared with dicts holding records resulting from comparison analysis.\n",
    "\n",
    "Here are the keys (and respective names) of datasets used in this study:\n",
    "\n",
    "- GD_neg_global2 (GDg2-)\n",
    "- GD_pos_global2 (GDg2+)\n",
    "- GD_neg_class2 (GDc2-)\n",
    "- GD_pos_class2 (GDc2+)\n",
    "- YD (YD 2/15)\n",
    "- YD2 (YD 6/15)\n",
    "- vitis_types (GD types)\n",
    "- HD (HD) **Added here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of the Human dataset\n",
    "\n",
    "Human Dataset - 249 samples belonging to 2 different classes of HILIC-MS data obtained in positive ionization mode. The Chromatography system was a Thermo Dionex Ultimate 3000 with the column being a Waters Xbridge BEH HILIC (75 x 2.1mm, 2.5um). Mass Spectrometry was performed in a Thermo Q Exactive HF hybrid Orbitrap operating in positive electrospray ionization mode. Further information in the data deposition site mentioned below.\n",
    "\n",
    "The 2 classes are:\n",
    "\n",
    "- 135 pre-operative blood (serum) samples from patients with **'No Recurrence'** of Prostate Cancer after Radical Prostatectomy\n",
    "- 114 pre-operative blood (serum) samples from patients with **'Recurrence'** of Prostate Cancer after Radical Prostatectomy\n",
    "\n",
    "I think this means samples were collected before the Prostatectomy before knowledge of recurrence or no recurrence? There was a total of 80 subjects.\n",
    "\n",
    "Data taken from data deposition site: https://www.metabolomicsworkbench.org/data/DRCCMetadata.php?Mode=Study&DataMode=AllData&StudyID=ST001082&StudyType=MS&ResultType=5#DataTabs\n",
    "\n",
    "This data is available at the NIH Common Fund's National Metabolomics Data Repository (NMDR) website, the Metabolomics Workbench, https://www.metabolomicsworkbench.org where it has been assigned Project ID PR000724. The data can be accessed directly via it's Project DOI: 10.21228/M83D5V. \n",
    "\n",
    "Data deposited by the Georgia Institute of Technology and researcher Facundo Fernandez. This data was taken already aligned in a 2D numerical data matrix. \n",
    "\n",
    "Data used in the paper: Clendinen CS, Gaul DA, Monge ME, et al. Preoperative Metabolic Signatures of Prostate Cancer Recurrence Following Radical  Prostatectomy. J Proteome Res. 2019;18(3):1316-1327. doi:10.1021/acs.jproteome.8b00926\n",
    "\n",
    "Dataset named **HD (HDg2-)** was generated after retaining only features that occur (globally) at least twice in all samples of the dataset. For the purpose of assessing the performance of supervised methods this dataset was used with target labels defining classes based on which of the 2 classes mentioned prior, the samples belonged to.\n",
    "\n",
    "\n",
    "The data read from the site seemed to be completely untreated, which was perfect for our efforts. We can't be sure but the presence of plenty of missing values (very useful for us) points to no great peak filtering and missing value imputation being made, the bar plot built from adding up all intensities of each sample seems to point that no normalization had yet been employed in the data, and the distribution observed in the boxplot of the intensities of some of the features in the samples seems to point that no scaling had been used yet in the data as well.\n",
    "\n",
    "The data also had 2 copies of each sample, where the 2nd copies were equal to the first ones multiplied by a constant. This constant was different for each sample. We removed the presence of the 2nd copies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Human dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the dataset, this dataset has two copies of the samples.The 2nd copy is equal to the 1st multiplied by a constant\n",
    "# unique to each sample.\n",
    "human_datamatrix_base = pd.read_excel('ST001082_AN001766_498recurrence.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_datamatrix = human_datamatrix_base.iloc[:-1, :-4] # Just select the rows corresponding to the dataset\n",
    "\n",
    "human_datamatrix = human_datamatrix.set_index(human_datamatrix.columns[0]) # Set index as the metabolites name\n",
    "human_datamatrix = human_datamatrix.replace({0:np.nan}) # Replacing 0 values as missing values\n",
    "\n",
    "human_datamatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select one of the two copies of samples in the dataset by removing the samples ending with '.1'\n",
    "human_datamatrix = human_datamatrix[[i for i in human_datamatrix.columns if not i.endswith('.1')]]\n",
    "\n",
    "human_datamatrix = human_datamatrix.T # Transpose the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current sample labels. Sample will consist of 'Sample Type:No Recurrence' and 'Sample Type:Recurrence'\n",
    "hd_labels = list(human_datamatrix['Factors'])\n",
    "set(hd_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# How many 'No Recurrence' samples in the dataset\n",
    "hd_labels.count('Sample Type:No Recurrence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_datamatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blank Treatment\n",
    "\n",
    "- If blank_treatment = True:\n",
    "\n",
    "An average of the blanks will be subtracted to the remaining dataset. Missing values in the blanks were replaced by 0 to calculate the average of the blanks. Negative values that arise in the dataset from subtracting the blanks will be coded as 0/missing values in the dataset. With the BinSim pre-treatment, they will be coded as 0.\n",
    "\n",
    "Possible problem for BinSim: features that are very similar between samples might be slightly negative in one and be coded as 0 and slightly positive in the other and be coded as 1, despite being basically identical.\n",
    "\n",
    "- If blank_treatment = False:\n",
    "\n",
    "Blank samples are removed and not accounted for. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blank_treatment = True\n",
    "#blank_treatment = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if blank_treatment:\n",
    "    blanks = human_datamatrix[human_datamatrix['Factors'] == 'Sample Type:Blank'].iloc[:,1:] # Select blank samples\n",
    "    blanks = blanks.replace({np.nan:0}) \n",
    "    blanks = blanks.astype(float) # Get blank samples with floats (needed since datamatrix has strings)\n",
    "    blanks_average = blanks.mean() # Average of the blanks\n",
    "    blanks_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the samples belonging to either the 'No Recurrence' or 'Recurrence' \n",
    "selection = []\n",
    "for i in human_datamatrix.loc[:, 'Factors']:\n",
    "    if i in ['Sample Type:No Recurrence', 'Sample Type:Recurrence']:\n",
    "        selection.append(True)\n",
    "    else:\n",
    "        selection.append(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_datamatrix = human_datamatrix[selection]\n",
    "# Creating the list of 'targets' (labels of samples) of the dataset with the 'No Recurrence' and 'Recurrence' classes \n",
    "hd_labels = []\n",
    "for i in list(human_datamatrix.iloc[:,0]):\n",
    "    if i == 'Sample Type:No Recurrence':\n",
    "        hd_labels.append('No Recurrence')\n",
    "    else:\n",
    "        hd_labels.append('Recurrence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_datamatrix = human_datamatrix.iloc[:,1:]\n",
    "human_datamatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_datamatrix = human_datamatrix.astype(float) # Passing the values from strings to floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if blank_treatment:\n",
    "    human_datamatrix = human_datamatrix.replace({np.nan:0}) - blanks_average\n",
    "    human_datamatrix[human_datamatrix<0] = 0\n",
    "    human_datamatrix = human_datamatrix.replace({0:np.nan})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_datamatrix = transf.keep_atleast(human_datamatrix, minimum=2) # Keep features that appear in at least two samples\n",
    "human_datamatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#human_datamatrix.replace({np.nan:1}).select_dtypes(exclude=[\"float\", 'int'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building human dataset with format of the dataset database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {}\n",
    "\n",
    "# HD (HDg2-)\n",
    "datasets['HD'] = {'source': 'human',\n",
    "                  'alignment': '1-2',\n",
    "                  'mode': '-',\n",
    "                  'name': 'HD',\n",
    "                  'data': human_datamatrix,\n",
    "                  'target': hd_labels,\n",
    "                  'classes': list(pd.unique(hd_labels))}\n",
    "\n",
    "print('target for human 2-class dataset')\n",
    "print(datasets['HD']['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets['HD']['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots comparing the sum of all peaks between samples and between-feature variation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing Total Intensity of Features (Sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sum_feat = datasets['HD']['data'].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with plt.style.context('seaborn-darkgrid'):\n",
    "    fig, ax = plt.subplots(figsize=(16,6))\n",
    "    \n",
    "    values = list(total_sum_feat)\n",
    "    x = np.arange(len(datasets['HD']['target']))  # the label locations\n",
    "    width = 0.30  # the width of the bars\n",
    "\n",
    "    ax.bar(x, values, width, ec='darkblue')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_ylabel('Total Feature Intensity', fontsize = 15)\n",
    "    ax.set_xlabel('Samples', fontsize = 15)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=5)\n",
    "    ax.get_xaxis().set_ticks([])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing Intensity in Features with a boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16,6))\n",
    "box_p = datasets['HD']['data'].iloc[:,:100].boxplot(ax=ax)\n",
    "ax.get_xaxis().set_ticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colors for plots to ensure consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 HD classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customize label colors for 2 HD classes\n",
    "\n",
    "colours = sns.color_palette('Set1', 5)\n",
    "hd_label_colors = {lbl: c for lbl, c in zip(datasets['HD']['classes'], colours)}\n",
    "datasets['HD']['label_colors'] = hd_label_colors\n",
    "datasets['HD']['sample_colors'] = [hd_label_colors[lbl] for lbl in datasets['HD']['target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.palplot(hd_label_colors.values())\n",
    "new_ticks = plt.xticks(range(len(datasets['HD']['classes'])), datasets['HD']['classes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Samples and respective target labels of each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def styled_sample_labels(sample_names, sample_labels, label_colors):\n",
    "\n",
    "    meta_table = pd.DataFrame({'label': sample_labels,\n",
    "                               'sample': sample_names}).set_index('sample').T\n",
    "\n",
    "    def apply_label_color(val):\n",
    "        red, green, blue = label_colors[val]\n",
    "        red, green, blue = int(red*255), int(green*255), int(blue*255)   \n",
    "        hexcode = '#%02x%02x%02x' % (red, green, blue)\n",
    "        css = f'background-color: {hexcode}'\n",
    "        return css\n",
    "    \n",
    "    return meta_table.style.applymap(apply_label_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed = mtl.parse_data(datasets['HD']['data'])\n",
    "y = datasets['HD']['target']\n",
    "label_colors = datasets['HD']['label_colors']\n",
    "s = styled_sample_labels(parsed.sample_names, y, label_colors)\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data transformations and pre-treatments\n",
    "\n",
    "Each data set is transformed by Binary simplification and missing values are imputed by two different methods and then treated by three combinations of more established treatments (generating 7 datasets with different treatments applied).\n",
    "\n",
    "### Traditional and more established  intensity-based Pre-Treatments\n",
    "\n",
    "Missing value imputation is mandatory for the data sets since many statistical methods performed downstream can't handle missing values. \n",
    "\n",
    "Two missing value imputation procedures were applied before any intensity-based pre-treatments: \n",
    "\n",
    "- Missing values were replaced with half of the minimum intensity value present in the whole data matrix - Limit of Detection type of imputation commonly applied in metabolomics data analysis. This was performed with the `fillna_frac_min`function from metabolinks. \n",
    "\n",
    "- Missing values were imputed by Random Forest missing value imputation. Parameters: 10 trees, 100 nearest features considered, 0 minimum value, 1.0e10 maximum value and 2 max iterations, others were left as default. This was performed by using the scikit-learn Python module with `IterativeImputer`and `ExtraTreesRegressor` as estimator. In cases where this method was used, an extra **`_RF`** was added to the 'name' of the pre-treatment.\n",
    "\n",
    "#### Combinations of intensity-based pre-treatments made:\n",
    "\n",
    "- Ionly Treatment - Only Missing Value Imputation.\n",
    "\n",
    "- **P Treatment** - Missing Value Imputation and Pareto Scaling.\n",
    "\n",
    "- **NP Treatment** - Missing Value Imputation, Normalization by reference feature (Leucine Enkephalin) and Pareto Scaling.\n",
    "\n",
    "- **NGP Treatment** - Missing Value Imputation, Normalization by reference feature (Leucine Enkephalin), Generalized Logarithmic Transformation and Pareto Scaling.\n",
    "\n",
    "Note: Leucine Enkephalin peak (reference feature) is removed upon normalization. Order of pre-treatments is the order in which they were mentioned.\n",
    "\n",
    "### Binary Simplification (BinSim)\n",
    "\n",
    "- **BinSim Treatment** - `df_to_bool` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_RF(df, nearest_features=100, n_trees=50):\n",
    "    rf_estimator = ExtraTreesRegressor(n_estimators=n_trees)\n",
    "    imputer = IterativeImputer(random_state=0, estimator=rf_estimator,\n",
    "                           n_nearest_features=nearest_features,\n",
    "                           min_value=0.0, max_value=1.0e10,\n",
    "                           max_iter=2,\n",
    "                           verbose=2)\n",
    "    imputed_data = imputer.fit_transform(df)\n",
    "    res = pd.DataFrame(imputed_data, index=df.index, columns=df.columns)\n",
    "    ncols = len(res.columns)\n",
    "    res = res.dropna(axis='columns', how='any')\n",
    "    ncols2 = len(res.columns)\n",
    "    if ncols > ncols2:\n",
    "        print(f'{ncols-ncols2} features dropped')\n",
    "    return res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Represents Binary Simplification pre-treatment\n",
    "def df_to_bool(df):\n",
    "    \"Transforms data into 'binary' matrices.\"\n",
    "    return df.mask(df.notnull(), 1).mask(df.isnull(), 0)\n",
    "\n",
    "# Performs all pre-treatment combinations mentioned\n",
    "def compute_transf(dataset, lamb=None):\n",
    "    \"Computes 3 combinations of pre-treatments and BinSim and returns after treatment datasets in a dict.\"\n",
    "    \n",
    "    data = dataset['data']\n",
    "    \n",
    "    # Imputation of Missing Values\n",
    "    imputed = transf.fillna_frac_min(data, fraction=0.5)\n",
    "    \n",
    "    # Imputation by RF\n",
    "    imputedRF = impute_RF(data, nearest_features=100, n_trees=10)\n",
    "    \n",
    "    \n",
    "    # Normalization by the total sum of peak areas\n",
    "    #norm = transf.normalize_sum(imputed)\n",
    "    #normRF = transf.normalize_sum(imputedRF)\n",
    "    \n",
    "    # Normalization by PQN\n",
    "    norm = transf.normalize_PQN(imputed, ref_sample='mean')\n",
    "    normRF = transf.normalize_PQN(imputedRF, ref_sample='mean')\n",
    "    \n",
    "    # Normalization by a reference feature RF\n",
    "    #if norm_ref is not None:\n",
    "    #    normRF = transf.normalize_ref_feature(imputedRF, norm_ref, remove=True)\n",
    "    #else:\n",
    "    #    normRF = imputedRF\n",
    "        \n",
    "    # Normalization by a reference feature\n",
    "    #if norm_ref is not None:\n",
    "    #    norm = transf.normalize_ref_feature(imputed, norm_ref, remove=True)\n",
    "    #else:\n",
    "    #    norm = imputed\n",
    "    \n",
    "    # Normalization by a reference feature RF\n",
    "    #if norm_ref is not None:\n",
    "    #    normRF = transf.normalize_ref_feature(imputedRF, norm_ref, remove=True)\n",
    "    #else:\n",
    "    #    normRF = imputedRF\n",
    "    \n",
    "    # Pareto Scaling and Generalized Logarithmic Transformation\n",
    "    P = transf.pareto_scale(imputed)\n",
    "    NP = transf.pareto_scale(norm)\n",
    "    NGP = transf.pareto_scale(transf.glog(norm, lamb=lamb))\n",
    "\n",
    "    # Pareto Scaling and Generalized Logarithmic Transformation\n",
    "    P_RF = transf.pareto_scale(imputedRF)\n",
    "    NP_RF = transf.pareto_scale(normRF)\n",
    "    NGP_RF = transf.pareto_scale(transf.glog(normRF, lamb=lamb))\n",
    "    \n",
    "    # Store results\n",
    "    dataset['BinSim'] = df_to_bool(data)\n",
    "    dataset['Ionly'] = imputed\n",
    "    dataset['P'] = P\n",
    "    dataset['NP'] = NP\n",
    "    dataset['NGP'] = NGP\n",
    "    \n",
    "    dataset['Ionly_RF'] = imputedRF\n",
    "    dataset['P_RF'] = P_RF\n",
    "    dataset['NP_RF'] = NP_RF\n",
    "    dataset['NGP_RF'] = NGP_RF    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Apply the different pre-treatments and get the results in their respective dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GENERATE:\n",
    "    for name, ds in datasets.items():\n",
    "        dataset_name = datasets[name][\"name\"]\n",
    "        print(f'Applying pre-processing transformations to data in {dataset_name}', end=' ...')\n",
    "        start = perf_counter()\n",
    "\n",
    "        compute_transf(ds)\n",
    "\n",
    "        end = perf_counter()\n",
    "\n",
    "        print(f'done! took {(end - start):.3f} s')\n",
    "    \n",
    "    hd_datasets = datasets\n",
    "    # ensure dir exists\n",
    "    path = Path.cwd() / \"paperimages\"\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    storepath = Path.cwd() / \"paperimages\" / 'processed_data_HD.h5'\n",
    "\n",
    "    store = pd.HDFStore(storepath, complevel=9, complib=\"blosc:blosclz\")\n",
    "    #pd.set_option('io.hdf.default_format','table')\n",
    "\n",
    "    # keep json serializable values and store dataFrames in HDF store\n",
    "\n",
    "    serializable = {}\n",
    "\n",
    "    for dskey, dataset in datasets.items():\n",
    "        if dskey != 'HD':\n",
    "            continue\n",
    "        serializable[dskey] = {}\n",
    "        for key, value in dataset.items():\n",
    "            #print(dskey, key)\n",
    "            if isinstance(value, pd.DataFrame):\n",
    "                storekey = dskey + '_' + key\n",
    "                #print('-----', storekey)\n",
    "                store[storekey] = value\n",
    "                serializable[dskey][key] = f\"INSTORE_{storekey}\"\n",
    "            else:\n",
    "                serializable[dskey][key] = value\n",
    "    store.close()\n",
    "            \n",
    "\n",
    "    path = path / 'processed_data_HD.json'\n",
    "    with open(path, \"w\", encoding='utf8') as write_file:\n",
    "        json.dump(serializable, write_file)\n",
    "\n",
    "    #serializable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading back the other benchmark datasets and adding HD to them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path.cwd() / \"paperimages\" / 'processed_data.json'\n",
    "storepath = Path.cwd() / \"paperimages\" / 'processed_data.h5'\n",
    "with pd.HDFStore(storepath) as store:\n",
    "\n",
    "    with open(path, encoding='utf8') as read_file:\n",
    "        datasets = json.load(read_file)\n",
    "    \n",
    "    for dskey, dataset in datasets.items():\n",
    "        for key in dataset:\n",
    "            value = dataset[key]\n",
    "            if isinstance(value, str) and value.startswith(\"INSTORE\"):\n",
    "                storekey = value.split(\"_\", 1)[1]\n",
    "                dataset[key] = store[storekey]\n",
    "            # transform colors, saved as lists in json, back into tuples\n",
    "            elif key == 'label_colors':\n",
    "                dataset[key] = {lbl: tuple(c) for lbl, c in value.items()}\n",
    "            elif key == 'sample_colors':\n",
    "                dataset[key] = [tuple(c) for c in value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not GENERATE:\n",
    "\n",
    "    path = Path.cwd() / \"paperimages\" / 'processed_data_HD.json'\n",
    "    storepath = Path.cwd() / \"paperimages\" / 'processed_data_HD.h5'\n",
    "    with pd.HDFStore(storepath) as store:\n",
    "\n",
    "        with open(path, encoding='utf8') as read_file:\n",
    "            hd_datasets = json.load(read_file)\n",
    "\n",
    "        for dskey, dataset in hd_datasets.items():\n",
    "            for key in dataset:\n",
    "                value = dataset[key]\n",
    "                if isinstance(value, str) and value.startswith(\"INSTORE\"):\n",
    "                    storekey = value.split(\"_\", 1)[1]\n",
    "                    dataset[key] = store[storekey]\n",
    "                # transform colors, saved as lists in json, back into tuples\n",
    "                elif key == 'label_colors':\n",
    "                    dataset[key] = {lbl: tuple(c) for lbl, c in value.items()}\n",
    "                elif key == 'sample_colors':\n",
    "                    dataset[key] = [tuple(c) for c in value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets['HD'] = hd_datasets['HD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_colors = datasets['GD_neg_class2']['label_colors']\n",
    "sns.palplot(label_colors.values())\n",
    "new_ticks = plt.xticks(range(len(label_colors)), label_colors.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_colors = datasets['YD']['label_colors']\n",
    "sns.palplot(label_colors.values())\n",
    "new_ticks = plt.xticks(range(len(label_colors)), label_colors.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_colors = datasets['vitis_types']['label_colors']\n",
    "sns.palplot(label_colors.values())\n",
    "new_ticks = plt.xticks(range(len(label_colors)), label_colors.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_colors = datasets['HD']['label_colors']\n",
    "sns.palplot(label_colors.values())\n",
    "new_ticks = plt.xticks(range(len(label_colors)), label_colors.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed = mtl.parse_data(datasets['GD_pos_class2']['data'], labels_loc='label')\n",
    "y = datasets['GD_pos_class2']['target']\n",
    "label_colors = datasets['GD_pos_class2']['label_colors']\n",
    "s = styled_sample_labels(parsed.sample_names, y, label_colors)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed = mtl.parse_data(datasets['vitis_types']['data'], labels_loc='label')\n",
    "y = datasets['vitis_types']['target']\n",
    "label_colors = datasets['vitis_types']['label_colors']\n",
    "s = styled_sample_labels(parsed.sample_names, y, label_colors)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed = mtl.parse_data(datasets['YD']['data'])\n",
    "y = datasets['YD']['target']\n",
    "label_colors = datasets['YD']['label_colors']\n",
    "s = styled_sample_labels(parsed.sample_names, y, label_colors)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed = mtl.parse_data(datasets['HD']['data'])\n",
    "y = datasets['HD']['target']\n",
    "label_colors = datasets['HD']['label_colors']\n",
    "s = styled_sample_labels(parsed.sample_names, y, label_colors)\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Characteristics\n",
    "\n",
    "Building a table with general characteristics about the 8 datasets studied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def characterize_dataset(dskey, ds):\n",
    "    \"Computes and returns some general characteristics of a dataset in a dictionary.\"\n",
    "\n",
    "    dataset_chrs = {}\n",
    "    \n",
    "    name = ds['name'] # Name of the dataset\n",
    "    n_samples, n_feats = ds['data'].shape\n",
    "    n_classes = len(ds['classes'])\n",
    "       \n",
    "    Feat_Sample = ds['data'].count(axis=1) # Nº Features in each sample\n",
    "    Min_Feat_Sample = str(Feat_Sample.min()) # Minimum nº Features in a sample\n",
    "    Max_Feat_Sample = str(Feat_Sample.max()) # Maximum nº Features in a sample\n",
    "    Average_Feat_Sample = Feat_Sample.mean() # Average nº Features in a sample\n",
    "    \n",
    "    avg_feat_per_sample = int(round(Average_Feat_Sample,0)) # Round\n",
    "    \n",
    "    Samp_Class = len(ds['target'])/len(ds['classes']) # Nº Sample per Class\n",
    "    if dskey == 'vitis_types':\n",
    "        Samp_Class = '15 Vitis vinifera, 18 Wild'\n",
    "        #Samp_Class = '15 $\\it{Vitis}$ $\\it{Vinifera}$, 18 Wild'\n",
    "    elif dskey == 'HD':\n",
    "        Samp_Class = '114 Recurrence, 135 no Recurrence'\n",
    "    else:\n",
    "        Samp_Class = str(int(Samp_Class))\n",
    "    \n",
    "    n_na = ds['data'].isna().sum().sum() # Nº of missing values in the dataset\n",
    "    \n",
    "    p_na = round(100.0 * n_na / (n_samples * n_feats), 2) # % of missing values in the dataset\n",
    "    \n",
    "    avg_na_per_feature = (n_samples - ds['data'].count(axis=0)).mean()\n",
    "    avg_na_per_feature = int(round(avg_na_per_feature, 0))\n",
    "    \n",
    "    return {'Data set': name,\n",
    "            '# samples': n_samples,\n",
    "            '# features': n_feats,\n",
    "            'features / sample (range)': f'{avg_feat_per_sample} ({Min_Feat_Sample}-{Max_Feat_Sample})',\n",
    "            '# classes': n_classes,\n",
    "            'samples / class':Samp_Class,\n",
    "            '% missing values': p_na,} \n",
    "            #'missing values / feature': avg_na_per_feature}\n",
    "\n",
    "data_characteristics = [characterize_dataset(dskey, ds) for dskey, ds in datasets.items()]\n",
    "data_characteristics = pd.DataFrame(data_characteristics).set_index('Data set')\n",
    "data_characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_characteristics.to_excel('paperimages/dataset_characteristics.xlsx', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA scores plots for the 8 datasets\n",
    "\n",
    "Representation of the samples when projected in the 2 Principal Components obtained from PCA.\n",
    "\n",
    "Preliminary assessment of the extent of class’s proximity, and consequent degree of difficulty for clustering and classification methods. Greater proximity/overlap would mean a more difficult task for the methods since it would mean the classes are similar to each other or less well defined.\n",
    "\n",
    "Ellipses shown are 95% confidence ellipses for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To consider: Maybe remove this function? No longer used\n",
    "def plot_PCA_old(principaldf, label_colors, label_symbols=None, components=(1,2), var_explained=None, title=\"PCA\", ax=None):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    \n",
    "    loc_c1, loc_c2 = [c - 1 for c in components]\n",
    "    col_c1_name, col_c2_name = principaldf.columns[[loc_c1, loc_c2]]\n",
    "    \n",
    "    #with sns.axes_style(\"whitegrid\"):\n",
    "    ax.axis('equal')\n",
    "    if var_explained is not None:\n",
    "        v1, v2 = var_explained[loc_c1], var_explained[loc_c2]\n",
    "        ax.set_xlabel(f'{col_c1_name} ({100*v1:.1f}%)', fontsize = 15)\n",
    "        ax.set_ylabel(f'{col_c2_name} ({100*v2:.1f}%)', fontsize = 15)\n",
    "    else:\n",
    "        ax.set_xlabel(f'{col_c1_name}', fontsize = 15)\n",
    "        ax.set_ylabel(f'{col_c2_name}', fontsize = 15)\n",
    "\n",
    "    unique_labels = principaldf['Label'].unique()\n",
    "    if label_symbols is None:\n",
    "        label_symbols = {lbl: 'o' for lbl in unique_labels}\n",
    "\n",
    "    lbl_handles = {}\n",
    "    for lbl in unique_labels:\n",
    "        subset = principaldf[principaldf['Label']==lbl]\n",
    "        hndl = ax.scatter(subset[col_c1_name], subset[col_c2_name],\n",
    "                          lw=1,ec='black', alpha=0.8,\n",
    "                          marker=label_symbols[lbl], \n",
    "                          s=80, color=label_colors[lbl], label=lbl)\n",
    "        lbl_handles[lbl] = hndl\n",
    "\n",
    "    #ax.legend(framealpha=1)\n",
    "    ax.set_title(title, fontsize=15)\n",
    "    return lbl_handles\n",
    "\n",
    "def plot_PCA(principaldf, label_colors, components=(1,2), title=\"PCA\", ax=None):\n",
    "    \"Plot the projection of samples in the 2 main components of a PCA model.\"\n",
    "    \n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    \n",
    "    loc_c1, loc_c2 = [c - 1 for c in components]\n",
    "    col_c1_name, col_c2_name = principaldf.columns[[loc_c1, loc_c2]]\n",
    "    \n",
    "    #ax.axis('equal')\n",
    "    ax.set_xlabel(f'{col_c1_name}')\n",
    "    ax.set_ylabel(f'{col_c2_name}')\n",
    "\n",
    "    unique_labels = principaldf['Label'].unique()\n",
    "\n",
    "    for lbl in unique_labels:\n",
    "        subset = principaldf[principaldf['Label']==lbl]\n",
    "        ax.scatter(subset[col_c1_name],\n",
    "                   subset[col_c2_name],\n",
    "                   s=50, color=label_colors[lbl], label=lbl)\n",
    "\n",
    "    #ax.legend(framealpha=1)\n",
    "    ax.set_title(title, fontsize=15)\n",
    "\n",
    "def plot_ellipses_PCA(principaldf, label_colors, components=(1,2),ax=None, q=None, nstd=2):\n",
    "    \"Plot confidence ellipses of a class' samples based on their projection in the 2 main components of a PCA model.\"\n",
    "    \n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    \n",
    "    loc_c1, loc_c2 = [c - 1 for c in components]\n",
    "    points = principaldf.iloc[:, [loc_c1, loc_c2]]\n",
    "    \n",
    "    #ax.axis('equal')\n",
    "\n",
    "    unique_labels = principaldf['Label'].unique()\n",
    "\n",
    "    for lbl in unique_labels:\n",
    "        subset_points = points[principaldf['Label']==lbl]\n",
    "        plot_confidence_ellipse(subset_points, q, nstd, ax=ax, ec=label_colors[lbl], fc='none')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style(\"whitegrid\"):\n",
    "    with sns.plotting_context(\"notebook\", font_scale=1):\n",
    "        f, axs = plt.subplots(3,3, figsize=(12,12), constrained_layout=True)\n",
    "\n",
    "        for (dskey, ds), ax in zip(datasets.items(), axs.ravel()):\n",
    "            df = datasets[dskey]['Ionly']\n",
    "            tf = transf.FeatureScaler(method='standard')\n",
    "            df = tf.fit_transform(df)\n",
    "            #print(df)\n",
    "            ax.axis('equal')\n",
    "            principaldf = ma.compute_df_with_PCs(df, n_components=5, whiten=True, labels=datasets[dskey]['target'], return_var_ratios=False)\n",
    "\n",
    "            lcolors = datasets[dskey]['label_colors']\n",
    "            #plot_PCA(principaldf, lcolors, components=(1,2), title=datasets[dskey]['name'], ax=ax)\n",
    "            plot_PCA(principaldf, lcolors, components=(1,2), title='', ax=ax)\n",
    "            plot_ellipses_PCA(principaldf, lcolors, components=(1,2),ax=ax, q=0.95)\n",
    "\n",
    "        #axs[2][1].remove()\n",
    "        axs[2][2].remove()\n",
    "        \n",
    "        \n",
    "        #axs[0][1].legend(loc='upper right', ncol=1)\n",
    "        #axs[1][1].legend(loc='upper right', ncol=1)\n",
    "        axs[2][0].legend(loc='upper center', ncol=1, framealpha=1)\n",
    "        axs[2][1].legend(loc='upper center', ncol=1, framealpha=1)\n",
    "        axs[2][1].set_xlim(-3, 3)\n",
    "        axs[2][1].set_ylim(-2, 2)\n",
    "        \n",
    "        locs_YD = {'WT':(-1,-0.65),\n",
    "                   'ΔGRE3':(-0.45, 1.45),\n",
    "                   'ΔENO1':(0.7, 0.1),\n",
    "                   'ΔGLO1':(0.5, -0.5),\n",
    "                   'ΔGLO2':(0,0.7) }\n",
    "        for lbl in datasets['YD']['classes']:\n",
    "            axs[1][1].text(*locs_YD[lbl], lbl, c=datasets['YD']['label_colors'][lbl])\n",
    "        for lbl in datasets['YD']['classes']:\n",
    "            axs[1][2].text(*locs_YD[lbl], lbl, c=datasets['YD']['label_colors'][lbl])\n",
    "\n",
    "        locs_GD = {'CAN':(-1.4,-0.2),\n",
    "                       'CS':(-0.45, 2),\n",
    "                       'LAB':(-0.25, -0.2),\n",
    "                       'PN':(-1, 0.2),\n",
    "                       'REG':(1.8,0.5),\n",
    "                       'RIP':(-0.3,-0.7),\n",
    "                       'RL':(-0.5, 1.45),\n",
    "                       'ROT':(-1, -1.2),\n",
    "                       'RU':(0.5, -1),\n",
    "                       'SYL':(-0.2,0),\n",
    "                       'TRI':(0.5,0.5),}\n",
    "        \n",
    "        for lbl in datasets['GD_neg_global2']['classes']:\n",
    "            axs[0][0].text(*locs_GD[lbl], lbl, c=datasets['GD_neg_global2']['label_colors'][lbl])\n",
    "\n",
    "        locs_GD = {'CAN':(-0.1,-1),\n",
    "                       'CS':(-0.45, 2),\n",
    "                       'LAB':(-0.2, -0.5),\n",
    "                       'PN':(-1, 0.6),\n",
    "                       'REG':(1.8,0.4),\n",
    "                       'RIP':(-0.3,-1.7),\n",
    "                       'RL':(-0.2, 1),\n",
    "                       'ROT':(-1, -1.2),\n",
    "                       'RU':(0.9, -1),\n",
    "                       'SYL':(-1.2,-0.2),\n",
    "                       'TRI':(0.5,0.1),}\n",
    "        \n",
    "        for lbl in datasets['GD_neg_global2']['classes']:\n",
    "            axs[0][2].text(*locs_GD[lbl], lbl, c=datasets['GD_neg_global2']['label_colors'][lbl])\n",
    "\n",
    "        locs_GD = {'CAN':(-0.25,0),\n",
    "                       'CS':(-0.7, -0.6),\n",
    "                       'LAB':(-1.2, 0),\n",
    "                       'PN':(0.5, 3),\n",
    "                       'REG':(2,-0.3),\n",
    "                       'RIP':(-0.25,-0.3),\n",
    "                       'RL':(-0.3, -0.6),\n",
    "                       'ROT':(-1.2,-0.6),\n",
    "                       'RU':(1, 0),\n",
    "                       'SYL':(-0.75,0),\n",
    "                       'TRI':(0.5,-0.5),}\n",
    "        \n",
    "        for lbl in datasets['GD_neg_global2']['classes']:\n",
    "            axs[1][0].text(*locs_GD[lbl], lbl, c=datasets['GD_neg_global2']['label_colors'][lbl])\n",
    "        \n",
    "        locs_GD = {'CAN':(-0.25,0),\n",
    "                       'CS':(-0.7, -0.6),\n",
    "                       'LAB':(-1.2, 0),\n",
    "                       'PN':(0.5, 3),\n",
    "                       'REG':(2,-0.4),\n",
    "                       'RIP':(-0.25,-0.3),\n",
    "                       'RL':(-0.3, -0.6),\n",
    "                       'ROT':(-1.2,-0.6),\n",
    "                       'RU':(1, 0.2),\n",
    "                       'SYL':(-0.75,0),\n",
    "                       'TRI':(0.5,-0.4),}\n",
    "        \n",
    "        for lbl in datasets['GD_neg_global2']['classes']:\n",
    "            axs[0][1].text(*locs_GD[lbl], lbl, c=datasets['GD_neg_global2']['label_colors'][lbl])\n",
    "        \n",
    "        for letter, ax in zip('ABCDEFGHIJ', axs.ravel()[0:8]):\n",
    "            ax.text(0.88, 0.9, letter, ha='left', va='center', fontsize=15, weight='bold',\n",
    "                    transform=ax.transAxes,\n",
    "                    bbox=dict(facecolor='white', alpha=0.9))\n",
    "        \n",
    "        plt.show()\n",
    "        #f.savefig('paperimages/PCAs.pdf', dpi=200)\n",
    "        #f.savefig('paperimages/PCAs.png', dpi=600)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Back to HD data only, to try the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = hd_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_HCA(df, metric='euclidean', method='average'):\n",
    "    \"Performs Hierarchical Clustering Analysis of a data set with chosen linkage method and distance metric.\"\n",
    "    print(metric)\n",
    "    \n",
    "    distances = dist.pdist(df, metric=metric)\n",
    "    print(np.all(np.isfinite(distances)))\n",
    "    \n",
    "    # method is one of\n",
    "    # ward, average, centroid, single, complete, weighted, median\n",
    "    Z = hier.linkage(distances, method=method)\n",
    "\n",
    "    # Cophenetic Correlation Coefficient\n",
    "    # (see how the clustering - from hier.linkage - preserves the original distances)\n",
    "    coph = hier.cophenet(Z, distances)\n",
    "    # Baker's gamma\n",
    "    mr = ma.mergerank(Z)\n",
    "    bg = mr[mr!=0]\n",
    "\n",
    "    return {'Z': Z, 'distances': distances, 'coph': coph, 'merge_rank': mr, \"Baker's Gamma\": bg}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionaries to contain results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HCA_all = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the clusterings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, ds in datasets.items():\n",
    "    HCA_all[name] = {}\n",
    "    for treat in 'P', 'NP', 'NGP', 'BinSim', 'P_RF', 'NP_RF', 'NGP_RF':\n",
    "        print(f'Performing HCA to {name} data set with treatment {treat}', end=' ...')\n",
    "        metric = 'jaccard' if treat == 'BinSim' else 'euclidean'\n",
    "        HCA_all[name][treat] = perform_HCA(datasets[name][treat], metric=metric, method='average')\n",
    "        print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternative dendogram plots - Newer\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "\n",
    "def color_list_to_matrix_and_cmap(colors, ind, axis=0):\n",
    "        if any(issubclass(type(x), list) for x in colors):\n",
    "            all_colors = set(itertools.chain(*colors))\n",
    "            n = len(colors)\n",
    "            m = len(colors[0])\n",
    "        else:\n",
    "            all_colors = set(colors)\n",
    "            n = 1\n",
    "            m = len(colors)\n",
    "            colors = [colors]\n",
    "        color_to_value = dict((col, i) for i, col in enumerate(all_colors))\n",
    "\n",
    "        matrix = np.array([color_to_value[c]\n",
    "                           for color in colors for c in color])\n",
    "\n",
    "        matrix = matrix.reshape((n, m))\n",
    "        matrix = matrix[:, ind]\n",
    "        if axis == 0:\n",
    "            # row-side:\n",
    "            matrix = matrix.T\n",
    "\n",
    "        cmap = mpl.colors.ListedColormap(all_colors)\n",
    "        return matrix, cmap\n",
    "\n",
    "def plot_dendogram2(Z, leaf_names, label_colors, title='', ax=None, no_labels=False, labelsize=12, **kwargs):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    hier.dendrogram(Z, labels=leaf_names, leaf_font_size=10, above_threshold_color='0.2', orientation='left',\n",
    "                    ax=ax, **kwargs)\n",
    "    #Coloring labels\n",
    "    #ax.set_ylabel('Distance (AU)')\n",
    "    ax.set_xlabel('Distance (AU)')\n",
    "    ax.set_title(title, fontsize = 15)\n",
    "    \n",
    "    #ax.tick_params(axis='x', which='major', pad=12)\n",
    "    ax.tick_params(axis='y', which='major', labelsize=labelsize, pad=12)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    \n",
    "    #xlbls = ax.get_xmajorticklabels()\n",
    "    xlbls = ax.get_ymajorticklabels()\n",
    "    rectimage = []\n",
    "    for lbl in xlbls:\n",
    "        col = label_colors[lbl.get_text()]\n",
    "        lbl.set_color(col)\n",
    "        #lbl.set_fontweight('bold')\n",
    "        if no_labels:\n",
    "            lbl.set_color('w')\n",
    "        rectimage.append(col)\n",
    "\n",
    "    cols, cmap = color_list_to_matrix_and_cmap(rectimage, range(len(rectimage)), axis=0)\n",
    "\n",
    "    axins = inset_axes(ax, width=\"5%\", height=\"100%\",\n",
    "                   bbox_to_anchor=(1, 0, 1, 1),\n",
    "                   bbox_transform=ax.transAxes, loc=3, borderpad=0)\n",
    "\n",
    "    axins.pcolor(cols, cmap=cmap, edgecolors='w', linewidths=1)\n",
    "    axins.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(5, 40))\n",
    "name = 'HD'\n",
    "title = f\"Data set {datasets[name]['name']}, BinSim treatment\"\n",
    "plot_dendogram2(HCA_all[name]['BinSim']['Z'], \n",
    "               datasets[name]['target'], ax=ax,\n",
    "               label_colors=datasets[name]['label_colors'], title=title,\n",
    "               color_threshold=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style(\"white\"):\n",
    "    f, axs = plt.subplots(1, 4, figsize=(14, 30), constrained_layout=True)\n",
    "    \n",
    "    name = 'HD'\n",
    "      \n",
    "    for treatment, ax in zip(('P_RF', 'NP_RF', 'NGP_RF', 'BinSim'), axs.ravel()):\n",
    "        color_threshold = 0.68 if treatment == 'BinSim' else None\n",
    "        plot_dendogram2(HCA_all[name][treatment]['Z'], \n",
    "                       datasets[name]['target'], ax=ax,\n",
    "                       label_colors=datasets[name]['label_colors'],\n",
    "                       title=treatment, color_threshold=0)\n",
    "\n",
    "    st = f.suptitle(f'Data set {datasets[name][\"name\"]}', fontsize=16)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style(\"white\"):\n",
    "    f, axs = plt.subplots(1, 4, figsize=(14, 30), constrained_layout=True)\n",
    "    \n",
    "    name = 'HD'\n",
    "      \n",
    "    for treatment, ax in zip(('P', 'NP', 'NGP', 'BinSim'), axs.ravel()):\n",
    "        color_threshold = 0.68 if treatment == 'BinSim' else None\n",
    "        plot_dendogram2(HCA_all[name][treatment]['Z'], \n",
    "                       datasets[name]['target'], ax=ax,\n",
    "                       label_colors=datasets[name]['label_colors'],\n",
    "                       title=treatment, color_threshold=0)\n",
    "\n",
    "    st = f.suptitle(f'Data set {datasets[name][\"name\"]}', fontsize=16)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_clustering_metrics(res_dict, labels):\n",
    "    \"\"\"Fill dict with clustering performance metrics.\"\"\"\n",
    "    \n",
    "    discrim = ma.dist_discrim(res_dict['Z'], labels, # all samples have the same order\n",
    "                              method = 'average')\n",
    "    res_dict['Average discrim dist'] = discrim[0]\n",
    "    correct = np.array(list(discrim[1].values()))\n",
    "    \n",
    "    classes = pd.unique(labels)\n",
    "    res_dict['% correct clustering'] = (100/len(classes)) * len(correct[correct>0])\n",
    "\n",
    "    # Correct First Cluster Percentage\n",
    "    res_dict['% correct 1st clustering'] = 100 * ma.correct_1stcluster_fraction(res_dict['Z'],labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HCA_performance = []\n",
    "for name, dataset in datasets.items():\n",
    "    for treatment in ('P', 'NP', 'NGP', 'P_RF', 'NP_RF', 'NGP_RF', 'BinSim'):\n",
    "        compute_clustering_metrics(HCA_all[name][treatment], datasets[name]['target'])\n",
    "        perform = {'dataset': name, 'treatment': treatment,\n",
    "                   'Discrimination Distance': HCA_all[name][treatment]['Average discrim dist'],\n",
    "                   '% correct clusters': HCA_all[name][treatment]['% correct clustering'],\n",
    "                   '% correct 1st clustering': HCA_all[name][treatment]['% correct 1st clustering']}\n",
    "        HCA_performance.append(perform)\n",
    "        \n",
    "HCA_performance = pd.DataFrame(HCA_performance)\n",
    "cv_dsnames = {name:datasets[name]['name'] for name in datasets}\n",
    "\n",
    "HCA_performance2 = HCA_performance.assign(dataset = HCA_performance['dataset'].map(cv_dsnames))\n",
    "HCA_performance2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import sklearn.ensemble as skensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(16)\n",
    "name = 'HD'\n",
    "pos_label = 'Recurrence'\n",
    "dataset = datasets[name]\n",
    "y = dataset['target']\n",
    "resROC = {}\n",
    "\n",
    "p7 = sns.color_palette('tab20', 7)\n",
    "treatments = ('P', 'P_RF', 'NP', 'NP_RF', 'NGP', 'NGP_RF', 'BinSim')\n",
    "            \n",
    "for treatment in treatments:\n",
    "    df = dataset[treatment]\n",
    "    res = ma.RF_ROC_cv(df, y, pos_label, n_fold=5, n_trees=20, n_iter=20)\n",
    "    resROC[treatment] = res\n",
    "\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    with sns.plotting_context(\"notebook\", font_scale=1.2):\n",
    "        f, ax = plt.subplots(1, 1, figsize=(5,5))\n",
    "        for treatment, color in zip(resROC, p7):\n",
    "            res = resROC[treatment]\n",
    "            mean_fpr = res['average fpr']\n",
    "            mean_tpr = res['average tpr']\n",
    "            mean_auc = res['mean AUC']\n",
    "            ax.plot(mean_fpr, mean_tpr, color=color,\n",
    "                   label=f'{treatment} (AUC = {mean_auc:.3f})',\n",
    "                   lw=2, alpha=0.8)\n",
    "        \n",
    "        ax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='lightgrey', alpha=.8)\n",
    "        ax.legend()\n",
    "        ax.set_xlim(None,1)\n",
    "        ax.set_ylim(0,None)\n",
    "        ax.set(xlabel='False positive rate', ylabel='True positive rate', title='')\n",
    "              # title=\"Random forest ROC curves for Vitis types data set\")\n",
    "        plt.show()\n",
    "        f.savefig('paperimages/ROC_HD.pdf', dpi=200)\n",
    "        f.savefig('paperimages/ROC_HD.png', dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization of Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATE = False\n",
    "if GENERATE:\n",
    "    # NOTE: for debugging\n",
    "    top_tree_in_grid=200\n",
    "    # otherwise\n",
    "    #top_tree_in_grid=200\n",
    "\n",
    "    # Vector with values for the parameter n_estimators\n",
    "    # Models will be built from 10 to top_tree_in_grid trees in 5 tree intervals\n",
    "    values = {'n_estimators': range(10,top_tree_in_grid,5)}\n",
    "\n",
    "    rf = skensemble.RandomForestClassifier(n_estimators=200)\n",
    "    clf = GridSearchCV(rf, values, cv=5)\n",
    "\n",
    "    # For each dataset, build  Random Forest models with the different number of trees\n",
    "    # and store the predictive accuracy (estimated by k-fold cross-validation)\n",
    "\n",
    "    RF_optim = {}\n",
    "    for name, dataset in datasets.items():\n",
    "        for treatment in ('P', 'NP', 'NGP', 'P_RF', 'NP_RF', 'NGP_RF', 'BinSim'):\n",
    "            print('Fitting to', dataset['name'], 'pre-treatment', treatment, '...', end=' ')\n",
    "            rfname = name + ' ' + treatment\n",
    "            RF_optim[rfname] = {'dskey': name, 'dataset': dataset['name'], 'treatment':treatment}\n",
    "\n",
    "            clf2use = clf\n",
    "            clf2use.fit(dataset[treatment], dataset['target'])\n",
    "            \n",
    "            RF_optim[rfname]['scores'] = list(clf2use.cv_results_['mean_test_score'])\n",
    "            RF_optim[rfname]['n_trees'] = list(clf2use.cv_results_['param_n_estimators'])\n",
    "\n",
    "            print('Done!')\n",
    "    #print('writing results to file')\n",
    "    #path = Path.cwd() / 'paperimages' / 'RF_optim_HD.json'\n",
    "    #print(path.name)\n",
    "    #with open(path, \"w\", encoding='utf8') as write_file:\n",
    "        #json.dump(RF_optim, write_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = Path.cwd() / 'paperimages' / 'RF_optim_HD.json'\n",
    "#with open(path, \"r\", encoding='utf8') as read_file:\n",
    "#    RF_optim = json.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results and adjusting parameters of the plot\n",
    "\n",
    "def plot_RF_otimization_ntrees(RF_optim, dskey, ax=None, ylabel='', title='', ylim=(20,101)):\n",
    "    p7 = sns.color_palette('tab20', 7)\n",
    "    to_plot = [optim for key, optim in RF_optim.items() if optim['dskey'] == dskey]\n",
    "    treatments = ('P', 'P_RF', 'NP', 'NP_RF', 'NGP', 'NGP_RF', 'BinSim')\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    for treatment, color in zip(treatments, p7):\n",
    "        for optim in to_plot:\n",
    "            if optim['treatment'] == treatment:\n",
    "                break\n",
    "        ax.plot(optim['n_trees'], [s*100 for s in optim['scores']], label=treatment, color=color)\n",
    "    ax.set(ylabel=ylabel, xlabel='Number of Trees', ylim=ylim, title=title)\n",
    "    ax.legend()\n",
    "\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    with sns.plotting_context(\"notebook\", font_scale=1.2):\n",
    "        f, ax = plt.subplots(1, 1, figsize=(6,6), constrained_layout=True)\n",
    "        dskey = 'HD'\n",
    "       \n",
    "        plot_RF_otimization_ntrees(RF_optim, dskey, ax=ax,\n",
    "                                   ylabel='Random Forest CV Mean Accuracy (%)',\n",
    "                                   title=datasets[dskey][\"name\"])\n",
    "\n",
    "        f.suptitle('Optimization of the number of trees')\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "100 trees for RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATE = False\n",
    "if GENERATE:\n",
    "    # NOTE: for debugging\n",
    "    iter_num=20\n",
    "    # otherwise\n",
    "    #iter_num=100\n",
    "\n",
    "    RF_all = {}\n",
    "\n",
    "    # Application of the Random Forests for each differently-treated dataset\n",
    "    for name, dataset in datasets.items():\n",
    "        for treatment in ('P', 'P_RF', 'NP', 'NP_RF', 'NGP', 'NGP_RF', 'BinSim'):\n",
    "            print(f'Fitting random forest for {name} with treatment {treatment}', end=' ...')\n",
    "            rfname = name + ' ' + treatment\n",
    "            RF_all[rfname] = {'dskey': name, 'dataset': dataset['name'], 'treatment':treatment}\n",
    "            n_fold = 5 #if (name in ['vitis_types', 'HD']) else 3\n",
    "\n",
    "            fit = ma.RF_model_CV(dataset[treatment], dataset['target'], iter_num=iter_num, n_fold=n_fold, n_trees=200)\n",
    "            RF_all[rfname].update(fit)\n",
    "\n",
    "            print(f'done')    \n",
    "    fname = 'paperimages/RF_all_HD.json'\n",
    "    with open(fname, \"w\", encoding='utf8') as write_file:\n",
    "        json.dump(RF_all, write_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'paperimages/RF_all_HD.json'\n",
    "with open(fname, \"r\", encoding='utf8') as read_file:\n",
    "    RF_all = json.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy across the iterations\n",
    "accuracies = pd.DataFrame({name: RF_all[name]['accuracy'] for name in RF_all})\n",
    "accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_stats = pd.DataFrame({'Average accuracy': accuracies.mean(axis=0),\n",
    "                               'STD': accuracies.std(axis=0)})\n",
    "accuracy_stats = accuracy_stats.assign(dataset=[RF_all[name]['dataset'] for name in RF_all],\n",
    "                                       treatment=[RF_all[name]['treatment'] for name in RF_all])\n",
    "accuracy_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p7 = sns.color_palette('tab20', 7)\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    with sns.plotting_context(\"notebook\", font_scale=1.1):\n",
    "        f, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "        x = np.arange(len(datasets))  # the label locations\n",
    "        labels = [datasets[name]['name'] for name in datasets]\n",
    "        width = 0.17  # the width of the bars\n",
    "        for i, treatment in enumerate(('P', 'P_RF', 'NP', 'NP_RF', 'NGP', 'NGP_RF','BinSim')):\n",
    "            acc_treatment = accuracy_stats[accuracy_stats['treatment']==treatment]\n",
    "            offset = - 0.3 + i * 0.2\n",
    "            rects = ax.bar(x + offset, acc_treatment['Average accuracy'], width, label=treatment, color = p7[i])\n",
    "            ax.errorbar(x + offset, y=acc_treatment['Average accuracy'], yerr=acc_treatment['STD'],\n",
    "                        ls='none', ecolor='0.2', capsize=3)\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(labels)\n",
    "        ax.set(ylabel='Average accuracy', title='', ylim=(0.5,1.02))\n",
    "        #ax.text(-0.5, 0.95, 'A', weight='bold', fontsize=15)\n",
    "        ax.legend(loc='lower left')#, bbox_to_anchor=(0.7, 1))\n",
    "        #f.savefig('paperimages/RF_performance.pdf' , dpi=200)\n",
    "        #f.savefig('paperimages/RF_performance.png' , dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLS-DA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization of the number of components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stdout\n",
    "# line above supresses PLS warnings\n",
    "\n",
    "GENERATE = False\n",
    "\n",
    "if GENERATE:\n",
    "    treatments = ('P', 'P_RF', 'NP', 'NP_RF', 'NGP', 'NGP_RF', 'BinSim')\n",
    "\n",
    "\n",
    "    # NOTE: for debugging\n",
    "    max_comp=20\n",
    "    # otherwise\n",
    "    #max_comp=50\n",
    "\n",
    "    # Store Results\n",
    "    PLS_optim = {}\n",
    "\n",
    "    # Build and extract metrics from models build with different number of components by using the optim_PLS function.\n",
    "    for name, dataset in datasets.items():\n",
    "        for treatment in treatments:\n",
    "            print(f'Fitting PLS-DA model for {name} with treatment {treatment}', end=' ...')\n",
    "            plsdaname = name + ' ' + treatment\n",
    "            PLS_optim[plsdaname] = {'dskey': name, 'dataset':dataset['name'], 'treatment':treatment}\n",
    "            n_fold = 5\n",
    "            optim = ma.optim_PLSDA_n_components(dataset[treatment], dataset['target'],\n",
    "                                                max_comp=max_comp, n_fold=n_fold).CVscores\n",
    "            PLS_optim[plsdaname]['CV_scores'] = optim\n",
    "            print(f'done')\n",
    "    fname = 'paperimages/PLSDA_optim_HD.json'\n",
    "    with open(fname, \"w\", encoding='utf8') as write_file:\n",
    "        json.dump(PLS_optim, write_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'paperimages/PLSDA_optim_HD.json'\n",
    "with open(fname, \"r\", encoding='utf8') as read_file:\n",
    "    PLS_optim = json.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treatments = ('P', 'P_RF', 'NP', 'NP_RF', 'NGP', 'NGP_RF', 'BinSim')\n",
    "treat_colors = dict(zip(treatments, sns.color_palette('tab20', 7)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results and adjusting plot parameters\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    with sns.plotting_context(\"notebook\", font_scale=1.2):\n",
    "        f, ax = plt.subplots(1, 1, figsize = (6,6))\n",
    "        plt.suptitle('Performance based on number of components, PLS-DA. Human data set', fontsize=15)\n",
    "\n",
    "        for name, data in PLS_optim.items():\n",
    "\n",
    "            if data['dskey'] == 'HD':\n",
    "\n",
    "                ax.plot(range(1, len(data['CV_scores']) + 1), data['CV_scores'],\n",
    "                         color = treat_colors[data['treatment']],\n",
    "                         label=data['treatment'])\n",
    "                ax.set(xlabel='Number of Components',\n",
    "                        ylabel='PLS Score (1 - PRESS/SS)',\n",
    "                        title='Human data set')\n",
    "                ax.legend()\n",
    "                ax.set_ylim([0, 1])\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15 components chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stdout\n",
    "GENERATE = False\n",
    "if GENERATE:\n",
    "    PLSDA_all = {}\n",
    "\n",
    "    iter_num=10\n",
    "\n",
    "    # For each differently-treated dataset, fit PLS-DA models on n randomly sampled folds (for stratified cross-validation)\n",
    "    for name, dataset in datasets.items():\n",
    "        for treatment in ('P', 'P_RF', 'NP', 'NP_RF', 'NGP', 'NGP_RF', 'BinSim'):\n",
    "            print(f'Fitting a PLS-DA model to {name} with treatment {treatment}', end=' ...')\n",
    "            plsdaname = name + ' ' + treatment\n",
    "            PLSDA_all[plsdaname] = {'dskey': name, 'dataset': dataset['name'], 'treatment':treatment}\n",
    "            n_comp = 15\n",
    "            n_fold = 5\n",
    "            fit = ma.PLSDA_model_CV(dataset[treatment], dataset['target'],\n",
    "                                    n_comp=n_comp, n_fold=n_fold,\n",
    "                                    iter_num=iter_num,\n",
    "                                    feat_type='VIP')\n",
    "            PLSDA_all[plsdaname].update(fit)\n",
    "            print(f'done')     \n",
    "\n",
    "    fname = 'paperimages/PLSDA_all_HD.json'\n",
    "    with open(fname, \"w\", encoding='utf8') as write_file:\n",
    "        json.dump(PLSDA_all, write_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy across iterations\n",
    "fname = 'paperimages/PLSDA_all_HD.json'\n",
    "with open(fname, \"r\", encoding='utf8') as read_file:\n",
    "    PLSDA_all = json.load(read_file)\n",
    "\n",
    "accuracies = pd.DataFrame({name: PLSDA_all[name]['accuracy'] for name in PLSDA_all})\n",
    "accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_stats = pd.DataFrame({'Average accuracy': accuracies.mean(axis=0),\n",
    "                               'STD': accuracies.std(axis=0)})\n",
    "accuracy_stats = accuracy_stats.assign(dataset=[PLSDA_all[name]['dataset'] for name in PLSDA_all],\n",
    "                                       treatment=[PLSDA_all[name]['treatment'] for name in PLSDA_all])\n",
    "accuracy_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p7 = sns.color_palette('tab20', 7)\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    with sns.plotting_context(\"notebook\", font_scale=1.1):\n",
    "        f, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "        x = np.arange(len(datasets))  # the label locations\n",
    "        labels = [datasets[name]['name'] for name in datasets]\n",
    "        width = 0.17  # the width of the bars\n",
    "        for i, treatment in enumerate(('P', 'P_RF', 'NP', 'NP_RF', 'NGP', 'NGP_RF', 'BinSim')):\n",
    "            acc_treatment = accuracy_stats[accuracy_stats['treatment']==treatment]\n",
    "            offset = - 0.3 + i * 0.2\n",
    "            rects = ax.bar(x + offset, acc_treatment['Average accuracy'], width, label=treatment, color = p7[i])\n",
    "            ax.errorbar(x + offset, y=acc_treatment['Average accuracy'], yerr=acc_treatment['STD'],\n",
    "                        ls='none', ecolor='0.2', capsize=3)\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(labels)\n",
    "        ax.set(ylabel='Average accuracy', title='', ylim=(0.2,1.02))\n",
    "        ax.legend(loc='lower left')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means Clustering Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.cluster as skclust\n",
    "from sklearn.metrics import adjusted_rand_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_KMeans(dataset, treatment, iter_num=150, best_fraction=0.1):\n",
    "    \"Perform K-means Clustering Analysis and calculate discrimination evaluation metrics.\"\n",
    "    \n",
    "    sample_labels = datasets[dataset]['target']\n",
    "    n_classes = len(pd.unique(sample_labels))\n",
    "    \n",
    "    df = datasets[dataset][treatment]\n",
    "    \n",
    "    discrim = ma.Kmeans_discrim(df, sample_labels,\n",
    "                                method='average', \n",
    "                                iter_num=iter_num,\n",
    "                                best_fraction=best_fraction)\n",
    "\n",
    "    \n",
    "    # Lists for the results of the best k-means clustering\n",
    "    average = []\n",
    "    correct = []\n",
    "    rand = []\n",
    "    \n",
    "    for j in discrim:\n",
    "        global_disc_dist, disc_dists, rand_index, SSE = discrim[j]\n",
    "        \n",
    "        # Average of discrimination distances\n",
    "        average.append(global_disc_dist) \n",
    "        \n",
    "        # Correct Clustering Percentages\n",
    "        all_correct = np.array(list(disc_dists.values()))\n",
    "        correct.append(len(all_correct[all_correct>0]))\n",
    "        \n",
    "        # Adjusted Rand Index\n",
    "        rand.append(rand_index) \n",
    "    \n",
    "    return{'dataset': dataset,\n",
    "           'treatment': treatment,\n",
    "           'Discrimination Distance': np.median(average),\n",
    "           '% correct clusters':np.median(correct)*100/n_classes,\n",
    "           'Rand Index': np.median(rand)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# NOTE: for debugging\n",
    "iter_num=15\n",
    "# otherwise\n",
    "#iter_num=150\n",
    "\n",
    "KMeans_all = []\n",
    "\n",
    "dsname = 'HD'\n",
    "for treatment in ('P', 'NP', 'NGP', 'P_RF', 'NP_RF', 'NGP_RF', 'BinSim'):\n",
    "    print(f'performing KMeans on {dsname} with treatment {treatment}' , end=' ...')\n",
    "    KMeans_all.append(perform_KMeans(dsname, treatment, iter_num=iter_num))\n",
    "    print('done!')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KMeans_all = pd.DataFrame(KMeans_all)\n",
    "\n",
    "cv_dsnames = {name:datasets[name]['name'] for name in datasets}\n",
    "KMeans_all2 = KMeans_all.assign(dataset = KMeans_all['dataset'].map(cv_dsnames))\n",
    "KMeans_all2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Supervised Statistical Analysis - Permutation Tests\n",
    "\n",
    "The Supervised Statistical Analysis methods used will be Random Forest and PLS-DA.\n",
    "\n",
    "The performance of the classifiers will be evaluated by their predictive **accuracy** (which will always be estimated by internal stratified 3-fold cross-validation or 5-fold cross-validation in `vitis_types`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permutation Tests (Very Slow)\n",
    "\n",
    "Permutation tests is based on shuffling the labels of the different samples, shuflling the groups where they belong with the intent to see if the classifier tested, whether it is Random Forest or PLS-DA found a significant class structure in the data - assess the significance of the predictive accuracy results. \n",
    "\n",
    "For that a random k-fold cross-validation is performed on the original dataset (to serve as a comparation point) and on 500 permutations of datasets with labels randomly shuffled around. The models are evaluated by their predictive accuracies. \n",
    "\n",
    "The empirical p-value is given by (the number of times the permutation accuracy was bigger than the random k-fold cross-validation made with the original dataset + 1) / (number of permutations + 1) (source: Ojala and Garriga, 2010).\n",
    "\n",
    "Ojala M, Garriga GC. Permutation Tests for Studying Classifier Performance. In: 2009 Ninth IEEE International Conference on Data Mining. ; 2009:908-913. doi:10.1109/ICDM.2009.108\n",
    "\n",
    "Histograms with the prediction accuracy of the different permutations were plotted and compared to the accuracy got with the original dataset.\n",
    "\n",
    "### Permutation Tests - Random Forests\n",
    "\n",
    "Use of `permutation_RF` function from multianalysis.py. See details about the application of this function in the multianalysis.py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json for persistence\n",
    "import json\n",
    "from time import perf_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `GENERATE = True` to perform permutation tests and persist results in json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATE = True #False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if GENERATE:\n",
    "    iter_num=500 # number of permutations\n",
    "\n",
    "\n",
    "    permuts_RF = []\n",
    "\n",
    "    to_permute = [name for name in datasets]\n",
    "    for name in to_permute:\n",
    "        for treatment in ('P', 'P_RF', 'NP', 'NP_RF', 'NGP', 'NGP_RF', 'BinSim'):\n",
    "            dataset = datasets[name]\n",
    "            print(f'{iter_num} permutations (Random Forest) for {name} with treatment {treatment}', end=' ...')\n",
    "            n_fold = 5\n",
    "            start = perf_counter()\n",
    "            permutations = ma.permutation_RF(dataset[treatment], dataset['target'], iter_num=iter_num, n_fold=n_fold, n_trees=100)\n",
    "            res = {'dataset': name, 'treatment': treatment,\n",
    "                   'non_permuted_CV': permutations[0],\n",
    "                   'permutations': permutations[1],\n",
    "                   'p-value': permutations[2]}       \n",
    "            permuts_RF.append(res)\n",
    "            end = perf_counter()\n",
    "            pvalue = permutations[2]\n",
    "            print(f'Done! took {(end - start):.3f} s, p-value = {pvalue:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "    \n",
    "    # Store in json file\n",
    "    fname = 'paperimages/permuts_rf_hd.json'\n",
    "    with open(fname, \"w\", encoding='utf8') as write_file:\n",
    "        json.dump(permuts_RF, write_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutation Tests - PLS-DA\n",
    "\n",
    "Use of `permutation_PLSDA` function from multianalysis.py. See details about the application of this function in the multianalysis.py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stdout\n",
    "if GENERATE:\n",
    "    iter_num=500\n",
    "\n",
    "    permuts_PLSDA = []\n",
    "\n",
    "    to_permute = [name for name in datasets]\n",
    "    for name in to_permute:\n",
    "        for treatment in ('P', 'P_RF', 'NP', 'NP_RF', 'NGP', 'NGP_RF', 'BinSim'):\n",
    "            dataset = datasets[name]\n",
    "            print(f'Permutation test (PLS-DA) for {name} with treatment {treatment}', end=' ...')\n",
    "            n_comp = 15\n",
    "            n_fold = 5\n",
    "            start = perf_counter()\n",
    "            permutations = ma.permutation_PLSDA(dataset[treatment], dataset['target'], n_comp=11,\n",
    "                                                iter_num=iter_num, n_fold=n_fold)\n",
    "            res = {'dataset': name, 'treatment': treatment,\n",
    "                   'non_permuted_CV': permutations[0],\n",
    "                   'permutations': permutations[1],\n",
    "                   'p-value': permutations[2]}\n",
    "            permuts_PLSDA.append(res)\n",
    "            end = perf_counter()\n",
    "            pvalue = permutations[2]\n",
    "            print(f'Done! took {(end - start):.3f} s, p-value = {pvalue:.4f}')\n",
    "            \n",
    "    # Store in json file\n",
    "    fname = 'paperimages/permuts_plsda_hd.json'\n",
    "    with open(fname, \"w\", encoding='utf8') as write_file:\n",
    "        json.dump(permuts_PLSDA, write_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data from json file - random forests\n",
    "fname = 'paperimages/permuts_rf_hd.json'\n",
    "with open(fname, \"r\", encoding='utf8') as read_file:\n",
    "    permuts_RF = json.load(read_file)\n",
    "\n",
    "for p in permuts_RF:\n",
    "    print(f\"{p['dataset']:<20}{p['treatment']:<8}{p['p-value']:10.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data from json file - PLS-DA\n",
    "fname = 'paperimages/permuts_plsda_hd.json'\n",
    "with open(fname, \"r\", encoding='utf8') as read_file:\n",
    "    permuts_PLSDA = json.load(read_file)\n",
    "\n",
    "for p in permuts_PLSDA:\n",
    "    print(f\"{p['dataset']:<20}{p['treatment']:<8}{p['p-value']:10.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
