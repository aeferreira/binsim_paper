{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Simplification in Metabolomics\n",
    "\n",
    "Notebook to support the study on the application of **Bin**ary **Sim**plification as a competing form of pre-processing procedure for high-resolution metabolomics data.\n",
    "\n",
    "This is notebook `paper_binsim_unsupervised.ipynb`\n",
    "\n",
    "\n",
    "## Organization of the Notebook\n",
    "\n",
    "- **Agglomerative Hierarchical Clustering and K-means Clustering: assessment of performence given a ground-truth of cluster assignments.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Needed Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from pathlib import Path\n",
    "\n",
    "# json for persistence\n",
    "import json\n",
    "from time import perf_counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.testing import assert_frame_equal\n",
    "\n",
    "import scipy.spatial.distance as dist\n",
    "import scipy.cluster.hierarchy as hier\n",
    "import scipy.stats as stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib import ticker\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Metabolinks package\n",
    "import metabolinks as mtl\n",
    "import metabolinks.transformations as transf\n",
    "\n",
    "# Python files in the repository\n",
    "import multianalysis as ma\n",
    "from elips import plot_confidence_ellipse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of dataset records\n",
    "\n",
    "`datasets` is the global dict that holds all data sets. It is a **dict of dict's**.\n",
    "\n",
    "Each data set is **represented as a dict**.\n",
    "\n",
    "Each record has the following fields (keys):\n",
    "\n",
    "- `name`: the table/figure name of the data set\n",
    "- `source`: the biological source for each dataset\n",
    "- `mode`: the aquisition mode\n",
    "- `alignment`: the alignment used to generate the data matrix\n",
    "- `data`: the data matrix\n",
    "- `target`: the sample labels, possibly already integer encoded\n",
    "- `<treatment name>`: transformed data matrix. These treatment names can be\n",
    "    - `Ionly`: missing value imputed data, only\n",
    "    - `P`: Pareto scaled data\n",
    "    - `NP`: Pareto scaled and normalized\n",
    "    - `NGP`: normalized, glog transformed and Pareto scaled\n",
    "    - `BinSim`: binary simplified data\n",
    "\n",
    "The keys of `datasets` may be shared with dicts holding records resulting from comparison analysis.\n",
    "\n",
    "Here are the keys (and respective names) of datasets used in this study:\n",
    "\n",
    "- GD_neg_global2 (GDg2-)\n",
    "- GD_pos_global2 (GDg2+)\n",
    "- GD_neg_class2 (GDc2-)\n",
    "- GD_pos_class2 (GDc2+)\n",
    "- YD (YD 2/15)\n",
    "- YD2 (YD 6/15)\n",
    "- vitis_types (GD types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path.cwd() / \"paperimages\" / 'processed_data.json'\n",
    "storepath = Path.cwd() / \"paperimages\" / 'processed_data.h5'\n",
    "with pd.HDFStore(storepath) as store:\n",
    "\n",
    "    with open(path, encoding='utf8') as read_file:\n",
    "        datasets = json.load(read_file)\n",
    "    \n",
    "    for dskey, dataset in datasets.items():\n",
    "        for key in dataset:\n",
    "            value = dataset[key]\n",
    "            if isinstance(value, str) and value.startswith(\"INSTORE\"):\n",
    "                storekey = value.split(\"_\", 1)[1]\n",
    "                dataset[key] = store[storekey]\n",
    "            elif key == 'label_colors':\n",
    "                dataset[key] = {lbl: tuple(c) for lbl, c in value.items()}\n",
    "            elif key == 'sample_colors':\n",
    "                dataset[key] = [tuple(c) for c in value]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agglomerative Hierarchical Cluster Analysis \n",
    "\n",
    "HCA analysis of each differently-treated dataset and corresponding dendrograms.\n",
    "\n",
    "**Euclidian distance** and **UPGMA linkage** used for datasets obtained with traditional pre-treatments.\n",
    "\n",
    "Several **binary distance** metrics (Jaccard as example) and **UPGMA linkage** for datasets obtained with BinSim.\n",
    "\n",
    "Note: `Vitis_types` data set was used only for supervised analysis and is thus not included here since unsupervised analysis does not depend on target labels (even if some of our metrics to evaluate them depend). For example, `Vitis_types` dendrograms would be equal to `GD_neg_class2` since the only difference between these data sets is the target labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_HCA(df, metric='euclidean', method='average'):\n",
    "    \"Performs Hierarchical Clustering Analysis of a data set with chosen linkage method and distance metric.\"\n",
    "    \n",
    "    distances = dist.pdist(df, metric=metric)\n",
    "    \n",
    "    # method is one of\n",
    "    # ward, average, centroid, single, complete, weighted, median\n",
    "    Z = hier.linkage(distances, method=method)\n",
    "\n",
    "    # Cophenetic Correlation Coefficient\n",
    "    # (see how the clustering - from hier.linkage - preserves the original distances)\n",
    "    coph = hier.cophenet(Z, distances)\n",
    "    # Baker's gamma\n",
    "    mr = ma.mergerank(Z)\n",
    "    bg = mr[mr!=0]\n",
    "\n",
    "    return {'Z': Z, 'distances': distances, 'coph': coph, 'merge_rank': mr, \"Baker's Gamma\": bg}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computation of linkages, distances and cophenetics\n",
    "\n",
    "- Ionly - Missing Value Imputation only (it is not performed for this one)\n",
    "- P - Imputed and Pareto scaled\n",
    "- NP - Imputed, Normalized (reference feature) and Pareto scaled\n",
    "- NGP - Imputed, Normalized (reference feature), g-log transformed and Pareto scaled\n",
    "- BinSim - Binary simplified data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionaries to contain results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HCA_all = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the clusterings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, ds in datasets.items():\n",
    "    HCA_all[name] = {}\n",
    "    for treat in 'P', 'NP', 'NGP', 'BinSim', 'P_RF', 'NP_RF', 'NGP_RF':\n",
    "        print(f'Performing HCA to {name} data set with treatment {treat}', end=' ...')\n",
    "        metric = 'jaccard' if treat == 'BinSim' else 'euclidean'\n",
    "        HCA_all[name][treat] = perform_HCA(datasets[name][treat], metric=metric, method='average')\n",
    "        print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Older\n",
    "def plot_dendogram(Z, leaf_names, label_colors=None, title='', color_threshold=None):\n",
    "    \"\"\"Plot a dendrogram from a Z linkage matrix with a chosen title and a chosen color_threshold.\"\"\"\n",
    "    \n",
    "    with plt.rc_context({'lines.linewidth':1.5}):\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10,6))\n",
    "        dn = hier.dendrogram(Z, labels=leaf_names, leaf_font_size=13, above_threshold_color='b', leaf_rotation=45,\n",
    "                             color_threshold=color_threshold)#,  orientation='left')\n",
    "\n",
    "        ax.set_ylabel('Distance (UA)')\n",
    "        ax.set_title(title, fontsize = 16)\n",
    "        # Coloring labels\n",
    "        if label_colors is not None:\n",
    "            for lbl in ax.get_xmajorticklabels():\n",
    "                lbl.set_color(label_colors[lbl.get_text()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# GD, P for neg mode - GDg2-\n",
    "\n",
    "name = 'GD_neg_global2'\n",
    "title = f\"Data set {datasets[name]['name']}, P treatment\"\n",
    "plot_dendogram(HCA_all[name]['P']['Z'],\n",
    "               datasets[name]['target'],\n",
    "               label_colors=datasets[name]['label_colors'], title=title,\n",
    "               color_threshold=175000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternative dendogram plots - Newer\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "\n",
    "def color_list_to_matrix_and_cmap(colors, ind, axis=0):\n",
    "        if any(issubclass(type(x), list) for x in colors):\n",
    "            all_colors = set(itertools.chain(*colors))\n",
    "            n = len(colors)\n",
    "            m = len(colors[0])\n",
    "        else:\n",
    "            all_colors = set(colors)\n",
    "            n = 1\n",
    "            m = len(colors)\n",
    "            colors = [colors]\n",
    "        color_to_value = dict((col, i) for i, col in enumerate(all_colors))\n",
    "\n",
    "        matrix = np.array([color_to_value[c]\n",
    "                           for color in colors for c in color])\n",
    "\n",
    "        matrix = matrix.reshape((n, m))\n",
    "        matrix = matrix[:, ind]\n",
    "        if axis == 0:\n",
    "            # row-side:\n",
    "            matrix = matrix.T\n",
    "\n",
    "        cmap = mpl.colors.ListedColormap(all_colors)\n",
    "        return matrix, cmap\n",
    "\n",
    "def plot_dendogram2(Z, leaf_names, label_colors, title='', ax=None, no_labels=False, labelsize=12, **kwargs):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    hier.dendrogram(Z, labels=leaf_names, leaf_font_size=10, above_threshold_color='0.2', orientation='left',\n",
    "                    ax=ax, **kwargs)\n",
    "    #Coloring labels\n",
    "    #ax.set_ylabel('Distance (AU)')\n",
    "    ax.set_xlabel('Distance (AU)')\n",
    "    ax.set_title(title, fontsize = 15)\n",
    "    \n",
    "    #ax.tick_params(axis='x', which='major', pad=12)\n",
    "    ax.tick_params(axis='y', which='major', labelsize=labelsize, pad=12)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    \n",
    "    #xlbls = ax.get_xmajorticklabels()\n",
    "    xlbls = ax.get_ymajorticklabels()\n",
    "    rectimage = []\n",
    "    for lbl in xlbls:\n",
    "        col = label_colors[lbl.get_text()]\n",
    "        lbl.set_color(col)\n",
    "        #lbl.set_fontweight('bold')\n",
    "        if no_labels:\n",
    "            lbl.set_color('w')\n",
    "        rectimage.append(col)\n",
    "\n",
    "    cols, cmap = color_list_to_matrix_and_cmap(rectimage, range(len(rectimage)), axis=0)\n",
    "\n",
    "    axins = inset_axes(ax, width=\"5%\", height=\"100%\",\n",
    "                   bbox_to_anchor=(1, 0, 1, 1),\n",
    "                   bbox_transform=ax.transAxes, loc=3, borderpad=0)\n",
    "\n",
    "    axins.pcolor(cols, cmap=cmap, edgecolors='w', linewidths=1)\n",
    "    axins.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GD, P for neg mode - GDg2-\n",
    "f, ax = plt.subplots(figsize=(5, 10))\n",
    "name = 'GD_neg_global2'\n",
    "title = f\"Data set {datasets[name]['name']}, P treatment\"\n",
    "plot_dendogram2(HCA_all[name]['P']['Z'], \n",
    "               datasets[name]['target'], ax=ax,\n",
    "               label_colors=datasets[name]['label_colors'], title=title,\n",
    "               color_threshold=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:darkred;\">------- Checkpoint for API migration</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = HCA_all['GD_neg_global2']['P']['Z']\n",
    "\n",
    "col_names = 'cluster a', 'cluster b', 'd(a,b)', '# cluster a-b'\n",
    "df_Z = pd.DataFrame(Z, index=range(1, len(Z)+1), columns=col_names)\n",
    "df_Z.index.name = 'iteration'\n",
    "df_Z.insert(2, 'new cluster', pd.Series(range(len(Z), 2*len(Z)+1)))\n",
    "\n",
    "new_dtypes = {'cluster a': 'int64', 'cluster b': 'int64', '# cluster a-b': 'int64', 'new cluster': 'int64'}\n",
    "df_Z = df_Z.astype(new_dtypes)\n",
    "\n",
    "roundrobin = pd.read_csv('df_Z_P_neg.csv', sep='\\t', index_col=0)\n",
    "\n",
    "assert_frame_equal(roundrobin, df_Z)\n",
    "#roundrobin\n",
    "\n",
    "#df_Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:darkred;\">------- END checkpoint for API migration</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dendrograms of the 4 differently-treated data sets for each of the 6 benchmark data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style(\"white\"):\n",
    "    f, axs = plt.subplots(1, 4, figsize=(14, 8), constrained_layout=True)\n",
    "    \n",
    "    name = 'GD_neg_global2'\n",
    "    \n",
    "    for treatment, ax in zip(('P', 'NP', 'NGP', 'BinSim'), axs.ravel()):\n",
    "        plot_dendogram2(HCA_all[name][treatment]['Z'], \n",
    "                       datasets[name]['target'], ax=ax,\n",
    "                       label_colors=datasets[name]['label_colors'],\n",
    "                       title=treatment, color_threshold=0)\n",
    "\n",
    "    st = f.suptitle(f\"Data set {datasets[name]['name']}\", fontsize=16)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style(\"white\"):\n",
    "    f, axs = plt.subplots(1, 4, figsize=(14, 8), constrained_layout=True)\n",
    "    \n",
    "    name = 'GD_neg_global2'\n",
    "    \n",
    "    for treatment, ax in zip(('P_RF', 'NP_RF', 'NGP_RF', 'BinSim'), axs.ravel()):\n",
    "        plot_dendogram2(HCA_all[name][treatment]['Z'], \n",
    "                       datasets[name]['target'], ax=ax,\n",
    "                       label_colors=datasets[name]['label_colors'],\n",
    "                       title=treatment, color_threshold=0)\n",
    "\n",
    "    st = f.suptitle(f\"Data set {datasets[name]['name']}\", fontsize=16)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style(\"white\"):\n",
    "    f, axs = plt.subplots(1, 4, figsize=(14, 8), constrained_layout=True)\n",
    "    \n",
    "    name = 'GD_pos_global2'\n",
    "      \n",
    "    for treatment, ax in zip(('P', 'NP', 'NGP', 'BinSim'), axs.ravel()):\n",
    "        color_threshold = 0.68 if treatment == 'BinSim' else None\n",
    "        plot_dendogram2(HCA_all[name][treatment]['Z'], \n",
    "                       datasets[name]['target'], ax=ax,\n",
    "                       label_colors=datasets[name]['label_colors'],\n",
    "                       title=treatment, color_threshold=0)\n",
    "\n",
    "    st = f.suptitle(f\"Data set {datasets[name]['name']}\", fontsize=16)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style(\"white\"):\n",
    "    f, axs = plt.subplots(1, 4, figsize=(14, 8), constrained_layout=True)\n",
    "    \n",
    "    name = 'GD_neg_class2'\n",
    "      \n",
    "    for treatment, ax in zip(('P', 'NP', 'NGP', 'BinSim'), axs.ravel()):\n",
    "        color_threshold = 0.68 if treatment == 'BinSim' else None\n",
    "        plot_dendogram2(HCA_all[name][treatment]['Z'], \n",
    "                       datasets[name]['target'], ax=ax,\n",
    "                       label_colors=datasets[name]['label_colors'],\n",
    "                       title=treatment, color_threshold=0)\n",
    "\n",
    "    st = f.suptitle(f\"Data set {datasets[name]['name']}\", fontsize=16)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style(\"white\"):\n",
    "    f, axs = plt.subplots(1, 4, figsize=(14, 8), constrained_layout=True)\n",
    "    \n",
    "    name = 'GD_neg_class2'\n",
    "      \n",
    "    for treatment, ax in zip(('P_RF', 'NP_RF', 'NGP_RF', 'BinSim'), axs.ravel()):\n",
    "        color_threshold = 0.68 if treatment == 'BinSim' else None\n",
    "        plot_dendogram2(HCA_all[name][treatment]['Z'], \n",
    "                       datasets[name]['target'], ax=ax,\n",
    "                       label_colors=datasets[name]['label_colors'],\n",
    "                       title=treatment, color_threshold=0)\n",
    "\n",
    "    st = f.suptitle(f\"Data set {datasets[name]['name']}\", fontsize=16)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data set GDc2+ (titles not shown - image for paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style(\"white\"):\n",
    "    f, axs = plt.subplots(1, 4, figsize=(14, 8), constrained_layout=True)\n",
    "    \n",
    "    name = 'GD_pos_class2'\n",
    "      \n",
    "    for treatment, ax in zip(('P', 'NP', 'NGP', 'BinSim'), axs.ravel()):\n",
    "        color_threshold = 0.68 if treatment == 'BinSim' else None\n",
    "        plot_dendogram2(HCA_all[name][treatment]['Z'], \n",
    "                       datasets[name]['target'], ax=ax,\n",
    "                       label_colors=datasets[name]['label_colors'],\n",
    "                       title='', color_threshold=0)\n",
    "\n",
    "    #st = f.suptitle(f\"Data set {datasets[name]['name']}\", fontsize=16)\n",
    "    for letter, ax in zip('ABCDEFGHIJ', axs.ravel()):\n",
    "        ax.text(0.3, 0.98, letter, ha='left', va='center', fontsize=15, weight='bold',\n",
    "                transform=ax.transAxes,\n",
    "                bbox=dict(facecolor='white', alpha=0.9))\n",
    "\n",
    "    plt.show()\n",
    "    f.savefig('paperimages/dendrosGDpos21.pdf', dpi=200)\n",
    "    f.savefig('paperimages/dendrosGDpos21.png', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style(\"white\"):\n",
    "    f, axs = plt.subplots(1, 4, figsize=(12, 4), constrained_layout=True)\n",
    "    \n",
    "    name = 'YD'\n",
    "      \n",
    "    for treatment, ax in zip(('P', 'NP', 'NGP', 'BinSim'), axs.ravel()):\n",
    "        color_threshold = 0.68 if treatment == 'BinSim' else None\n",
    "        plot_dendogram2(HCA_all[name][treatment]['Z'], \n",
    "                       datasets[name]['target'], ax=ax,\n",
    "                       label_colors=datasets[name]['label_colors'],\n",
    "                       title=treatment, color_threshold=0)\n",
    "\n",
    "    st = f.suptitle('Data set YD', fontsize=16)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style(\"white\"):\n",
    "    f, axs = plt.subplots(1, 4, figsize=(12, 4), constrained_layout=True)\n",
    "    \n",
    "    name = 'YD'\n",
    "      \n",
    "    for treatment, ax in zip(('P_RF', 'NP_RF', 'NGP_RF', 'BinSim'), axs.ravel()):\n",
    "        color_threshold = 0.68 if treatment == 'BinSim' else None\n",
    "        plot_dendogram2(HCA_all[name][treatment]['Z'], \n",
    "                       datasets[name]['target'], ax=ax,\n",
    "                       label_colors=datasets[name]['label_colors'],\n",
    "                       title=treatment, color_threshold=0)\n",
    "\n",
    "    st = f.suptitle('Data set YD', fontsize=16)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style(\"white\"):\n",
    "    f, axs = plt.subplots(1, 4, figsize=(12, 4), constrained_layout=True)\n",
    "    \n",
    "    name = 'YD2'\n",
    "      \n",
    "    for treatment, ax in zip(('P', 'NP', 'NGP', 'BinSim'), axs.ravel()):\n",
    "        color_threshold = 0.68 if treatment == 'BinSim' else None\n",
    "        plot_dendogram2(HCA_all[name][treatment]['Z'], \n",
    "                       datasets[name]['target'], ax=ax,\n",
    "                       label_colors=datasets[name]['label_colors'],\n",
    "                       title=treatment, color_threshold=0)\n",
    "\n",
    "    st = f.suptitle(f'Data set {datasets[name][\"name\"]}', fontsize=16)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style(\"white\"):\n",
    "    f, axs = plt.subplots(1, 4, figsize=(12, 4), constrained_layout=True)\n",
    "    \n",
    "    name = 'YD2'\n",
    "      \n",
    "    for treatment, ax in zip(('P_RF', 'NP_RF', 'NGP_RF', 'BinSim'), axs.ravel()):\n",
    "        color_threshold = 0.68 if treatment == 'BinSim' else None\n",
    "        plot_dendogram2(HCA_all[name][treatment]['Z'], \n",
    "                       datasets[name]['target'], ax=ax,\n",
    "                       label_colors=datasets[name]['label_colors'],\n",
    "                       title=treatment, color_threshold=0)\n",
    "\n",
    "    st = f.suptitle(f'Data set {datasets[name][\"name\"]}', fontsize=16)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###  Dendrogram Similarity Comparison\n",
    "\n",
    "The similarity of the dendrograms built from the differently-treated data sets of each of the benchmark data sets were compared using two correlation coefficients:\n",
    "\n",
    "#### Cophenetic Correlation Coefficient\n",
    "\n",
    "- Pearson Correlation of the matrix of cophenetic distances of two dendrograms.\n",
    "\n",
    "#### Baker's Gamma Correlation Coefficient\n",
    "\n",
    "- Use of the `mergerank` function from multianalysis.py to create a 'rank' of the iteration number two samples were linked to the same cluster. Then see Kendall Correlation between the results from 2 dendrograms according to Baker's paper or Spearman Correlation according to explanation given in the R package `dendextend`.\n",
    "\n",
    "Baker's paper: Baker FB. Stability of Two Hierarchical Grouping Techniques Case 1: Sensitivity to Data Errors. J Am Stat Assoc. 1974;69(346):440-445. doi:10.2307/2285675\n",
    "\n",
    "The information from HCA for these methods is already collected for the intensity-based pre-treatment treated data sets (and for BinSim treated data sets using the Jaccard distance for HCA) but it is not collected for BinSim treated data sets for other binary distance metrics. Thus, it will also be collected for these other binary distance metrics to compare the similarity of their dendrograms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples of procedure with these methods with the Negative Grapevine Dataset - GDg2-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Correation metrics\n",
    "pearsonr = stats.pearsonr\n",
    "kendalltau = stats.kendalltau\n",
    "spearmanr = stats.spearmanr\n",
    "\n",
    "table = []\n",
    "t1 = HCA_all['GD_neg_global2']['P']\n",
    "t2 = HCA_all['GD_neg_global2']['NP']\n",
    "t3 = HCA_all['GD_neg_global2']['NGP']\n",
    "\n",
    "r, p_value = pearsonr(t1['coph'][1], t2['coph'][1])\n",
    "k, p_value_k = kendalltau(t1[\"Baker's Gamma\"], t2[\"Baker's Gamma\"])\n",
    "s, p_value_s = spearmanr(t1[\"Baker's Gamma\"], t2[\"Baker's Gamma\"])\n",
    "table.append({'Pair of samples': 'P Treat-NP Treat',\n",
    "              'Cophenetic (Pearson)': r,\n",
    "              '(coph) p-value': p_value,\n",
    "              \"Baker's (Kendall)\":k,\n",
    "              '(B-K) p-value': p_value_k,\n",
    "              \"Baker's (Spearman)\":s,\n",
    "              '(B-S) p-value': p_value_s,})\n",
    "\n",
    "r, p_value = pearsonr(t1['coph'][1], t3['coph'][1])\n",
    "k, p_value_k = kendalltau(t1[\"Baker's Gamma\"], t3[\"Baker's Gamma\"])\n",
    "s, p_value_s = spearmanr(t1[\"Baker's Gamma\"], t3[\"Baker's Gamma\"])\n",
    "table.append({'Pair of samples': 'P Treat-NGP Treat',\n",
    "              'Cophenetic (Pearson)': r,\n",
    "              '(coph) p-value': p_value,\n",
    "              \"Baker's (Kendall)\":k,\n",
    "              '(B-K) p-value': p_value_k,\n",
    "              \"Baker's (Spearman)\":s,\n",
    "              '(B-S) p-value': p_value_s,})\n",
    "\n",
    "pd.DataFrame(table).set_index('Pair of samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BinSim Treated Datasets: behaviour of different binary distance measures in HCA\n",
    "\n",
    "#### Part 1 (Perform HCAs to BinSim treated datasets using different binary distance measures)\n",
    "\n",
    "**Note**: Applied to GDg2 data sets only, Negative and Positive modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9 binary distance metrics accepted in pdist (scipy.spatial.distances.pdist)\n",
    "binary_metrics = ('dice', 'hamming', 'jaccard', 'rogerstanimoto',\n",
    "                  'sokalmichener', 'sokalsneath', 'yule', 'kulsinski',\n",
    "                  'russellrao')\n",
    "# 3 metrics that will be used as representative of the others\n",
    "#(others not present have similar variations to one of those 3)\n",
    "#binary = ( 'jaccard', 'hamming', 'yule')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complement with HCA based on other binary distances using the BinSim treatment matrices\n",
    "\n",
    "ds2use = 'GD_neg_global2', 'GD_pos_global2'\n",
    "\n",
    "for name in ds2use:\n",
    "    for bin_metric in binary_metrics:\n",
    "        print(f'Performing HCA to {name} BinSim data set with metric {bin_metric}', end=' ...')\n",
    "        HCA_all[name][bin_metric] = perform_HCA(datasets[name]['BinSim'], metric=bin_metric, method='average')\n",
    "        print('done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 (Calculate all the pairwise correlations between the dendrograms)\n",
    "\n",
    "Choose the set of treatment/distance metric combination to consider to calculate the pairwise correlations. These are indicated by the strings in the treatments list. The colnames list has to follow the same logic as the treatments list.\n",
    "\n",
    "#### GDg2 with 3 different binary distance metrics (Jaccard, Hamming and Yule) and the 3 intensity-based pre-treatments (P, NP and NGP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column names and row names for the dataframes and heatmaps\n",
    "# Collect results of HCAs\n",
    "\n",
    "colnames = ['P', 'NP', 'NGP','J' , 'H', 'Y']\n",
    "treatments = ['P', 'NP', 'NGP','jaccard' , 'hamming', 'yule']\n",
    "\n",
    "# Calculation of correlation coefficient by each method\n",
    "\n",
    "def create_HCA_correlations(HCA_results, treatments, colnames):\n",
    "    n_res = len(colnames)\n",
    "    correlations = {key: np.empty((n_res, n_res)) for key in ('K', 'S', 'C', 'K_p', 'S_p', 'C_p')}\n",
    "\n",
    "    for i, treat1 in enumerate(treatments):\n",
    "        for j, treat2 in enumerate(treatments):\n",
    "            Si, Sj = HCA_results[treat1], HCA_results[treat2]\n",
    "            # K - Kendall (Baker's Gamma)\n",
    "            k, p_value_k = stats.kendalltau(Si[\"Baker's Gamma\"], Sj[\"Baker's Gamma\"])\n",
    "            correlations['K'][i,j], correlations['K_p'][i,j] = k, p_value_k\n",
    "\n",
    "            # S - Spearman (Baker's Gamma)\n",
    "            s, p_value_s = stats.spearmanr(Si[\"Baker's Gamma\"], Sj[\"Baker's Gamma\"])\n",
    "            correlations['S'][i,j], correlations['S_p'][i,j] = s, p_value_s\n",
    "\n",
    "            # C - Cophenetic Correlation\n",
    "            r, p_value = stats.pearsonr(Si['coph'][1], Sj['coph'][1])\n",
    "            correlations['C'][i,j], correlations['C_p'][i,j] = r, p_value\n",
    "\n",
    "    for k in correlations:\n",
    "        correlations[k] = pd.DataFrame(correlations[k], columns=colnames, index=colnames)\n",
    "    return correlations\n",
    "\n",
    "correlations_neg = create_HCA_correlations(HCA_all['GD_neg_global2'], treatments, colnames)\n",
    "correlations_pos = create_HCA_correlations(HCA_all['GD_pos_global2'], treatments, colnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 (Heatmaps of the correlation coeficients)\n",
    "\n",
    "As for the Baker's Gamma Correlation, the heatmaps presented will be the ones with corelation calculated with Kendall correlation (according to the original paper - Baker FB. Stability of Two Hierarchical Grouping Techniques Case 1: Sensitivity to Data Errors. J Am Stat Assoc. 1974;69(346):440-445. doi:10.2307/2285675).\n",
    "\n",
    "Although, seeing the other correlations is just a case of changing the 'C's and 'K's to 'S's based on which set of correlations you want to see in the `combineCK` function (2 cells below).\n",
    "\n",
    "Here, **two sets of heatmaps** that mimic the figures presented in the paper are shown - Baker's gamma (Kendall) correlation (upper) and cophenetic correlation (lower) of the GDg2- or GDg2+ data sets with 3 different binary distance metrics (Jaccard, Hamming and Yule) and the 3 combinations of intensity-based pre-treatments.\n",
    "\n",
    "Then, **one heatmap (cophenetic correlation coefficient)** with the 9 binary distance metrics (and the 3 combinations of intensity-based pre-treatments) for both the GDg2- and GDg2+ at the same time are shown.\n",
    "\n",
    "\n",
    "Below, are the functions to build these heatmaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relative_luminance(color):\n",
    "    \"\"\"Calculate the relative luminance of a color according to W3C standards\n",
    "    Parameters\n",
    "    ----------\n",
    "    color : matplotlib color or sequence of matplotlib colors\n",
    "        Hex code, rgb-tuple, or html color name.\n",
    "    Returns\n",
    "    -------\n",
    "    luminance : float(s) between 0 and 1\n",
    "    \"\"\"\n",
    "    rgb = mpl.colors.to_rgba_array(color)[:, :3]\n",
    "    rgb = np.where(rgb <= .03928, rgb / 12.92, ((rgb + .055) / 1.055) ** 2.4)\n",
    "    lum = rgb.dot([.2126, .7152, .0722])\n",
    "    try:\n",
    "        return lum.item()\n",
    "    except ValueError:\n",
    "        return lum\n",
    "\n",
    "def plot_partitioned_df_asheatmap(df, ax=None, cmap='viridis', vmin=None, vmax=None, norm=None,\n",
    "                                  partition_point=0, top_rotate=False, fontsize=14, colorbar=True):\n",
    "    \n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    \n",
    "    values = df.values.copy()\n",
    "    #values = np.flipud(values)\n",
    "    # handle partition point\n",
    "    \n",
    "    # insert NaN column/row in values\n",
    "    values = np.insert(values, partition_point, np.nan, axis=1)\n",
    "    #values = np.insert(values, df.shape[0]- partition_point, np.nan, axis=0)\n",
    "    values = np.insert(values, partition_point, np.nan, axis=0)\n",
    "    \n",
    "    # compute and insert 2% offset\n",
    "    X = np.array(range(values.shape[0] + 1), dtype=float)\n",
    "    Y = np.array(range(values.shape[1] + 1), dtype=float)\n",
    "    offset = X[-1] * 0.02\n",
    "    \n",
    "    X[(partition_point+1):] = np.arange(float(partition_point)+offset, float(len(X)-1), 1.0)\n",
    "    Y[(partition_point+1):] = np.arange(float(partition_point)+offset, float(len(Y)-1), 1.0)\n",
    "    #Y[(len(Y)-partition_point-1):] = np.arange(float(len(Y)-partition_point-2)+offset, float(len(Y)-1), 1.0)\n",
    "\n",
    "    # draw pcolormesh\n",
    "    pm = ax.pcolormesh(X, Y, values, cmap=cmap, vmin=vmin, vmax=vmax, norm=norm)\n",
    "    ax.set_ylim(ax.get_ylim()[1], ax.get_ylim()[0])\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    \n",
    "    # handle labels\n",
    "    midpoints_x = (X[1:] - X[:-1]) / 2 + X[:-1]\n",
    "    midpoints_x = np.delete(midpoints_x, partition_point)\n",
    "    midpoints_y = (Y[1:] - Y[:-1]) / 2 + Y[:-1]\n",
    "    midpoints_y = np.delete(midpoints_y, partition_point)\n",
    "    ax.set_xticks(midpoints_x)\n",
    "    ax.set_yticks(midpoints_y)\n",
    "    ax.set_xticklabels(df.columns)\n",
    "    ax.set_yticklabels(df.index)\n",
    "    ax.tick_params(labeltop=True, labelbottom=False, labelsize=fontsize,\n",
    "                   top=False, bottom=False, left=False, right=False)\n",
    "    if top_rotate:\n",
    "        # Rotate the tick labels and set their alignment.\n",
    "        plt.setp(ax.get_xticklabels(), rotation=90, ha=\"left\", va='center', rotation_mode=\"anchor\")\n",
    "    \n",
    "    # handle annotations\n",
    "    \n",
    "    pm_colors = pm.cmap(pm.norm(pm.get_array())).reshape(values.shape[0], values.shape[1], 4)\n",
    "    mask = np.ones((values.shape[0], values.shape[1]), dtype=bool)\n",
    "    mask[:, partition_point] = False\n",
    "    mask[partition_point, :] = False\n",
    "    pm_colors = pm_colors[mask].reshape(df.shape[0], df.shape[1], 4)\n",
    "    #print(pm_colors)\n",
    "\n",
    "    for i in range(df.shape[0]):\n",
    "        for j in range(df.shape[1]):\n",
    "            locx = midpoints_x[j]\n",
    "            locy = midpoints_y[i]\n",
    "            # handle label color according to cell color\n",
    "            cell_color = pm_colors[i, j, :]\n",
    "            lum = relative_luminance(cell_color)\n",
    "            text_color = \".15\" if lum > .408 else \"w\"\n",
    "            cell_value = df.iloc[i, j]\n",
    "            if cell_value < 0:\n",
    "                annot = f'−{abs(cell_value):.2g}' # typographically accurate minus sign\n",
    "            else:\n",
    "                annot = f'{cell_value:.2g}'\n",
    "            text = ax.text(locx, locy, annot, fontsize=fontsize,\n",
    "                           ha=\"center\", va=\"center\", color=text_color)\n",
    "\n",
    "    if colorbar:\n",
    "        plt.colorbar(pm)\n",
    "    return pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combineCK(correlations):\n",
    "    correlations['CK'] = correlations['C'].copy()\n",
    "    # lower tringular mask\n",
    "    upper_mask = np.triu(np.ones(correlations['CK'].shape)).astype(bool)\n",
    "    correlations['CK'][upper_mask] = correlations['K']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(10,8))\n",
    "\n",
    "combineCK(correlations_neg)\n",
    "\n",
    "pm = plot_partitioned_df_asheatmap(correlations_neg['CK'], ax=ax, vmin=-0.2, vmax=1,\n",
    "                                    cmap=sns.color_palette(\"rocket_r\", as_cmap=True),\n",
    "                                    partition_point=3, top_rotate=False)\n",
    "ax.text(3.5, -0.9, \"Baker's gamma (Kendall) correlation (upper) and cophenetic correlation (lower)\",\n",
    "        ha='center', fontsize=14)\n",
    "\n",
    "f.suptitle(\"Data set GDg2-\", fontsize=16, y=1.04)\n",
    "ax.text(1.5,-0.45, 'Traditional treatments', fontsize=14, ha='center')\n",
    "ax.text(4.5,-0.45, 'Binary distances', fontsize=14, ha='center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(10,8))\n",
    "\n",
    "combineCK(correlations_pos)\n",
    "\n",
    "pm = plot_partitioned_df_asheatmap(correlations_pos['CK'], ax=ax, vmin=-0.2, vmax=1,\n",
    "                                    cmap=sns.color_palette(\"rocket_r\", as_cmap=True),\n",
    "                                    partition_point=3, top_rotate=False)\n",
    "ax.text(3.5, -0.9, \"Baker's gamma (Kendall) correlation (upper) and cophenetic correlation (lower)\",\n",
    "        ha='center', fontsize=14)\n",
    "\n",
    "f.suptitle(\"Data set GDg2+\", fontsize=16, y=1.04)\n",
    "ax.text(1.5,-0.45, 'Traditional treatments', fontsize=14, ha='center')\n",
    "ax.text(4.5,-0.45, 'Binary distances', fontsize=14, ha='center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heatmaps combined in a single figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (axl, axr) = plt.subplots(1,2, figsize=(10,4.7), constrained_layout=True)\n",
    "\n",
    "combineCK(correlations_neg)\n",
    "\n",
    "pm = plot_partitioned_df_asheatmap(correlations_neg['CK'], ax=axl, vmin=-0.2, vmax=1,\n",
    "                                    cmap=sns.color_palette(\"rocket_r\", as_cmap=True), fontsize=13,\n",
    "                                    partition_point=3, top_rotate=False, colorbar=False)\n",
    "#axl.set_title(\"Data set GDg2-\", fontsize=16, y=1.15)\n",
    "axl.text(1.5,-0.6, 'Traditional treatments', fontsize=13, ha='center')\n",
    "axl.text(4.7,-0.6, 'Binary distances', fontsize=13, ha='center')\n",
    "\n",
    "combineCK(correlations_pos)\n",
    "\n",
    "pm = plot_partitioned_df_asheatmap(correlations_pos['CK'], ax=axr, vmin=-0.2, vmax=1,\n",
    "                                    cmap=sns.color_palette(\"rocket_r\", as_cmap=True), fontsize=12,\n",
    "                                    partition_point=3, top_rotate=False)\n",
    "\n",
    "#axr.set_title(\"Data set GDg2+\", fontsize=16, y=1.15)\n",
    "axr.text(1.5,-0.6, 'Traditional treatments', fontsize=13, ha='center')\n",
    "axr.text(4.7,-0.6, 'Binary distances', fontsize=13, ha='center')\n",
    "\n",
    "for letter, ax in zip('ABCDEFGHIJ', (axl, axr)):\n",
    "    ax.text(-0.1, 1.2, letter, ha='left', va='center', fontsize=17, weight='bold',\n",
    "            transform=ax.transAxes,\n",
    "            bbox=dict(facecolor='white', edgecolor='white', alpha=0.9))\n",
    "\n",
    "\n",
    "f.savefig('paperimages/coph_baker_corrs.pdf', dpi=400)\n",
    "f.savefig('paperimages/coph_baker_corrs.png', dpi=600)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap with all treatment/distance metrics for both GDg2 data sets - Example with all 9 binary distance metrics\n",
    "\n",
    "Change the treatments and colnames list to account for all binary distance metrics and the two ionization modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames = ['P -', 'NP -', 'NGP -', 'P +', 'NP +', 'NGP +','dice -', 'hamming -',\n",
    "            'jaccard -', 'rogerstanimoto -', 'sokalmichener -', 'sokalsneath -','yule -', \n",
    "            'kulsinski -', 'russellrao -','dice +', 'hamming +', 'jaccard +', 'rogerstanimoto +', \n",
    "            'sokalmichener +', 'sokalsneath +', 'yule +', 'kulsinski +', 'russellrao +']\n",
    "\n",
    "treatments = colnames\n",
    "\n",
    "collectedHCAs = {}\n",
    "for name in treatments:\n",
    "    tname, mode = name.split()\n",
    "    if mode == '-':\n",
    "        collectedHCAs[name] = HCA_all['GD_neg_global2'][tname]\n",
    "    else:\n",
    "        collectedHCAs[name] = HCA_all['GD_pos_global2'][tname]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_correlations = create_HCA_correlations(collectedHCAs, treatments, colnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(18,14))\n",
    "\n",
    "combineCK(all_correlations)\n",
    "\n",
    "pm = plot_partitioned_df_asheatmap(all_correlations['CK'], ax=ax, vmin=-0.2, vmax=1, fontsize=10,\n",
    "                                    cmap=sns.color_palette(\"rocket_r\", as_cmap=True),\n",
    "                                    partition_point=6, top_rotate=True)\n",
    "\n",
    "f.suptitle(\"Baker's gamma (Kendall) correlation (upper) and cophenetic correlation (lower)\", fontsize=16, y=1.03)\n",
    "\n",
    "ax.text(3,-3, 'Traditional treatments', fontsize=14, ha='center')\n",
    "ax.text(14,-3, 'Binary distances', fontsize=14, ha='center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Dendrogram (HCAs) Sample Discrimination\n",
    "\n",
    "To evaluate the discrimination achieved with each HCA, 3 different metrics were used (explained in the paper):\n",
    "\n",
    "- **Discrimination Distance** - the average of “group discrimination distance”. For each group, the discrimination distance is 0 if the group is not “correctly clustered” or it is the distance between the node that includes all the samples of the group and the next closest node (including those samples) in the agglomerative procedure, normalized by the maximum distance of any pair of nodes in the final resulting clustering.\n",
    "- **Correct Clustering Percentage** - the percentage of the groups who are correctly clustered.\n",
    "- **Correct First Cluster Percentage** - the percentage of samples whose first clustering was only with a sample(s) from its group.\n",
    "\n",
    "Correct (Group) Clustering definition - samples of a group all clustered together before any other clustering with other samples or already-formed clusters in the agglomerative procedure.\n",
    "\n",
    "Functions applied here (`dist_discrim` and `correct_1stcluster_fraction`) from multianalysis.py file of this repository with explanations of each step to calculate the different metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_clustering_metrics(res_dict, labels):\n",
    "    \"\"\"Fill dict with clustering performance metrics.\"\"\"\n",
    "    \n",
    "    discrim = ma.dist_discrim(res_dict['Z'], labels, # all samples have the same order\n",
    "                              method = 'average')\n",
    "    res_dict['Average discrim dist'] = discrim[0]\n",
    "    correct = np.array(list(discrim[1].values()))\n",
    "    \n",
    "    classes = pd.unique(labels)\n",
    "    res_dict['% correct clustering'] = (100/len(classes)) * len(correct[correct>0])\n",
    "\n",
    "    # Correct First Cluster Percentage\n",
    "    res_dict['% correct 1st clustering'] = 100 * ma.correct_1stcluster_fraction(res_dict['Z'],labels)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute clustering metrics for the dendrograms built from the GDg2- and GDg2+ data sets treated with one of 3 combinations of intensity-based pre-treatments or with BinSim with one of nine binary distance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collectedHCAs exists!\n",
    "\n",
    "for name, res_dict in collectedHCAs.items():\n",
    "    compute_clustering_metrics(res_dict, datasets['GD_pos_class2']['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build table - summary of results\n",
    "clust_performance = {}\n",
    "\n",
    "for metric in ('Average discrim dist', '% correct clustering', '% correct 1st clustering'):\n",
    "    clust_performance[metric] = {d: collectedHCAs[d][metric] for d in collectedHCAs}\n",
    "clust_performance = pd.DataFrame(clust_performance, index=collectedHCAs)\n",
    "clust_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clust_performance_pos = clust_performance[clust_performance.index.str.endswith('+')]\n",
    "clust_performance_neg = clust_performance[clust_performance.index.str.endswith('-')]\n",
    "#clust_performance_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering Results \n",
    "\n",
    "All different binary distance measures lead to similar results, so Jaccard dissimilarity will represent the binary distance metrics for all the 6 benchmark data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style(\"darkgrid\"):\n",
    "    f, axs = plt.subplots(2, 3, figsize=(12, 9), sharey='row', sharex='col', constrained_layout=True)\n",
    "    for ax in axs.ravel():\n",
    "        ax.tick_params(labelsize=14)\n",
    "        ax.xaxis.label.set_size(16)\n",
    "        ax.axhspan(-0.5, 2.45, color='red', alpha=0.2)\n",
    "        ax.axhspan(2.55, 11.5, color='darkblue', alpha=0.2)\n",
    "    sns.barplot(y=clust_performance_neg.index, x=clust_performance_neg['% correct clustering'], ax=axs[0][1], palette='tab10', ec='0.3')\n",
    "    sns.barplot(y=clust_performance_neg.index, x=clust_performance_neg['% correct 1st clustering'], ax=axs[0][2], palette='tab10', ec='0.3')\n",
    "    sns.barplot(y=clust_performance_neg.index, x=clust_performance_neg['Average discrim dist'], ax=axs[0][0], palette='tab10', ec='0.3')\n",
    "\n",
    "    sns.barplot(y=clust_performance_pos.index, x=clust_performance_pos['Average discrim dist'], ax=axs[1][0], palette='tab10', ec='0.3')\n",
    "    sns.barplot(y=clust_performance_pos.index, x=clust_performance_pos['% correct clustering'], ax=axs[1][1], palette='tab10', ec='0.3')\n",
    "    sns.barplot(y=clust_performance_pos.index, x=clust_performance_pos['% correct 1st clustering'], ax=axs[1][2], palette='tab10', ec='0.3')\n",
    "\n",
    "    for ax in axs[0]:\n",
    "        ax.xaxis.label.set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dendrograms Discrimination Results\n",
    "\n",
    "Compute clustering metrics for the dendrograms built from the 6 benchmark data sets treated with one of 4 pre-treatments studied here (for BinSim, Jaccard dissimilarity was used to build the dendrograms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HCA_performance = []\n",
    "for name, dataset in datasets.items():\n",
    "    if name == 'vitis_types':\n",
    "        continue\n",
    "    for treatment in ('P', 'NP', 'NGP', 'P_RF', 'NP_RF', 'NGP_RF', 'BinSim'):\n",
    "        compute_clustering_metrics(HCA_all[name][treatment], datasets[name]['target'])\n",
    "        perform = {'dataset': name, 'treatment': treatment,\n",
    "                   'Discrimination Distance': HCA_all[name][treatment]['Average discrim dist'],\n",
    "                   '% correct clusters': HCA_all[name][treatment]['% correct clustering'],\n",
    "                   '% correct 1st clustering': HCA_all[name][treatment]['% correct 1st clustering']}\n",
    "        HCA_performance.append(perform)\n",
    "        \n",
    "HCA_performance = pd.DataFrame(HCA_performance)\n",
    "\n",
    "#cv_dsnames = {'GD_neg_global2': 'global2 -',\n",
    "              #'GD_pos_global2': 'global2 +',\n",
    "              #'GD_neg_class2': 'class2 -',\n",
    "              #'GD_pos_class2': 'class2 +',\n",
    "              #'YD': 'YD'}\n",
    "cv_dsnames = {name:datasets[name]['name'] for name in datasets}\n",
    "\n",
    "HCA_performance2 = HCA_performance.assign(dataset = HCA_performance['dataset'].map(cv_dsnames))\n",
    "HCA_performance2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_treatment(t):\n",
    "    if t == 'BinSim':\n",
    "        return t\n",
    "    if t.endswith('RF'):\n",
    "        return 'RF imputation ' + t.split('_', 1)[0]\n",
    "    else:\n",
    "        return '1/2 min imputation ' + t\n",
    "\n",
    "\n",
    "\n",
    "t_treatment = HCA_performance2['treatment'].transform(transform_treatment)\n",
    "HCA_performance_table = HCA_performance2.copy()\n",
    "HCA_performance_table['treatment'] = t_treatment\n",
    "#HCA_performance_table\n",
    "HCA_performance_table.to_excel('paperimages/HCA_performance_table.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HCA_performance_MinBinSim = HCA_performance2[HCA_performance2['treatment'].isin(['P', 'NP', 'NGP', 'BinSim'])]\n",
    "HCA_performance_RFBinSim = HCA_performance2[HCA_performance2['treatment'].isin(['P_RF', 'NP_RF', 'NGP_RF', 'BinSim'])]\n",
    "\n",
    "num_halfmin = HCA_performance_MinBinSim.iloc[:, 2:]\n",
    "num_RF = HCA_performance_RFBinSim.iloc[:, 2:]\n",
    "num_RF.index = HCA_performance_MinBinSim.index\n",
    "\n",
    "HCA_performance_max = num_halfmin.where(num_halfmin > num_RF, num_RF)\n",
    "HCA_performance_best = pd.concat([HCA_performance_MinBinSim.iloc[:, :2], HCA_performance_max], axis=1)          \n",
    "\n",
    "HCA_performance_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p4 = sns.color_palette('tab10', 4)\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    f, axs = plt.subplots(1, 3, figsize=(12, 4), constrained_layout=True)\n",
    "    sns.barplot(x=\"dataset\", y=\"Discrimination Distance\", hue=\"treatment\", data=HCA_performance_MinBinSim, ax=axs[0], palette=p4)\n",
    "    sns.barplot(x=\"dataset\", y=\"% correct clusters\", hue=\"treatment\", data=HCA_performance_MinBinSim, ax=axs[1], palette=p4)\n",
    "    sns.barplot(x=\"dataset\", y=\"% correct 1st clustering\", hue=\"treatment\", data=HCA_performance_MinBinSim, ax=axs[2], palette=p4)\n",
    "    axs[1].legend().set_visible(False)\n",
    "    axs[2].legend().set_visible(False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p4 = sns.color_palette('tab10', 4)\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    f, axs = plt.subplots(1, 3, figsize=(12, 4), constrained_layout=True)\n",
    "    sns.barplot(x=\"dataset\", y=\"Discrimination Distance\", hue=\"treatment\", data=HCA_performance_RFBinSim, ax=axs[0], palette=p4)\n",
    "    sns.barplot(x=\"dataset\", y=\"% correct clusters\", hue=\"treatment\", data=HCA_performance_RFBinSim, ax=axs[1], palette=p4)\n",
    "    sns.barplot(x=\"dataset\", y=\"% correct 1st clustering\", hue=\"treatment\", data=HCA_performance_RFBinSim, ax=axs[2], palette=p4)\n",
    "    axs[1].legend().set_visible(False)\n",
    "    axs[2].legend().set_visible(False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best of two imputations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p4 = sns.color_palette('tab10', 4)\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    f, axs = plt.subplots(1, 3, figsize=(12, 4), constrained_layout=True)\n",
    "    sns.barplot(x=\"dataset\", y=\"Discrimination Distance\", hue=\"treatment\", data=HCA_performance_best, ax=axs[0], palette=p4)\n",
    "    sns.barplot(x=\"dataset\", y=\"% correct clusters\", hue=\"treatment\", data=HCA_performance_best, ax=axs[1], palette=p4)\n",
    "    sns.barplot(x=\"dataset\", y=\"% correct 1st clustering\", hue=\"treatment\", data=HCA_performance_best, ax=axs[2], palette=p4)\n",
    "    axs[1].legend().set_visible(False)\n",
    "    axs[2].legend().set_visible(False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means Clustering Analysis\n",
    "\n",
    "K-means clustering analysis was applied by using the appropriate functions of the scikit-learn as done in the following cells.\n",
    "\n",
    "#### K-means clustering was applied to all the 4 differently-treated data sets for each of the 6 benchmark data sets\n",
    "\n",
    "The number of clusters chosen was equal to the amount of groups. Apart from this, default parameters were used (Euclidian distance was used for the BinSim case since in this method, binary distance metrics can't be used).\n",
    "\n",
    "K-means clustering analysis has an intrinsically random side to it depending on the starting position of the clusters centroids and due to the existence of local minima. Due to this randomness, the algorithm was repeated 15 (n) times and the result with the least inertia (greater minimization of the objective function - sum of squared distances of the samples to the cluster centroids) was retained (best 10% of results, in this case, only the best).\n",
    "\n",
    "To evaluate the discrimination achieved with each K-means Clustering, 3 different metrics were used (explained in the paper):\n",
    "\n",
    "- **Discrimination Distance** (for K-means clustering, identical idea to HCA)\n",
    "- **Correct Clustering Percentage** (for K-means clustering, identical idea to HCA)\n",
    "- **Adjusted Rand Index** (calculated by scikit-learn - `adjusted_rand_index`) - proportion of sample pairs which are correctly clustered or correctly not clustered, adjusted for the expected percentage of samples which would be in those situations randomly.\n",
    "\n",
    "The function `Kmeans_discrim` from multianalysis.py was applied to calculate these metrics with explanations of each step to calculate the different metrics.\n",
    "\n",
    "Correct clustering definition - K-means Cluster contains all and only the samples of a single group (stricter definition than in HCA). Samples of a group can all be together in a cluster, but if another sample (of another group) is present, the group is not correctly clustered. Thus, the Correct Clustering Percentage is expected to be lower in this case.\n",
    "\n",
    "In this case, the distances are calculated by the distance between different cluster centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.cluster as skclust\n",
    "from sklearn.metrics import adjusted_rand_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_KMeans(dataset, treatment, iter_num=150, best_fraction=0.1):\n",
    "    \"Perform K-means Clustering Analysis and calculate discrimination evaluation metrics.\"\n",
    "    \n",
    "    sample_labels = datasets[dataset]['target']\n",
    "    n_classes = len(pd.unique(sample_labels))\n",
    "    \n",
    "    df = datasets[dataset][treatment]\n",
    "    \n",
    "    discrim = ma.Kmeans_discrim(df, sample_labels,\n",
    "                                method='average', \n",
    "                                iter_num=iter_num,\n",
    "                                best_fraction=best_fraction)\n",
    "\n",
    "    \n",
    "    # Lists for the results of the best k-means clustering\n",
    "    average = []\n",
    "    correct = []\n",
    "    rand = []\n",
    "    \n",
    "    for j in discrim:\n",
    "        global_disc_dist, disc_dists, rand_index, SSE = discrim[j]\n",
    "        \n",
    "        # Average of discrimination distances\n",
    "        average.append(global_disc_dist) \n",
    "        \n",
    "        # Correct Clustering Percentages\n",
    "        all_correct = np.array(list(disc_dists.values()))\n",
    "        correct.append(len(all_correct[all_correct>0]))\n",
    "        \n",
    "        # Adjusted Rand Index\n",
    "        rand.append(rand_index) \n",
    "    \n",
    "    return{'dataset': dataset,\n",
    "           'treatment': treatment,\n",
    "           'Discrimination Distance': np.median(average),\n",
    "           '% correct clusters':np.median(correct)*100/n_classes,\n",
    "           'Rand Index': np.median(rand)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: for debugging\n",
    "iter_num=15\n",
    "# otherwise\n",
    "#iter_num=150\n",
    "\n",
    "KMeans_all = []\n",
    "\n",
    "for dsname in ('GD_neg_global2', 'GD_pos_global2', 'GD_neg_class2', 'GD_pos_class2', 'YD', 'YD2'):\n",
    "    for treatment in ('P', 'NP', 'NGP', 'P_RF', 'NP_RF', 'NGP_RF', 'BinSim'):\n",
    "        print(f'performing KMeans on {dsname} with treatment {treatment}' , end=' ...')\n",
    "        KMeans_all.append(perform_KMeans(dsname, treatment, iter_num=iter_num))\n",
    "        print('done!')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KMeans_all = pd.DataFrame(KMeans_all)\n",
    "#cv_dsnames = {'GD_neg_global2': 'global2 -',\n",
    "              #'GD_pos_global2': 'global2 +',\n",
    "              #'GD_neg_class2': 'class2 -',\n",
    "              #'GD_pos_class2': 'class2 +',\n",
    "              #'YD': 'YD'}\n",
    "cv_dsnames = {name:datasets[name]['name'] for name in datasets}\n",
    "KMeans_all2 = KMeans_all.assign(dataset = KMeans_all['dataset'].map(cv_dsnames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KMeans_all2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_treatment(t):\n",
    "    if t == 'BinSim':\n",
    "        return t\n",
    "    if t.endswith('RF'):\n",
    "        return 'RF imputation ' + t.split('_', 1)[0]\n",
    "    else:\n",
    "        return '1/2 min imputation ' + t\n",
    "\n",
    "\n",
    "\n",
    "t_treatment = KMeans_all2['treatment'].transform(transform_treatment)\n",
    "KMeans_performance_table = KMeans_all2.copy()\n",
    "KMeans_performance_table['treatment'] = t_treatment\n",
    "\n",
    "KMeans_performance_table.to_excel('paperimages/Kmeans_performance_table.xlsx', index=False)\n",
    "KMeans_performance_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KMeans_performance_MinBinSim = KMeans_all2[KMeans_all2['treatment'].isin(['P', 'NP', 'NGP', 'BinSim'])]\n",
    "KMeans_performance_RFBinSim = KMeans_all2[KMeans_all2['treatment'].isin(['P_RF', 'NP_RF', 'NGP_RF', 'BinSim'])]\n",
    "\n",
    "num_halfmin = KMeans_performance_MinBinSim.iloc[:, 2:]\n",
    "num_RF = KMeans_performance_RFBinSim.iloc[:, 2:]\n",
    "num_RF.index = KMeans_performance_MinBinSim.index\n",
    "\n",
    "KMeans_performance_max = num_halfmin.where(num_halfmin > num_RF, num_RF)\n",
    "KMeans_performance_best = pd.concat([KMeans_performance_MinBinSim.iloc[:, :2], KMeans_performance_max], axis=1)          \n",
    "\n",
    "KMeans_performance_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p4 = sns.color_palette('tab10', 4)\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    f, axs = plt.subplots(1, 3, figsize=(12, 4), constrained_layout=True)\n",
    "    sns.barplot(x=\"dataset\", y=\"Discrimination Distance\", hue=\"treatment\", data=KMeans_performance_MinBinSim, ax=axs[0], palette=p4)\n",
    "    sns.barplot(x=\"dataset\", y=\"% correct clusters\", hue=\"treatment\", data=KMeans_performance_MinBinSim, ax=axs[1], palette=p4)\n",
    "    sns.barplot(x=\"dataset\", y=\"Rand Index\", hue=\"treatment\", data=KMeans_performance_MinBinSim, ax=axs[2], palette=p4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p4 = sns.color_palette('tab10', 4)\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    f, axs = plt.subplots(1, 3, figsize=(12, 4), constrained_layout=True)\n",
    "    sns.barplot(x=\"dataset\", y=\"Discrimination Distance\", hue=\"treatment\", data=KMeans_performance_RFBinSim, ax=axs[0], palette=p4)\n",
    "    sns.barplot(x=\"dataset\", y=\"% correct clusters\", hue=\"treatment\", data=KMeans_performance_RFBinSim, ax=axs[1], palette=p4)\n",
    "    sns.barplot(x=\"dataset\", y=\"Rand Index\", hue=\"treatment\", data=KMeans_performance_RFBinSim, ax=axs[2], palette=p4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showing best of the two imputations and then P, NP, NGP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p4 = sns.color_palette('tab10', 4)\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    f, axs = plt.subplots(1, 3, figsize=(12, 4), constrained_layout=True)\n",
    "    sns.barplot(x=\"dataset\", y=\"Discrimination Distance\", hue=\"treatment\", data=KMeans_performance_best, ax=axs[0], palette=p4)\n",
    "    sns.barplot(x=\"dataset\", y=\"% correct clusters\", hue=\"treatment\", data=KMeans_performance_best, ax=axs[1], palette=p4)\n",
    "    sns.barplot(x=\"dataset\", y=\"Rand Index\", hue=\"treatment\", data=KMeans_performance_best, ax=axs[2], palette=p4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Clustering performance\n",
    "\n",
    "HCA and K-means Clustering results combined.\n",
    "\n",
    "Only the best of the two imputations for P, NP, NGP is shown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WE must try to make the minus sign a little longer in the figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HCA_performance_best_old = HCA_performance_best.copy()\n",
    "KMeans_performance_best_old = KMeans_performance_best.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HCA_performance_best = HCA_performance_best_old.copy()\n",
    "KMeans_performance_best = KMeans_performance_best_old.copy()\n",
    "HCA_performance_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def endminus(x):\n",
    "    if x.endswith('-'):\n",
    "        return x.replace('-', '−')\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "HCA_performance_best['dataset'] = HCA_performance_best['dataset'].apply(endminus)\n",
    "KMeans_performance_best['dataset'] = KMeans_performance_best['dataset'].apply(endminus)\n",
    "\n",
    "HCA_performance_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p4 = sns.color_palette('tab10', 4)\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    with sns.plotting_context(\"notebook\", font_scale=1.4):\n",
    "        f, axs = plt.subplots(2, 3, figsize=(14, 8), constrained_layout=True)\n",
    "        f.set_constrained_layout_pads(hspace=0.05, wspace=0.05)\n",
    "        sns.barplot(x=\"dataset\", y=\"Discrimination Distance\", hue=\"treatment\", data=HCA_performance_best, ax=axs[0][2], palette=p4)\n",
    "        sns.barplot(x=\"dataset\", y=\"% correct clusters\", hue=\"treatment\", data=HCA_performance_best, ax=axs[0][0], palette=p4)\n",
    "        sns.barplot(x=\"dataset\", y=\"% correct 1st clustering\", hue=\"treatment\", data=HCA_performance_best, ax=axs[0][1], palette=p4)\n",
    "\n",
    "        sns.barplot(x=\"dataset\", y=\"Discrimination Distance\", hue=\"treatment\", data=KMeans_performance_best, ax=axs[1][2], palette=p4)\n",
    "        sns.barplot(x=\"dataset\", y=\"% correct clusters\", hue=\"treatment\", data=KMeans_performance_best, ax=axs[1][0], palette=p4)\n",
    "        sns.barplot(x=\"dataset\", y=\"Rand Index\", hue=\"treatment\", data=KMeans_performance_best, ax=axs[1][1], palette=p4)\n",
    "        for i, ax in enumerate(axs.ravel()):\n",
    "            ax.set_ylim(0,100)\n",
    "            ax.xaxis.label.set_visible(False)\n",
    "            ax.legend().set_visible(False)\n",
    "            ax.tick_params(axis='x', which='major', labelsize=12)\n",
    "        #axs[0][1].set_title('HCA', fontsize=15)\n",
    "        #axs[1][1].set_title('KMeans', fontsize=15)\n",
    "        axs[0][2].legend(bbox_to_anchor=(1,1), loc=\"upper right\", framealpha=1)\n",
    "        axs[1][2].set_ylim(0,1.0)\n",
    "        axs[0][2].set_ylim(0,1)\n",
    "        axs[1][1].set_ylim(0,1)\n",
    "        \n",
    "        for letter, ax in zip('ABCDEFGHIJ', axs.ravel()):\n",
    "            ax.text(0.05, 0.9, letter, ha='left', va='center', fontsize=16, weight='bold',\n",
    "                    transform=ax.transAxes,\n",
    "                    bbox=dict(facecolor='white', edgecolor='white', alpha=0.9))\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        f.savefig('paperimages/clust_performance.pdf' , dpi=200)\n",
    "        f.savefig('paperimages/clust_performance.png' , dpi=600)\n",
    "        f.savefig('paperimages/clust_performance.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
