{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Simplification in Metabolomics\n",
    "\n",
    "Notebook to support the study on the application of **Bin**ary **Sim**plification as a competing form of pre-processing procedure for high-resolution metabolomics data.\n",
    "\n",
    "This is notebook `paper_binsim_data_prep.ipynb`\n",
    "\n",
    "In this notebook, the benchmark datasets are cleaned, transformed into data matrices and pre-treatment procedures are carried out, including BinSim.\n",
    "\n",
    "\n",
    "## Organization of the Notebook\n",
    "\n",
    "- Set up database of data sets\n",
    "- Application of different pre-treatments (including BinSim) to each data set\n",
    "- Creation of persistance storage of data\n",
    "- Analysis of dataset characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Needed Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from pathlib import Path\n",
    "\n",
    "# json for persistence\n",
    "import json\n",
    "from time import perf_counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.testing import assert_frame_equal\n",
    "\n",
    "import scipy.spatial.distance as dist\n",
    "import scipy.cluster.hierarchy as hier\n",
    "import scipy.stats as stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib import ticker\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "# Metabolinks package\n",
    "import metabolinks as mtl\n",
    "import metabolinks.transformations as transf\n",
    "\n",
    "# Python files in the repository\n",
    "import multianalysis as ma\n",
    "from elips import plot_confidence_ellipse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of dataset records\n",
    "\n",
    "`datasets` is the global dict that holds all data sets. It is a **dict of dict's**.\n",
    "\n",
    "Each data set is **represented as a dict**.\n",
    "\n",
    "Each record has the following fields (keys):\n",
    "\n",
    "- `name`: name of the data set (see below the list of data sets)\n",
    "- `source`: the biological source for each dataset\n",
    "- `mode`: the aquisition mode\n",
    "- `alignment`: the alignment used to generate the data matrix\n",
    "- `data`: the data matrix, the original data before any pre-treatments\n",
    "- `target`: the sample labels, possibly already integer encoded\n",
    "- `<treatment name>`: transformed data matrix. These treatment names can be\n",
    "    - `Ionly`: missing value imputation by 1/2 min, only\n",
    "    - `P`: Pareto scaled data, after missing value imputation\n",
    "    - `NP`: Missing value imputed, Pareto scaled and normalized\n",
    "    - `NGP`: Missing value imputed, normalized, glog transformed and Pareto scaled\n",
    "    - `BinSim`: binary simplified data\n",
    "\n",
    "The keys of `datasets` may be shared with dicts holding records resulting from comparison analysis.\n",
    "\n",
    "Here are the keys (and respective names) of datasets used in this study:\n",
    "\n",
    "- GD_neg_global2 (GDg2-)\n",
    "- GD_pos_global2 (GDg2+)\n",
    "- GD_neg_class2 (GDc2-)\n",
    "- GD_pos_class2 (GDc2+)\n",
    "- YD (YD 2/15)\n",
    "- YD2 (YD 6/15)\n",
    "- vitis_types (GD types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of grapevine data sets\n",
    "\n",
    "Grapevine Datasets (Negative and Positive) - 33 samples belonging to 11 different grapevine varieties (3 samples per variety/biological group) of FT-ICR-MS metabolomics data obtained in negative and positive ionization mode.\n",
    "\n",
    "5 different _Vitis_ species (other than _V. vinifera_) varieties:\n",
    "\n",
    "- CAN - 3 Samples (14, 15, 16) of _V. candicans Engelmann_ (VIVC variety number: 13508)\n",
    "- RIP - 3 Samples (17, 18, 19) of _V. riparia Michaux_ (Riparia Gloire de Montpellier, VIVC variety number: 4824) \n",
    "- ROT - 3 Samples (20, 21, 22) of _V. rotundifolia_ (Muscadinia Rotundifolia Michaux cv. Rotundifolia, VIVC variety number: 13586)\n",
    "- RU - 3 Samples (35, 36, 37) of _V. rupestris Scheele_ (Rupestris du lot, VIVC variety number: 10389)\n",
    "- LAB - 3 Samples (8, 9, 10) of _V. labrusca_ (Isabella, VIVC variety number: 5560)\n",
    "\n",
    "6 different _V. vinifera_ cultivars varieties are:\n",
    "\n",
    "- SYL - 3 samples (11, 12, 13) of the subspecies _sylvestris_ (VIVC variety number: -)\n",
    "- CS - 3 Samples (29, 30, 31) of the subspecies _sativa_ cultivar Cabernet Sauvignon (VIVC variety number: 1929)\n",
    "- PN - 3 Samples (23, 24, 25) of the subspecies _sativa_ cultivar Pinot Noir (VIVC variety number: 9279)\n",
    "- REG - 3 Samples (38, 39, 40) of the subspecies _sativa_ cultivar Regent (VIVC variety number: 4572)\n",
    "- RL - 3 Samples (26, 27, 28) of the subspecies _sativa_ cultivar Riesling Weiss (VIVC variety number: 10077)\n",
    "- TRI - 3 Samples (32, 33, 34) of the subspecies _sativa_ cultivar Cabernet Sauvignon (VIVC variety number: 15685)\n",
    "\n",
    "Data acquired by Maia et al. (2020):\n",
    "\n",
    "- Maia M, Ferreira AEN, Nascimento R, et al. Integrating metabolomics and targeted gene expression to uncover potential biomarkers of fungal / oomycetes ‑ associated disease susceptibility in grapevine. Sci Rep. Published online 2020:1-15. doi:10.1038/s41598-020-72781-2\n",
    "- Maia M, Figueiredo A, Silva MS, Ferreira A. Grapevine untargeted metabolomics to uncover potential biomarkers of fungal/oomycetes-associated diseases. 2020. doi:10.6084/m9.figshare.12357314.v1\n",
    "\n",
    "**Peak Alignment** and **Peak Filtering** were performed with function `metabolinks.peak_alignment.align()`. Human leucine enkephalin (Sigma Aldrich) was used as the reference feature (internal standard, [M+H]+ = 556.276575 Da or [M-H]- = 554.262022 Da).\n",
    "\n",
    "**4** data matrices were constructed from this data:\n",
    "\n",
    "- Data sets named `GD_pos_global2` (GDg2+) and `GD_neg_global2` (GDg2-) were generated after retaining only features that occur (globally) at least twice in all 33 samples of the data sets (filtering/alignment) for the **positive mode** data acquisition and the **negative mode** data acquisition, respectively.\n",
    "- Data sets named `GD_pos_class2` (GDc2+) and `GD_neg_class2` (GDc2-) were generated after retaining only features that occur at least twice in the three replicates of at least one _Vitis_ variety in the data sets (filtering/alignment) for the **positive mode** data acquisition and the **negative mode** data acquisition, respectively.\n",
    "\n",
    "For the purpose of assessing the performance of supervised methods each of these four datasets was used with target labels defining classes corresponding to replicates of each of the 11 Vitis species/cultivars.\n",
    "\n",
    "For the purpose of assessing the performance of supervised methods under a binary (two-class) problem, data set `GD_neg_class2` was also used with target labels defining two classes: Vitis vinifera cultivars and \"wild\", non-vinifera Vitis species. This is dataset `vitis_types` (GD types)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of the yeast data set\n",
    "\n",
    "Yeast dataset - 15 samples belonging to 5 different yeast strains of _Saccharomyces cerevisiae_ (3 biological replicates per strain/biological group) of FT-ICR-MS metabolomics data obtained in positive ionization mode. The 5 strains were: the reference strain BY4741 (represented as BY) and 4 single-gene deletion mutants of this strain – ΔGLO1 (GLO1), ΔGLO2 (GLO2), ΔGRE3 (GRE3) and ΔENO1 (ENO1). These deleted genes are directly or indirectly related to methylglyoxal metabolism.\n",
    "\n",
    "Data acquired by Luz et al. (2021):\n",
    "\n",
    "- Luz J, Pendão AS, Silva MS, Cordeiro C. FT-ICR-MS based untargeted metabolomics for the discrimination of yeast mutants. 2021. doi:10.6084/m9.figshare.15173559.v1\n",
    "\n",
    "**Peak Alignment** and **Peak Filtering** was performed with MetaboScape 4.0 software (see reference above for details in sample preparation, pre-processing, formula assignment). In short, Yeast Dataset was obtained with Electrospray Ionization in Positive Mode and pre-processed by MetaboScape 4.0 (Bruker Daltonics). Human leucine enkephalin (Sigma Aldrich) was used as the reference feature (internal standard, [M+H]+ = 556.276575 Da or [M-H]- = 554.262022 Da).\n",
    "\n",
    "**2** data matrices were constructed from this data:\n",
    "\n",
    "- Data set named `YD` (YD 2/15) was generated after retaining only features that occur (globally) at least twice in all 15 samples (filtering/alignment).\n",
    "- Data set named `YD2` (YD 6/15) was generated after retaining only features that occur (globally) at least six times in all 15 samples (filtering/alignment).\n",
    "\n",
    "For the purpose of assessing the performance of supervised methods, this data set was used with target labels defining classes corresponding to replicates of each of the 4 yeast strains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human dataset\n",
    "\n",
    "The human dataset (*HD*) is decribed, processed and pre-treated in notebook `paper_binsim_dataset_HD.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading yeast data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_MetScape_file(filename,\n",
    "                          col_renamer=None,\n",
    "                          add_labels=None,\n",
    "                          remove_ref_feat=None,):\n",
    "    \n",
    "    \"\"\"Read in a MetaboScape bucket table from a CSV file.\"\"\"\n",
    "    \n",
    "    data = pd.read_csv(filename).set_index('Bucket label')\n",
    "    \n",
    "    # optionally rename sample_names\n",
    "    if col_renamer is not None:\n",
    "        data = data.rename(columns=renamer)\n",
    "    \n",
    "    # optionally remove a reference feature (if already normalized)\n",
    "    if remove_ref_feat is not None:\n",
    "        #print(f'Feature {remove_ref_feat}\\n{data.loc[remove_ref_feat, :]}\\n----------')\n",
    "        data = data.drop(index=[remove_ref_feat])\n",
    "    \n",
    "    # split in peak metadata and intensities\n",
    "    peak_cols = ['m/z', 'Name', 'Formula']\n",
    "    intensity_cols = [c for c in list(data.columns) if c not in peak_cols]\n",
    "    peaks = data[peak_cols]\n",
    "    intensities = data[intensity_cols]\n",
    "\n",
    "    # replace zeros for NaN's\n",
    "    intensities = intensities.replace(0, np.nan).dropna(how='all')\n",
    "    \n",
    "    # force peaks to have the same features as the (trimmed) intensities\n",
    "    peaks = peaks.reindex(intensities.index)\n",
    "\n",
    "    # optionally, add labels to intensities\n",
    "    if add_labels is not None:\n",
    "        intensities = mtl.add_labels(intensities, labels=add_labels)\n",
    "    \n",
    "    return {'filename': filename,\n",
    "            'peaks':peaks,\n",
    "            'intensities': intensities}\n",
    "\n",
    "def renamer(colname):\n",
    "    # Util to optionally remove all those 00000 from sample names\n",
    "    return ''.join(colname.split('00000'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels of the 5 biological groups (5 yeast strains) - only added after\n",
    "yeast_classes = 'WT ΔGRE3 ΔENO1 ΔGLO1 ΔGLO2'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the file and keep results in dicts \n",
    "\n",
    "prefix_to_drop = None # change to 'ENO' to remove ENO strain\n",
    "\n",
    "# MScape non-normalized dataset\n",
    "MS_data = read_MetScape_file('5yeasts_notnorm.csv', \n",
    "                             remove_ref_feat=None,\n",
    "                             add_labels=None,\n",
    "                             col_renamer=renamer)\n",
    "\n",
    "#MS_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep features that appear in at least two samples\n",
    "yeast_datamatrix = transf.keep_atleast(MS_data['intensities'].transpose(), minimum=2)\n",
    "yeast_datamatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building data-sets data base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {}\n",
    "# From the alignments_new store\n",
    "\n",
    "# GD_neg_global2 (GDg2-)\n",
    "data_df = pd.HDFStore('alignments_new.h5').get('all_1ppm_min2_neg').transpose()\n",
    "gd_labels = mtl.parse_data(data_df, labels_loc='label').sample_labels\n",
    "\n",
    "datasets['GD_neg_global2'] = {'source': 'grapevine',\n",
    "                              'alignment': '1-2',\n",
    "                              'mode': '-',\n",
    "                              'name': 'GDg2-',\n",
    "                              'data': data_df,\n",
    "                              'target': gd_labels,\n",
    "                              'classes': list(pd.unique(gd_labels))}\n",
    "\n",
    "# GD_pos_global2 (GDg2+)\n",
    "data_df = pd.HDFStore('alignments_new.h5').get('all_1ppm_min2_pos').transpose()\n",
    "labels = mtl.parse_data(data_df, labels_loc='label').sample_labels\n",
    "\n",
    "datasets['GD_pos_global2'] = {'source': 'grapevine',\n",
    "                              'alignment': '1-2',\n",
    "                              'mode': '+',\n",
    "                              'name': 'GDg2+',\n",
    "                              'data': data_df,\n",
    "                              'target': gd_labels,\n",
    "                              'classes': list(pd.unique(gd_labels))}\n",
    "\n",
    "# GD_neg_class2 (GDc2-)\n",
    "data_df = pd.HDFStore('alignments_new.h5').get('groups_1ppm_min2_all_1ppm_neg').transpose()\n",
    "labels = mtl.parse_data(data_df, labels_loc='label').sample_labels\n",
    "\n",
    "datasets['GD_neg_class2'] = {'source': 'grapevine',\n",
    "                              'alignment': '2-1',\n",
    "                              'mode': '-',\n",
    "                              'name': 'GDc2-',\n",
    "                              'data': data_df,\n",
    "                              'target': gd_labels,\n",
    "                              'classes': list(pd.unique(gd_labels))}\n",
    "\n",
    "# GD_pos_class2 (GDc2+)\n",
    "data_df = pd.HDFStore('alignments_new.h5').get('groups_1ppm_min2_all_1ppm_pos').transpose()\n",
    "labels = mtl.parse_data(data_df, labels_loc='label').sample_labels\n",
    "\n",
    "datasets['GD_pos_class2'] = {'source': 'grapevine',\n",
    "                              'alignment': '2-1',\n",
    "                              'mode': '+',\n",
    "                              'name': 'GDc2+',\n",
    "                              'data': data_df,\n",
    "                              'target': gd_labels,\n",
    "                              'classes': list(pd.unique(gd_labels))}\n",
    "\n",
    "# YD (YD 2/15)\n",
    "yeast_labels = [item for item in yeast_classes for i in range(3)]\n",
    "\n",
    "datasets['YD'] = {'source': 'yeast',\n",
    "                            'alignment': '1-2',\n",
    "                            'mode': '+',\n",
    "                            'name': 'YD 2/15',\n",
    "                            'data': yeast_datamatrix,\n",
    "                            'target': yeast_labels,\n",
    "                            'classes': list(pd.unique(yeast_labels))}\n",
    "\n",
    "# YD (YD 6/15)\n",
    "min_non_na = '6/15'\n",
    "\n",
    "yeast_datamatrix_less_na = transf.keep_atleast(yeast_datamatrix, minimum=eval(min_non_na))\n",
    "\n",
    "datasets['YD2'] = {'source': 'yeast',\n",
    "                            f'alignment': 'at_least {min_non_na}',\n",
    "                            'mode': '+',\n",
    "                            'name': 'YD ' + min_non_na,\n",
    "                            'data': yeast_datamatrix_less_na,\n",
    "                            'target': yeast_labels,\n",
    "                            'classes': list(pd.unique(yeast_labels))}\n",
    "\n",
    "# vitis_types (GD types)\n",
    "vitis_types = {'CAN': 'wild', 'RIP': 'wild', 'ROT': 'wild','RU': 'wild', 'LAB': 'wild',\n",
    "               'SYL': 'wild','REG': 'vinifera','CS': 'vinifera','PN': 'vinifera','RL': 'vinifera',\n",
    "               'TRI': 'vinifera'}\n",
    "\n",
    "gd_type_labels = [vitis_types[lbl] for lbl in gd_labels]\n",
    "\n",
    "datasets['vitis_types'] = {'source': 'grapevine',\n",
    "                            'alignment': '2-1',\n",
    "                            'mode': '-',\n",
    "                            'name': 'GD types',\n",
    "                            'data': datasets['GD_neg_class2']['data'],\n",
    "                            'target': gd_type_labels,\n",
    "                            'classes': list(pd.unique(gd_type_labels))}\n",
    "\n",
    "print('target for grapevine 11-variety data sets')\n",
    "print(datasets['GD_neg_global2']['target'])\n",
    "print('------\\ntarget for 4 yeast strains data set')\n",
    "print(datasets['YD']['target'])\n",
    "print('------\\ntarget for 2-class wild Vitis vs Vitis vinifera data set')\n",
    "print(datasets['vitis_types']['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "datasets['YD2']['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colors for plots to ensure consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11 variety grapevine data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customize label colors for 11 grapevine varieties\n",
    "\n",
    "colours = sns.color_palette('Blues', 3)\n",
    "colours.extend(sns.color_palette('Greens', 3))\n",
    "#colours = sns.cubehelix_palette(n_colors=6, start=2, rot=0, dark=0.2, light=.9, reverse=True)\n",
    "colours.extend(sns.color_palette('flare', 5))\n",
    "\n",
    "ordered_vitis_labels = ('CAN','RIP','ROT','RU','LAB','SYL','REG','CS','PN','RL','TRI')\n",
    "\n",
    "vitis_label_colors = {lbl: c for lbl, c in zip(ordered_vitis_labels, colours)}\n",
    "\n",
    "tab20bcols = sns.color_palette('tab20b', 20)\n",
    "tab20ccols = sns.color_palette('tab20c', 20)\n",
    "tab20cols = sns.color_palette('tab20', 20)\n",
    "tab10cols = sns.color_palette('tab10', 10)\n",
    "dark2cols = sns.color_palette('Dark2', 8)\n",
    "\n",
    "vitis_label_colors['RU'] = tab20bcols[8]\n",
    "vitis_label_colors['CAN'] = tab20ccols[5]\n",
    "vitis_label_colors['REG'] = tab10cols[3]\n",
    "\n",
    "for name in datasets:\n",
    "    if name.startswith('GD'):\n",
    "        datasets[name]['label_colors'] = vitis_label_colors\n",
    "        datasets[name]['sample_colors'] = [vitis_label_colors[lbl] for lbl in datasets[name]['target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.palplot(vitis_label_colors.values())\n",
    "new_ticks = plt.xticks(range(len(ordered_vitis_labels)), ordered_vitis_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 yeast strains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customize label colors for 5 yeast strains\n",
    "\n",
    "colours = sns.color_palette('Set1', 5)\n",
    "yeast_label_colors = {lbl: c for lbl, c in zip(yeast_classes, colours)}\n",
    "datasets['YD']['label_colors'] = yeast_label_colors\n",
    "datasets['YD']['sample_colors'] = [yeast_label_colors[lbl] for lbl in datasets['YD']['target']]\n",
    "datasets['YD2']['label_colors'] = yeast_label_colors\n",
    "datasets['YD2']['sample_colors'] = [yeast_label_colors[lbl] for lbl in datasets['YD2']['target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.palplot(yeast_label_colors.values())\n",
    "new_ticks = plt.xticks(range(len(yeast_classes)), yeast_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 classes of Vitis types (wild and _vinifera_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customize label colors for 2 types of Vitis varieties\n",
    "\n",
    "colours = [vitis_label_colors['SYL'], vitis_label_colors['TRI']]\n",
    "vitis_type_classes = datasets['vitis_types']['classes']\n",
    "vitis_types_label_colors = {lbl: c for lbl, c in zip(vitis_type_classes, colours)}\n",
    "datasets['vitis_types']['label_colors'] = vitis_types_label_colors\n",
    "datasets['vitis_types']['sample_colors'] = [vitis_types_label_colors[lbl] for lbl in datasets['vitis_types']['target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.palplot(datasets['vitis_types']['label_colors'].values())\n",
    "new_ticks = plt.xticks(range(len(datasets['vitis_types']['classes'])), datasets['vitis_types']['classes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Samples and respective target labels of each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def styled_sample_labels(sample_names, sample_labels, label_colors):\n",
    "\n",
    "    meta_table = pd.DataFrame({'label': sample_labels,\n",
    "                               'sample': sample_names}).set_index('sample').T\n",
    "\n",
    "    def apply_label_color(val):\n",
    "        red, green, blue = label_colors[val]\n",
    "        red, green, blue = int(red*255), int(green*255), int(blue*255)   \n",
    "        hexcode = '#%02x%02x%02x' % (red, green, blue)\n",
    "        css = f'background-color: {hexcode}'\n",
    "        return css\n",
    "    \n",
    "    return meta_table.style.applymap(apply_label_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed = mtl.parse_data(datasets['GD_pos_class2']['data'], labels_loc='label')\n",
    "y = datasets['GD_pos_class2']['target']\n",
    "label_colors = datasets['GD_pos_class2']['label_colors']\n",
    "s = styled_sample_labels(parsed.sample_names, y, label_colors)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed = mtl.parse_data(datasets['YD']['data'])\n",
    "y = datasets['YD']['target']\n",
    "label_colors = datasets['YD']['label_colors']\n",
    "s = styled_sample_labels(parsed.sample_names, y, label_colors)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed = mtl.parse_data(datasets['vitis_types']['data'], labels_loc='label')\n",
    "y = datasets['vitis_types']['target']\n",
    "label_colors = datasets['vitis_types']['label_colors']\n",
    "s = styled_sample_labels(parsed.sample_names, y, label_colors)\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup of Random Forest missing-value imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_estimator = ExtraTreesRegressor(n_estimators=10)\n",
    "imputer = IterativeImputer(random_state=0, estimator=rf_estimator,\n",
    "                           n_nearest_features=100, min_value=0.0,\n",
    "                           verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = datasets[\"YD\"][\"data\"]\n",
    "\n",
    "sample_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = perf_counter()\n",
    "print('Starting imputation')\n",
    "\n",
    "imputed_data = imputer.fit_transform(sample_data)\n",
    "\n",
    "end = perf_counter()\n",
    "print(f'Done! took {(end - start):.3f} s')\n",
    "\n",
    "imputed_data = pd.DataFrame(imputed_data, index=sample_data.index, columns=sample_data.columns)\n",
    "imputed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_RF(df, nearest_features=100, n_trees=10):\n",
    "    rf_estimator = ExtraTreesRegressor(n_estimators=n_trees)\n",
    "    imputer = IterativeImputer(random_state=0, estimator=rf_estimator,\n",
    "                           n_nearest_features=nearest_features, min_value=0.0,\n",
    "                           verbose=0)\n",
    "    imputed_data = imputer.fit_transform(df)\n",
    "    return pd.DataFrame(imputed_data, index=df.index, columns=df.columns)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data transformations and pre-treatments\n",
    "\n",
    "Each data set is transformed by Binary simplification and treated by three combinations of more established treatments (generating 4 data sets with different treatments applied).\n",
    "\n",
    "### Traditional and more established  intensity-based Pre-Treatments\n",
    "\n",
    "Missing value imputation is mandatory for the data sets since many statistical methods performed downstream can't handle missing values. Missing value imputation procedure was applied before any treatment: missing values were replaced with half of the minimum intensity value present in the whole data matrix - Limit of Detection type of imputation commonly applied in metabolomics data analysis. This was performed with the `fillna_frac_min`function from metabolinks.\n",
    "\n",
    "#### Combinations of intensity-based pre-treatments made:\n",
    "\n",
    "- Ionly Treatment - Only Missing Value Imputation.\n",
    "\n",
    "- **P Treatment** - Missing Value Imputation and Pareto Scaling.\n",
    "\n",
    "- **NP Treatment** - Missing Value Imputation, Normalization by reference feature (Leucine Enkephalin) and Pareto Scaling.\n",
    "\n",
    "- **NGP Treatment** - Missing Value Imputation, Normalization by reference feature (Leucine Enkephalin), Generalized Logarithmic Transformation and Pareto Scaling.\n",
    "\n",
    "Note: Leucine Enkephalin peak (reference feature) is removed upon normalization. Order of pre-treatments is the order in which they were mentioned.\n",
    "\n",
    "### Binary Simplification (BinSim)\n",
    "\n",
    "- **BinSim Treatment** - `df_to_bool` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Represents Binary Simplification pre-treatment\n",
    "def df_to_bool(df):\n",
    "    \"Transforms data into 'binary' matrices.\"\n",
    "    return df.mask(df.notnull(), 1).mask(df.isnull(), 0)\n",
    "\n",
    "# Performs all pre-treatment combinations mentioned\n",
    "def compute_transf(dataset, norm_ref=None, lamb=None):\n",
    "    \"Computes 3 combinations of pre-treatments and BinSim and returns after treatment datasets in a dict.\"\n",
    "    \n",
    "    data = dataset['data']\n",
    "    \n",
    "    # Imputation of Missing Values\n",
    "    imputed = transf.fillna_frac_min(data, fraction=0.5)\n",
    "    \n",
    "    # Imputation by RF\n",
    "    imputedRF = impute_RF(data, nearest_features=100, n_trees=10)\n",
    "    \n",
    "    \n",
    "    # Normalization by a reference feature\n",
    "    if norm_ref is not None:\n",
    "        norm = transf.normalize_ref_feature(imputed, norm_ref, remove=True)\n",
    "    else:\n",
    "        norm = imputed\n",
    "    \n",
    "    # Normalization by a reference feature RF\n",
    "    if norm_ref is not None:\n",
    "        normRF = transf.normalize_ref_feature(imputedRF, norm_ref, remove=True)\n",
    "    else:\n",
    "        normRF = imputedRF\n",
    "    \n",
    "    # Pareto Scaling and Generalized Logarithmic Transformation\n",
    "    P = transf.pareto_scale(imputed)\n",
    "    NP = transf.pareto_scale(norm)\n",
    "    NGP = transf.pareto_scale(transf.glog(norm, lamb=lamb))\n",
    "\n",
    "    # Pareto Scaling and Generalized Logarithmic Transformation\n",
    "    P_RF = transf.pareto_scale(imputedRF)\n",
    "    NP_RF = transf.pareto_scale(normRF)\n",
    "    NGP_RF = transf.pareto_scale(transf.glog(normRF, lamb=lamb))\n",
    "    \n",
    "    # Store results\n",
    "    dataset['BinSim'] = df_to_bool(data)\n",
    "    dataset['Ionly'] = imputed\n",
    "    dataset['P'] = P\n",
    "    dataset['NP'] = NP\n",
    "    dataset['NGP'] = NGP\n",
    "    \n",
    "    dataset['Ionly_RF'] = imputedRF\n",
    "    dataset['P_RF'] = P_RF\n",
    "    dataset['NP_RF'] = NP_RF\n",
    "    dataset['NGP_RF'] = NGP_RF    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Human leucine enkephalin (Sigma Aldrich) is the reference feature (internal standard, [M+H]+ = 556.276575 Da or [M-H]- = 554.262022 Da) used for these datasets.\n",
    "\n",
    "Search in the grapevine data sets for the reference feature and confirm the reference feature in the yeast data sets with `find_closest_features` from metabolinks package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Theoretical mass for negative mode Leucine Enkephalin - 554.262022\n",
    "# Theoretical mass for positive mode Leucine Enkephalin - 556.276575\n",
    "Leu_Enk_neg = 554.262022\n",
    "Leu_Enk_pos = 556.276575\n",
    "\n",
    "# Reference Feature in the yeast dataset\n",
    "leu_enk_name = '555.2692975341 Da'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_datasets = [name for name in datasets if datasets[name]['source']=='grapevine']\n",
    "\n",
    "for name in query_datasets:\n",
    "    ds = datasets[name]\n",
    "    print(f'looking for reference in {name} ...')\n",
    "    ref_variable = Leu_Enk_neg if ds['mode'] == '-' else Leu_Enk_pos\n",
    "    closest = transf.find_closest_features(ds['data'], features=[ref_variable])\n",
    "    if closest[ref_variable] is not None:\n",
    "        print('Found ref feature', ref_variable)\n",
    "        delta = closest[ref_variable] - ref_variable\n",
    "        print(f'In data: {closest[ref_variable]} delta = {delta:.3e}\\n')\n",
    "\n",
    "query_datasets = [name for name in datasets if name.startswith('YD')]\n",
    "\n",
    "ref_variable = leu_enk_name\n",
    "for name in query_datasets:\n",
    "    ds = datasets[name]\n",
    "    print(f'looking for reference in {name} ...') \n",
    "    closest = transf.find_closest_features(ds['data'], features=[ref_variable])\n",
    "    if closest[ref_variable] is not None:\n",
    "        print('Found ref feature', ref_variable, '\\n')\n",
    "    else:\n",
    "        print('Ref feature not found\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Apply the different pre-treatments and get the results in their respective dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, ds in datasets.items():\n",
    "    dataset_name = datasets[name][\"name\"]\n",
    "    print(f'Applying pre-processing transformations to data in {dataset_name}', end=' ...')\n",
    "    start = perf_counter()\n",
    "    \n",
    "    if name.startswith('YD'):\n",
    "        ref_variable = leu_enk_name\n",
    "    else:\n",
    "        ref_variable = Leu_Enk_neg if ds['mode'] == '-' else Leu_Enk_pos\n",
    "    \n",
    "    compute_transf(ds, norm_ref=ref_variable)\n",
    "    \n",
    "    end = perf_counter()\n",
    "\n",
    "    print(f'done! took {(end - start):.3f} s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:darkred;\">------- Checkpoint for API migration</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to generate checkpoint for old API\n",
    "#treat_neg2['NGP'].to_csv('treat_neg2_NGP.csv', sep='\\t')\n",
    "\n",
    "roundrobin = pd.read_csv('treat_neg2_NGP.csv', sep='\\t', header=[0,1], index_col=0)\n",
    "\n",
    "# should not raise AssertionError:\n",
    "assert_frame_equal(roundrobin, datasets['GD_neg_class2']['NGP'].transpose())\n",
    "\n",
    "\n",
    "# A NOTE on pandas problem:\n",
    "# assert_frame_equal(roundrobin.transpose(), treat_neg2['NGP'])\n",
    "# fails the assertion because of m/z value float innacuracy when save to csv.\n",
    "\n",
    "#roundrobin.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:darkred;\">------- END checkpoint for API migration</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure dir exists\n",
    "path = Path.cwd() / \"paperimages\"\n",
    "path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "storepath = Path.cwd() / \"paperimages\" / 'processed_data.h5'\n",
    "\n",
    "store = pd.HDFStore(storepath, complevel=9, complib=\"blosc:blosclz\")\n",
    "#pd.set_option('io.hdf.default_format','table')\n",
    "\n",
    "# keep json serializable values and store dataFrames in HDF store\n",
    "\n",
    "serializable = {}\n",
    "\n",
    "for dskey, dataset in datasets.items():\n",
    "    serializable[dskey] = {}\n",
    "    for key, value in dataset.items():\n",
    "        #print(dskey, key)\n",
    "        if isinstance(value, pd.DataFrame):\n",
    "            storekey = dskey + '_' + key\n",
    "            #print('-----', storekey)\n",
    "            store[storekey] = value\n",
    "            serializable[dskey][key] = f\"INSTORE_{storekey}\"\n",
    "        else:\n",
    "            serializable[dskey][key] = value\n",
    "store.close()\n",
    "            \n",
    "\n",
    "path = path / 'processed_data.json'\n",
    "with open(path, \"w\", encoding='utf8') as write_file:\n",
    "    json.dump(serializable, write_file)\n",
    "\n",
    "#serializable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check data persistence, by reading it back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_datasets = datasets\n",
    "\n",
    "path = Path.cwd() / \"paperimages\" / 'processed_data.json'\n",
    "storepath = Path.cwd() / \"paperimages\" / 'processed_data.h5'\n",
    "with pd.HDFStore(storepath) as store:\n",
    "\n",
    "    with open(path, encoding='utf8') as read_file:\n",
    "        datasets = json.load(read_file)\n",
    "    \n",
    "    for dskey, dataset in datasets.items():\n",
    "        for key in dataset:\n",
    "            value = dataset[key]\n",
    "            if isinstance(value, str) and value.startswith(\"INSTORE\"):\n",
    "                storekey = value.split(\"_\", 1)[1]\n",
    "                dataset[key] = store[storekey]\n",
    "            # transform colors, saved as lists in json, back into tuples\n",
    "            elif key == 'label_colors':\n",
    "                dataset[key] = {lbl: tuple(c) for lbl, c in value.items()}\n",
    "            elif key == 'sample_colors':\n",
    "                dataset[key] = [tuple(c) for c in value]\n",
    "#datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roundrobin = pd.read_csv('treat_neg2_NGP.csv', sep='\\t', header=[0,1], index_col=0)\n",
    "\n",
    "# should not raise AssertionError:\n",
    "assert_frame_equal(roundrobin, datasets['GD_neg_class2']['NGP'].transpose())\n",
    "\n",
    "# check all dataframes are equal in retrieved data and memory data\n",
    "for dskey, dataset in datasets.items():\n",
    "    for key, value in dataset.items():\n",
    "        if isinstance(value, pd.DataFrame):\n",
    "            assert_frame_equal(value, old_datasets[dskey][key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_colors = datasets['GD_neg_class2']['label_colors']\n",
    "sns.palplot(label_colors.values())\n",
    "new_ticks = plt.xticks(range(len(label_colors)), label_colors.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_colors = datasets['YD']['label_colors']\n",
    "sns.palplot(label_colors.values())\n",
    "new_ticks = plt.xticks(range(len(label_colors)), label_colors.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_colors = datasets['vitis_types']['label_colors']\n",
    "sns.palplot(label_colors.values())\n",
    "new_ticks = plt.xticks(range(len(label_colors)), label_colors.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed = mtl.parse_data(datasets['GD_pos_class2']['data'], labels_loc='label')\n",
    "y = datasets['GD_pos_class2']['target']\n",
    "label_colors = datasets['GD_pos_class2']['label_colors']\n",
    "s = styled_sample_labels(parsed.sample_names, y, label_colors)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed = mtl.parse_data(datasets['vitis_types']['data'], labels_loc='label')\n",
    "y = datasets['vitis_types']['target']\n",
    "label_colors = datasets['vitis_types']['label_colors']\n",
    "s = styled_sample_labels(parsed.sample_names, y, label_colors)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed = mtl.parse_data(datasets['YD']['data'])\n",
    "y = datasets['YD']['target']\n",
    "label_colors = datasets['YD']['label_colors']\n",
    "s = styled_sample_labels(parsed.sample_names, y, label_colors)\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Characteristics\n",
    "\n",
    "<font color='darkred'>Warning: this section was repeated in notebook `paper_binsim_dataset_HD.ipynb` and adatset HD is included there</font>\n",
    "\n",
    "Building a table with general characteristics about the 7 data sets studied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def characterize_dataset(dskey, ds):\n",
    "    \"Computes and returns some general characteristics of a dataset in a dictionary.\"\n",
    "\n",
    "    dataset_chrs = {}\n",
    "    \n",
    "    name = ds['name'] # Name of the dataset\n",
    "    n_samples, n_feats = ds['data'].shape\n",
    "    n_classes = len(ds['classes'])\n",
    "       \n",
    "    Feat_Sample = ds['data'].count(axis=1) # Nº Features in each sample\n",
    "    Min_Feat_Sample = str(Feat_Sample.min()) # Minimum nº Features in a sample\n",
    "    Max_Feat_Sample = str(Feat_Sample.max()) # Maximum nº Features in a sample\n",
    "    Average_Feat_Sample = Feat_Sample.mean() # Average nº Features in a sample\n",
    "    \n",
    "    avg_feat_per_sample = int(round(Average_Feat_Sample,0)) # Round\n",
    "    \n",
    "    Samp_Class = len(ds['target'])/len(ds['classes']) # Nº Sample per Class\n",
    "    if dskey == 'vitis_types':\n",
    "        Samp_Class = '15 Vitis vinifera, 18 Wild'\n",
    "        #Samp_Class = '15 $\\it{Vitis}$ $\\it{Vinifera}$, 18 Wild'\n",
    "    else:\n",
    "        Samp_Class = str(int(Samp_Class))\n",
    "    \n",
    "    n_na = ds['data'].isna().sum().sum() # Nº of missing values in the dataset\n",
    "    \n",
    "    p_na = round(100.0 * n_na / (n_samples * n_feats), 2) # % of missing values in the dataset\n",
    "    \n",
    "    avg_na_per_feature = (n_samples - ds['data'].count(axis=0)).mean()\n",
    "    avg_na_per_feature = int(round(avg_na_per_feature, 0))\n",
    "    \n",
    "    return {'Data set': name,\n",
    "            '# samples': n_samples,\n",
    "            '# features': n_feats,\n",
    "            'features / sample (range)': f'{avg_feat_per_sample} ({Min_Feat_Sample}-{Max_Feat_Sample})',\n",
    "            '# classes': n_classes,\n",
    "            'samples / class':Samp_Class,\n",
    "            '% missing values': p_na,} \n",
    "            #'missing values / feature': avg_na_per_feature}\n",
    "\n",
    "data_characteristics = [characterize_dataset(dskey, ds) for dskey, ds in datasets.items()]\n",
    "data_characteristics = pd.DataFrame(data_characteristics).set_index('Data set')\n",
    "data_characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_characteristics.to_excel('paperimages/dataset_characteristics.xlsx', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA scores plots for the 7 data sets\n",
    "\n",
    "Representation of the samples when projected in the 2 Principal Components obtained from PCA.\n",
    "\n",
    "Preliminary assessment of the extent of class’s proximity, and consequent degree of difficulty for clustering and classification methods. Greater proximity/overlap would mean a more difficult task for the methods since it would mean the classes are similar to each other or less well defined.\n",
    "\n",
    "Ellipses shown are 95% confidence ellipses for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To consider: Maybe remove this function? No longer used\n",
    "def plot_PCA_old(principaldf, label_colors, label_symbols=None, components=(1,2), var_explained=None, title=\"PCA\", ax=None):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    \n",
    "    loc_c1, loc_c2 = [c - 1 for c in components]\n",
    "    col_c1_name, col_c2_name = principaldf.columns[[loc_c1, loc_c2]]\n",
    "    \n",
    "    #with sns.axes_style(\"whitegrid\"):\n",
    "    ax.axis('equal')\n",
    "    if var_explained is not None:\n",
    "        v1, v2 = var_explained[loc_c1], var_explained[loc_c2]\n",
    "        ax.set_xlabel(f'{col_c1_name} ({100*v1:.1f}%)', fontsize = 15)\n",
    "        ax.set_ylabel(f'{col_c2_name} ({100*v2:.1f}%)', fontsize = 15)\n",
    "    else:\n",
    "        ax.set_xlabel(f'{col_c1_name}', fontsize = 15)\n",
    "        ax.set_ylabel(f'{col_c2_name}', fontsize = 15)\n",
    "\n",
    "    unique_labels = principaldf['Label'].unique()\n",
    "    if label_symbols is None:\n",
    "        label_symbols = {lbl: 'o' for lbl in unique_labels}\n",
    "\n",
    "    lbl_handles = {}\n",
    "    for lbl in unique_labels:\n",
    "        subset = principaldf[principaldf['Label']==lbl]\n",
    "        hndl = ax.scatter(subset[col_c1_name], subset[col_c2_name],\n",
    "                          lw=1,ec='black', alpha=0.8,\n",
    "                          marker=label_symbols[lbl], \n",
    "                          s=80, color=label_colors[lbl], label=lbl)\n",
    "        lbl_handles[lbl] = hndl\n",
    "\n",
    "    #ax.legend(framealpha=1)\n",
    "    ax.set_title(title, fontsize=15)\n",
    "    return lbl_handles\n",
    "\n",
    "def plot_PCA(principaldf, label_colors, components=(1,2), title=\"PCA\", ax=None):\n",
    "    \"Plot the projection of samples in the 2 main components of a PCA model.\"\n",
    "    \n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    \n",
    "    loc_c1, loc_c2 = [c - 1 for c in components]\n",
    "    col_c1_name, col_c2_name = principaldf.columns[[loc_c1, loc_c2]]\n",
    "    \n",
    "    #ax.axis('equal')\n",
    "    ax.set_xlabel(f'{col_c1_name}')\n",
    "    ax.set_ylabel(f'{col_c2_name}')\n",
    "\n",
    "    unique_labels = principaldf['Label'].unique()\n",
    "\n",
    "    for lbl in unique_labels:\n",
    "        subset = principaldf[principaldf['Label']==lbl]\n",
    "        ax.scatter(subset[col_c1_name],\n",
    "                   subset[col_c2_name],\n",
    "                   s=50, color=label_colors[lbl], label=lbl)\n",
    "\n",
    "    #ax.legend(framealpha=1)\n",
    "    ax.set_title(title, fontsize=15)\n",
    "\n",
    "def plot_ellipses_PCA(principaldf, label_colors, components=(1,2),ax=None, q=None, nstd=2):\n",
    "    \"Plot confidence ellipses of a class' samples based on their projection in the 2 main components of a PCA model.\"\n",
    "    \n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    \n",
    "    loc_c1, loc_c2 = [c - 1 for c in components]\n",
    "    points = principaldf.iloc[:, [loc_c1, loc_c2]]\n",
    "    \n",
    "    #ax.axis('equal')\n",
    "\n",
    "    unique_labels = principaldf['Label'].unique()\n",
    "\n",
    "    for lbl in unique_labels:\n",
    "        subset_points = points[principaldf['Label']==lbl]\n",
    "        plot_confidence_ellipse(subset_points, q, nstd, ax=ax, ec=label_colors[lbl], fc='none')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style(\"whitegrid\"):\n",
    "    with sns.plotting_context(\"notebook\", font_scale=1):\n",
    "        f, axs = plt.subplots(3,3, figsize=(12,12), constrained_layout=True)\n",
    "\n",
    "        for (dskey, ds), ax in zip(datasets.items(), axs.ravel()):\n",
    "            df = datasets[dskey]['Ionly']\n",
    "            tf = transf.FeatureScaler(method='standard')\n",
    "            df = tf.fit_transform(df)\n",
    "            #print(df)\n",
    "            ax.axis('equal')\n",
    "            principaldf = ma.compute_df_with_PCs(df, n_components=5, whiten=True, labels=datasets[dskey]['target'], return_var_ratios=False)\n",
    "\n",
    "            lcolors = datasets[dskey]['label_colors']\n",
    "            #plot_PCA(principaldf, lcolors, components=(1,2), title=datasets[dskey]['name'], ax=ax)\n",
    "            plot_PCA(principaldf, lcolors, components=(1,2), title='', ax=ax)\n",
    "            plot_ellipses_PCA(principaldf, lcolors, components=(1,2),ax=ax, q=0.95)\n",
    "\n",
    "        axs[2][1].remove()\n",
    "        axs[2][2].remove()\n",
    "        #axs[0][1].legend(loc='upper right', ncol=1)\n",
    "        #axs[1][1].legend(loc='upper right', ncol=1)\n",
    "        axs[2][0].legend(loc='upper center', ncol=1, framealpha=1)\n",
    "        locs_YD = {'WT':(-1,-0.65),\n",
    "                   'ΔGRE3':(-0.45, 1.45),\n",
    "                   'ΔENO1':(0.7, 0.1),\n",
    "                   'ΔGLO1':(0.5, -0.5),\n",
    "                   'ΔGLO2':(0,0.7) }\n",
    "        for lbl in datasets['YD']['classes']:\n",
    "            axs[1][1].text(*locs_YD[lbl], lbl, c=datasets['YD']['label_colors'][lbl])\n",
    "        for lbl in datasets['YD']['classes']:\n",
    "            axs[1][2].text(*locs_YD[lbl], lbl, c=datasets['YD']['label_colors'][lbl])\n",
    "\n",
    "        locs_GD = {'CAN':(-1.4,-0.2),\n",
    "                       'CS':(-0.45, 2),\n",
    "                       'LAB':(-0.25, -0.2),\n",
    "                       'PN':(-1, 0.2),\n",
    "                       'REG':(1.8,0.5),\n",
    "                       'RIP':(-0.3,-0.7),\n",
    "                       'RL':(-0.5, 1.45),\n",
    "                       'ROT':(-1, -1.2),\n",
    "                       'RU':(0.5, -1),\n",
    "                       'SYL':(-0.2,0),\n",
    "                       'TRI':(0.5,0.5),}\n",
    "        \n",
    "        for lbl in datasets['GD_neg_global2']['classes']:\n",
    "            axs[0][0].text(*locs_GD[lbl], lbl, c=datasets['GD_neg_global2']['label_colors'][lbl])\n",
    "\n",
    "        locs_GD = {'CAN':(-0.1,-1),\n",
    "                       'CS':(-0.45, 2),\n",
    "                       'LAB':(-0.2, -0.5),\n",
    "                       'PN':(-1, 0.6),\n",
    "                       'REG':(1.8,0.4),\n",
    "                       'RIP':(-0.3,-1.7),\n",
    "                       'RL':(-0.2, 1),\n",
    "                       'ROT':(-1, -1.2),\n",
    "                       'RU':(0.9, -1),\n",
    "                       'SYL':(-1.2,-0.2),\n",
    "                       'TRI':(0.5,0.1),}\n",
    "        \n",
    "        for lbl in datasets['GD_neg_global2']['classes']:\n",
    "            axs[0][2].text(*locs_GD[lbl], lbl, c=datasets['GD_neg_global2']['label_colors'][lbl])\n",
    "\n",
    "        locs_GD = {'CAN':(-0.25,0),\n",
    "                       'CS':(-0.7, -0.6),\n",
    "                       'LAB':(-1.2, 0),\n",
    "                       'PN':(0.5, 3),\n",
    "                       'REG':(2,-0.3),\n",
    "                       'RIP':(-0.25,-0.3),\n",
    "                       'RL':(-0.3, -0.6),\n",
    "                       'ROT':(-1.2,-0.6),\n",
    "                       'RU':(1, 0),\n",
    "                       'SYL':(-0.75,0),\n",
    "                       'TRI':(0.5,-0.5),}\n",
    "        \n",
    "        for lbl in datasets['GD_neg_global2']['classes']:\n",
    "            axs[1][0].text(*locs_GD[lbl], lbl, c=datasets['GD_neg_global2']['label_colors'][lbl])\n",
    "        \n",
    "        locs_GD = {'CAN':(-0.25,0),\n",
    "                       'CS':(-0.7, -0.6),\n",
    "                       'LAB':(-1.2, 0),\n",
    "                       'PN':(0.5, 3),\n",
    "                       'REG':(2,-0.4),\n",
    "                       'RIP':(-0.25,-0.3),\n",
    "                       'RL':(-0.3, -0.6),\n",
    "                       'ROT':(-1.2,-0.6),\n",
    "                       'RU':(1, 0.2),\n",
    "                       'SYL':(-0.75,0),\n",
    "                       'TRI':(0.5,-0.4),}\n",
    "        \n",
    "        for lbl in datasets['GD_neg_global2']['classes']:\n",
    "            axs[0][1].text(*locs_GD[lbl], lbl, c=datasets['GD_neg_global2']['label_colors'][lbl])\n",
    "        \n",
    "        for letter, ax in zip('ABCDEFGHIJ', axs.ravel()[0:7]):\n",
    "            ax.text(0.88, 0.9, letter, ha='left', va='center', fontsize=15, weight='bold',\n",
    "                    transform=ax.transAxes,\n",
    "                    bbox=dict(facecolor='white', alpha=0.9))\n",
    "        \n",
    "        plt.show()\n",
    "        #f.savefig('paperimages/PCAs.pdf', dpi=200)\n",
    "        #f.savefig('paperimages/PCAs.png', dpi=600)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphical Representation of a portion of the GDc2- dataset after BinSim or NGP pre-treatments\n",
    "\n",
    "Portion represented: From feature 51 to 150 from the GDc2- dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed extra import\n",
    "from matplotlib import cm\n",
    "\n",
    "# For the BinSim legend\n",
    "Reds = cm.get_cmap('Reds', 20)\n",
    "Present_Feature = mpatches.Patch(color=Reds(range(20))[-1], label='Present Feature (1)')\n",
    "Absent_Feature = mpatches.Patch(color='white', label='Absent Feature (0)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (axl, axr) = plt.subplots(1,2, figsize=(16,8))\n",
    "\n",
    "cbar_ax = fig.add_axes([0.485, .13, .02, .74]) # For the colorbar\n",
    "\n",
    "# NGP Pre-Treatment\n",
    "hm = sns.heatmap(datasets['GD_neg_class2']['NGP'].iloc[:,50:150], annot=False, ax=axl, cmap = 'Greens', linewidths = 1, \n",
    "                 linecolor = 'black',  xticklabels = False, cbar_ax=cbar_ax)#, norm=LogNorm())#cbar = False,\n",
    "bottom, top = axl.get_ylim()\n",
    "\n",
    "axl.set_ylim(bottom + 0.5, top - 0.5)\n",
    "axl.set_title('NGP', fontsize=15)\n",
    "axl.set_xlabel('Features', fontsize=14)\n",
    "axl.set_ylabel('Samples', fontsize=14)\n",
    "\n",
    "# BinSim Pre-Treatment\n",
    "hm = sns.heatmap(datasets['GD_neg_class2']['BinSim'].iloc[:,50:150], annot=False, ax=axr, cmap = 'Reds', linewidths = 1, \n",
    "                 linecolor = 'black', cbar = False, xticklabels = False, yticklabels = False)\n",
    "bottom, top = axr.get_ylim()\n",
    "axr.set_ylim(bottom + 0.5, top - 0.5)\n",
    "axr.set_title('Binary Simplification', fontsize=15)\n",
    "axr.set_xlabel('Features', fontsize=14)\n",
    "axr.set_ylabel('Samples', fontsize=14)\n",
    "axr.legend(handles = [Present_Feature, Absent_Feature], bbox_to_anchor=(1.01, 0.99), loc='upper left', borderaxespad=0., \n",
    "           fontsize = 12, ncol=1, frameon=True, framealpha=0.5, facecolor='white')\n",
    "# Black box around legend colours\n",
    "for legend_handle in axr.get_legend().legendHandles:\n",
    "    legend_handle.set_edgecolor('black')\n",
    "    \n",
    "fig.suptitle('GDc2-', fontsize = 16, y=0.95)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (axl, axr) = plt.subplots(1,2, figsize=(16,8))\n",
    "\n",
    "cbar_ax = fig.add_axes([0.485, .13, .02, .74]) # For the colorbar\n",
    "\n",
    "# For the BinSim legend\n",
    "Blues = cm.get_cmap('Blues', 20)\n",
    "Present_Feature = mpatches.Patch(color=Blues(range(20))[-1], label='Present Feature (1)')\n",
    "Absent_Feature = mpatches.Patch(color='white', label='Absent Feature (0)')\n",
    "\n",
    "# NGP Pre-Treatment\n",
    "hm = sns.heatmap(datasets['GD_neg_class2']['NGP'].iloc[:,50:150], annot=False, ax=axl, cmap = 'Blues', linewidths = 1, \n",
    "                 linecolor = 'black',  xticklabels = False, cbar_ax=cbar_ax)#, norm=LogNorm())#cbar = False,\n",
    "bottom, top = axl.get_ylim()\n",
    "\n",
    "axl.set_ylim(bottom + 0.5, top - 0.5)\n",
    "axl.set_title('NGP', fontsize=15)\n",
    "axl.set_xlabel('Features', fontsize=14)\n",
    "axl.set_ylabel('Samples', fontsize=14)\n",
    "\n",
    "# BinSim Pre-Treatment\n",
    "hm = sns.heatmap(datasets['GD_neg_class2']['BinSim'].iloc[:,50:150], annot=False, ax=axr, cmap = 'Blues', linewidths = 1, \n",
    "                 linecolor = 'black', cbar = False, xticklabels = False, yticklabels = False)\n",
    "bottom, top = axr.get_ylim()\n",
    "axr.set_ylim(bottom + 0.5, top - 0.5)\n",
    "axr.set_title('Binary Simplification', fontsize=15)\n",
    "axr.set_xlabel('Features', fontsize=14)\n",
    "axr.set_ylabel('Samples', fontsize=14)\n",
    "axr.legend(handles = [Present_Feature, Absent_Feature], bbox_to_anchor=(1.01, 0.99), loc='upper left', borderaxespad=0., \n",
    "           fontsize = 12, ncol=1, frameon=True, framealpha=0.5, facecolor='white')\n",
    "# Black box around legend colours\n",
    "for legend_handle in axr.get_legend().legendHandles:\n",
    "    legend_handle.set_edgecolor('black')\n",
    "    \n",
    "fig.suptitle('GDc2-', fontsize = 16, y=0.95)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
